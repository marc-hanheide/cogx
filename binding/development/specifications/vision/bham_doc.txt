/*!
\page bham_spec Specification for the vision subarchitecture (BHAM)

\section scenario Scenario

The main scenario we are considering is the colour and shape game,
which will also be used to learn the representations of actions
demonstrated by a human. For details on the action representation
learning, please see: \ref bham_scenarios

\section requirements Requirements (from Vision)

 \li Ability to recognise objects from a variety of views.

 \li Ability to recognise complex actions (composed of sequences of
 actions). 

 \li Ability to use learned categories for shape, colour and
 projective spatial relations.

 \li Ability to provide information to perform visual servoing
 reliably (for pick and place actions).

 \li Ability to understand which events in a sequence an utterance
 refers to. (Somboon)

 \li Ability to reason about episodes and sequences of
 events. (Somboon)

 

\section representations Representations

 The representations we intend to use for representing objects in the
 scene are based on those used by the rest of the vision system (see
 \ref uol_spec for complete details). We only describe the
 additions/modifications here.

 \subsection Representations of complex actions (Somboon)



 \subsection Recognition of complex action and episodes (Somboon)



 \subsection Using learned categories for shape, colour and object
 classifiers

 The learned categories for shape, colour and object classification is
 necessary for the scenarios we plan to test (\ref
 bham_scenarios). The plan is to use the colour and shape learner
 developed by UoL, and the existing SIFT code for the object
 classification. The existing definitions of ROI and SceneObject
 already include the necessary 'flags' to store the class information
 for all these features.

 Until the UoL learners are integrated in the code, we already have
 existing code that can learn models for colour distributions and
 shape (developed at Bham). These are currently being used for
 detection of objects such as 'red triangle' and 'blue square', i.e.\
 we can proceed with work on the planning aspects while the lifetime
 learning scheme is developed at UoL.

 Another addition in the Bham-developed equivalents of the colour and
 shape classifiers (same feature has been added to the SIFT-based
 object classifier as well) is that the components internally maintain
 a probability of match across the different classes. For instance,
 the colour classification component maintains:

 \code
    m_colorProbs[N]
 \endcode

 where N = number of colour classes under consideration. This feature
 is used in our probabilistic planning scheme for joint planning of
 information processing and sensing actions (\ref blah).

 \subsection Perceptual grouping for visual servoing
 
 Perceptual grouping (developed by mxz) has been encapsulated into a
 separate component that can run on ROIs. The component is similar to
 the colour, shape and object classification (SIFT) component in that
 it takes in an input ROI and outputs a list of 'perceptual groups'
 within that ROI. Currently, it supports the specification of
 rectangles and ellipses (closures) as the perceptual groups to be
 found, but soon it will allow for other perceptual groups as well
 (junctions, arcs etc that are being found already but not being
 stored separately).  

 The perceptual grouping operator will primarily be used by the arm
 camera to detect the top surface of the objects that need to be
 picked up. But we plan to also use it for shape recognition
 (rectangles, triangles, circles etc) as an additional source of
 information. The difference when compared to the colour and shape
 operators is that there is currently no computation of a probability
 of match with the different shape classes (since there are no
 'training' samples to compare against) but we will provide a
 probability measure based on the degree of similarity with the
 different shape classes.

 When used for detecting the top surface of the objects (to be picked
 up), the perceptual grouping operation will be used as part of the
 sequence of components incorporated in response to the pick-and-place
 command initiated by the planning module. Currently the perceptual
 grouping is performed only before the arm decides to move towards the
 object (and never after that) but we plan to change it so that it
 periodically checks for the object's top surface -- this will help us
 eliminate several failure cases, for instance the case where the
 object falls from the robot's grip soon after it is picked up but the
 arm still keeps moving until it has placed the object. Note that the
 change is only in the sequence of operations triggerred by a
 pick-and-place command. The planning and manipulation modules remain
 unchanged (or so we hope!.


 \subsection Combining information from different viewpoints

 Combining information from different viewpoints is necessary for the
 scenarios we plan to test -- for instance, when the robot has to
 'look behind the wall' the information is to be collected by the
 camera on the manipulator/arm and not by the stereo camera which
 stays stationary. The different viewpoints will provide different
 ROIs which may or may not correspond to the same object. We deal with
 this situation by adding viewpoint as an additional entry in the ROI,
 which will be used to determine the corresponding projective
 transform while deciding on the SceneObject.


 \code
 /* region of interest*/
 struct ROI {
    Image          m_region;    // The region of the original image.
    Image          m_mask;      // The segmentation mask for this ROI.
    Matlab::Matrix m_features;
    Matlab::Matrix m_attributes;
    Matlab::Matrix m_probableAttributes;
    string         m_objId;   // working memory address of the corresponding
                              // object
    BBox2D         m_bbox; 
    long           m_camNum;  // 0: left, 1: right, 2: arm-mounted
    FrameworkBasics::BALTTime  m_time;

    // Vector of points to store the contour points -- in case want to
    // use for shape analysis...
    vector2DSequence m_contourPoints;

    // Maintain a notion of viewpoint where the ROI was formed...
    long	   m_viewpoint;  // 0-N based on the different viewpoints...
 }; 
 \endcode

 Note that the <SceneObject> structure itself stays unchanged because
 it represents the 3D pose of the object, which does not change with
 the change in viewpoint. Different viewpoints will automatically
 trigger different projective transformations resulting in changes in
 the corresponding 3D object's properties, i.e.\ within the
 corresponding <SceneObject> struct.



*/
