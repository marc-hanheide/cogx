/*!
\mainpage Specification for the comsys subarchitecture

\section motivation Motivation

The main purpose of the comsys subarchitecture is to process spoken dialogue in human-robot interaction. This covers both 
comprehension and production -- the robot needs to be able to understand what you say, and produce a response. For comprehending an 
utterance, the subarchitecture is to create an interpretation of the utterance. This interpretation is to be based 
on (a) the (linguistic) meaning revealed by its grammatical structure, and how that linguistic meaning can be related to (b) the preceding 
dialogue context, and (c) the situated context. For producing an utterance, the subarchitecture is to turn a communicative goal into one 
or more utterances. These utterances should convey the intended communicative goal, and refer to the dialogue- and situated context in such ways that these references are easy to resolve for
a hearer (i.e. the human). To relate linguistic content to the situated context, the subarchitecture needs to bind its internal content representations to corresponding representations of other subarchitectures. 

The comsys subarchitecture is documented here: http://www.dfki.de/cosy/www/pubs/pdfs/kruijff+etal.incrsitdial.langro2007.pdf

\section scenario Scenario

See other scenario: \ref main_scenarios

In the context of these scenarios, spoken dialogue in human-robot interaction is used primarily for: 

\li Instructing the robot (commands, as direct or indirect speech acts):<br> 
	"Go to the kitchen!" "Could you please get me a coffee cup?"
\li Informing the robot about the environment (assertions): <br>
	"This is a red box." "You will find the library at the end of the corridor."
\li Asking the robot for information (questions): <br> 
	"Do you know where the toilets are?" "What kind of object is this?"
\li Asking the robot for clarification (questions): <br>
	"Why did you fail to deliver the book?" 
\li Asking the human for information (questions): <br>
	"Could you tell me where the kitchen is?" "What color is this mug?"
\li Asking the human for clarification (questions): <br>
	"Is there a door here?"
\li Informing the human about the environment (assertions, answers): <br>
	"The toilets are opposite to the lab."
\li Providing spoken grounding feedback: <br> 
	"Okay."

\section comsys_requirements Requirements

The requirements can roughly be divided into
\li \ref comsys_scalability
\li \ref comsys_robustness
\li \ref comsys_situation_binding

\subsection comsys_scalability Requirements on scalability of spoken dialogue comprehension and -production.

Language provides a very rich medium for conveying content. The subarchitecture needs to be scalable in that additional levels of interpretation can be introduced, and interrelated. 
It must be possible to interrelate levels of interpretation to arrive at a convergent, holistic interpretation of an utterance "in context." 
Furthermore, it needs to be possible to guide processing at a particular level of interpretation, using context -- i.e. interpretations already established at other levels, 
or information available from extra-linguistic interpretations (through binding). 

\subsection comsys_robustness Requirements on robustness of spoken dialogue comprehension

A fundamental problem of spoken dialogue is that utterances may be incomplete ("I mean the ..uh.."), fragmentary ("Yes, red") or ungrammatical ("Now take the red ball ... red box and put it near the mug."). 
People correct themselves, change their mind about what they want to say, or reply in a very concise way relying on the hearer to be able to establish the ellided meaning from the current context. 

The subarchitecture must be robust to these phenomena, in several ways. It needs to be able to interpret them, and build an interpretation which integrates as much of the content as possible. Information from the 
dialogue- and situated context should be used to anticipate how incomplete, or incorrectly structured, content could be put together into a single interpretation. On the other hand, should the subarchitecture fail 
to come up with an interpretation, it should fail gracefully. It should be able to clarify with the speaker, what was said (perception failure), meant (comprehension failure), or how something was meant (intention failure). 

\subsection comsys_situation_binding Requirements on cross-modal interconnectivity

The subarchitecture needs to be able to connect linguistic content to content about the situated context. There are two reasons for this. 
One, the subarchitecture needs to build utterance interpretations which are supported by the situated context. Two, dialogue is just one type of action, set in the larger context of what the robot is intending to do. 
This means that when the robot is to say something, what it says and how it is saying should fit in the larger behavioral context. 

\subsubsection cmi_comsys Cross-modal interconnectivity for grounding linguistic content

Linguistic meaning can be modeled as an ontologically richly sorted, relational structure. Cross-modal interconnectivity should reflect the meaning differentiation arising from sorting and relating content. 
When binding linguistic content, it thus needs to be possible to bind both propositions and relations. The results of binding content, notably whether or not content could be bound, should then be returned to the 
subarchitecture. These results should then be used to prime attentional processes at the different levels of interpretation. The purpose is to guide processing such that the subarchitecture converges on 
"contextually supported" interpretations for an utterance -- i.e. utterances which content can be bound (inasfar as necessary; possibly, still, ambiguously). 

Because binding results are only useful for priming processes if these processes are still ongoing, the above requirements place the additional requirement of \e incremental \e processing on the subarchitecture. Processing
within the subarchitecture should produce and openly publish results in a step-wise fashion, to offer the possibility for other processes to (a) retrieve these results, (b) process these results, and (c) provide 
information that can guide selectional attention (priming the processing as analyses unfold further). 

\subsubsection cmi_context Cross-modal interconnectivity for planning and producing linguistic content

Dialogue is a type of action. As such, it is set within the larger context of situated acting and interacting. This requires the subarchitecture to be able to retrieve information about the current action context, 
and the relevant situated contexts, when planning what to say next and how. Dialogue planning must be closely integrated with action planning, to form a continuum in which both action and interaction can be co-planned, 
possibly performing multiple "communicative" functions at once. 

\section binding_representations Representations

The comsys subarchitecture is concerned with creating two types of
representations: \e linguistic representations, and \e binding representations. 
The linguistic representations are produced by the subarchitecture components, 
and are stored on the comsys working memory. The binding representations 
are produced by the binding monitor, and include the proxies (binding working memory) and
the binding results (comsys working memory). 

The \ref linguistic_repr linguistic representations for comprehension are:
\li \ref comsys_recogresult
\li \ref comsys_phonstring
\li \ref comsys_packedlfs
\li \ref comsys_cache
\li \ref comsys_nucleus
\li \ref comsys_sdrs

On the production side, we have the following representations: 

\todo To be provided. 

The \ref comsys_binding representations for binding are:
\li \ref comsys_proxies
\li \ref comsys_results

Below we discuss these representations in more detail. 

\subsection linguistic_repr Linguistic representations

\subsubsection comsys_recogresult Speech recognition - audio recognition results

The \b RecogResult represents the result of a transaction with the Nuance Speech Recognition Server. Transactions are over a VoiP link. 

The boolean flag \e isRecognized is set to true if something has been recognized by the ASR engine, false otherwise. 
The boolean flag \e isConnectionClosed us set to true if the connection has been shut down by the caller, false otherwise
The string \e recString	provides the string which has been recognized if isRecognized is true, or the empty string otherwise. For this string, 
the long parameter \e confidence gives the confidence value of the result if there is one, or 0.0 otherwise. 
The long parameter \e probability gives the probability value of the result if there is one, or 0.0 otherwise. 
Finally, to identify the source (VoiP), the string \e ipAddress	gives the IP address of the caller. 


\code
	struct RecogResult {
		boolean isRecognized ;
		boolean isConnectionClosed ;
		string recString ;
		long confidence ;
		long probability ;
		string ipAddress ;
	  } ;
\endcode
\sa \c ComsysEssentials::RecogResult


\subsubsection comsys_phonstring Speech recognition - phonetic string

The \b PhonString provides a (simplified) data structure for information about a recognized string. It states the length, the string as a sequence of words, and a confidence value (as per the recognition result). 

\code
	struct PhonString { 
		Id id;
		string wordSequence; 
		long length;
		float confidenceValue;
	}; // PhonString
\endcode
\sa \c ComsysEssentials::PhonString

\subsubsection comsys_packedlfs Parsing - packed logical forms


The \b PackedLFs struct encapsulate packed logical forms. A packed logical form is a graph-based representation of the logical form(s) that represent interpretations for a (given) string. 
In a packed logical form, representations shared between the different logical forms are represented only once, as a \e packing \e node. A single packing node contains a graph structure -- 
up to the complete logical form for the utterance, if the utterance has an unambiguous interpretation. If an interpretation is ambiguous, the packed logical form will contain more than one packing node, 
connected by \e packing \e edges. Packing edges are named, following the named relations in logical forms. Variation between ambiguous logical forms is represented as different nodes 
(e.g. to deal with alternative lexical interpretations) or as different connectivity (e.g. to deal with attachment ambiguities). A packed logical form has a unique root. 

The long parameter \e finalized indicates whether the interpretations are finalized (0=unfinished; 1=finished parsing, 2=finished final pruning) . 

\code	
	struct PackedLFs { 
		Id id;
		PhonString phon;
		long stringPos;
		LFPacking::PackedLogicalForm packedLF;
		long finalized; 
	}; // end PackedLFs
\endcode
\sa \c ComsysEssentials::PackedLFs ,  
\c LFPacking::PackedLogicalForm

\subsubsection comsys_cache Dialogue interpretation - dialogue contextual referents

An important step for interpreting an utterance at the level of dialogue context is relating the individual objects and events in an utterance to what has already been previously mentioned. 
For each event and object node in a (packed) logical form for an utterance, a referent is established. This referent may be new (first mention), or it is an already existing referent (resolved reference to 
a previous mention). Referents are stored in a \b Cache. A cache is a pair of pointers to content representations (one of which is a set of graph structures from the packed logical form) , and a mapping between indices 
in these representations to associate content. 

A Cache has a \e cacheType, indicating the type of information it associates with a packed logical form \e plf. The type is as per the comsys ontology. The \e content2 is a set of pointers to graph structures 
within the packed logical form, and \e content1 is a set of pointers to the associated content (in this case, referents). The \e mapping associates these two sets of pointers. 

\code
	struct Cache {
		string CacheId ;  			
	    string cacheType ;			
		PackedLFs plf ;				
		sequence<any> content1 ;
		sequence<any> content2 ;
		CacheMapping mapping ;	
	} ;
\endcode
\sa \c ComsysEssentials::Cache

\subsubsection comsys_nucleus Dialogue interpretation - event structure

A \b Nucleus captures the basic structure of an event, following the model proposed by Moens and Steedman (1988). 

\code
	struct Nucleus {
		string nucleusId;
		string plfId;
		Cache  discRefs;
		sequence<Event> events;
		sequence<State> states;
		sequence<AspectualRelation> aspectualRelations;
		sequence<TemporalRelation> temporalRelations;		
	};
\endcode
\sa \c ComsysEssentials::Nucleus


\subsubsection comsys_sdrs Dialogue interpretation - dialogue context model 

The comsys maintains a model of the dialogue context. This model stores the interpretations of all the utterances exchanged so far. Utterances are related using \e dialogue \e moves, and \e event \e structures. 
The dialogue moves indicate how an utterance contributes to the dialogue (e.g. type of question, answer, etc.). The sequence resulting from relating utterances by dialogue moves indicates the temporal sequence 
of the dialogue. The related event structures represent a (possibly) different temporal structure, namely of how the events being talked about can be temporally related. (Event temporal order need not be the same
as order of mention.)

The formulation of the context model follows the Segmented Dialogue Representation Structures of Asher and Lascarides (2003). The model consists of labelled formulas, and references. Formulas represent packed 
logical forms, or relations (dialogue moves). Relations range over labels assigned to formulas. 

\sa \c ComsysEssentials::SDRS , 
\c ComsysEssentials::SDRSFormula , 
\c ComsysEssentials::SDRSRelation 

\subsection comsys_binding Binding representations

Most of the interaction between the comsys and other subarchitectures will be over the binding subarchitecture, using the proxies provided by the comsys. 

\subsubsection comsys_proxies Proxies 

The binding monitor for the comsys subarchitecture currently provides two classes of proxies: 

\li \ref object_proxies
\li \ref event_proxies

These proxies are produced by cycling over the graph structure of a packed logical form. For each node in a logical form on a packing node in a packed logical form, a \e proxy \e factory is invoked based on the 
ontological sort for the content on that logical form node. The result of applying a proxy factory is that information is being added to the current proxy. Furthermore, the proxy factory returns a 
(possibly non-empty) list of nodes which should be no longer considered for further processing in the current cycle. 

Below we specify the proxy classes in more detail. 

\paragraph object_proxies Object proxies

An object proxy is a single proxy (not a graph structure). It can be produced by the \b Thing proxy factory, or by the \b Ascription proxy factory. The Thing proxy factory takes the logical form for an object, 
usually resulting from a noun with a determiner and zero or more adjectival properties. The properties for \e color, \e size, and \e shape are translated into the corresponding proxy features. Furthermore, 
number is provided, indicating whether there is a singular object, or several ones. Any other properties are currently represented only in the \e DebugString field of a proxy. 
These properties include \e determinacy (existential, unique), \e quantification (specific_singular, specific_nonsingular, nonspecific_singular), derived from determiners; \e deictic_proximity (proximal), derived
from demonstratives or deictic pronouns. 

The Ascription proxy factory deals with ascriptions, i.e. copula constructions in which a property is being attributed to an object: "The mug is red", "It also is big", "The shape of the mug is round." The factory 
folds the properties and the object description into a single proxy, of the format as produced by the Thing proxy factory. 

\paragraph event_proxies Action-event proxies

The comsys binding monitor also creates proxies to model action events. The difference between these proxies and those for objects, is that action events lead to \e graphs connecting several proxies. These graphs 
are constructed using proxies and proxy relations. The proxy-based representation for action events always includes the following three components: the proxy for the action itself, proxies for the "arguments" of 
the action (Actor, Patient, etc.), and the event structure. 

More specifically, for the following action-events these proxy structures are generated: 

\li \b take yields a consequent state in which the addressed Actor is in possession of the Patient object: "Take the mug." There are proxies for the event, the addressed actor (proxy relation with event: Actor), 
and the affected object (proxy relation with event: Object). Between the actor proxy and the object proxy there is an additional proxy relation \e has, to indicate possession. The \e has proxy relation is itself
related to the event by a \e consequence \e future relation, to indicate the possession is a desired goal state. 

\li \b go yields a consequent state in which the addressed actor is in the destination area: "Go to the kitchen." There are proxies for the event, the actor (Actor proxy relation with event), and the area (proxy relation, 
usually of type "To"). The actor and the area are related with an "In" proxy relation, which in turn is related to the event by a consequence future relation to indicate desired state. 

\li \b put yields a consequent state in which the patient is in the destination area: "Put the mug near the ball." There are proxies for the event, the actor (Actor), the affected object (Patient), and the 
landmark relative to which the object should be put. The actor and object are connected to the event as above. The object and the landmark are connected with a proxy relation, indicating the spatial relation (here, 
\e near). This proxy relation is related to the event as consequence future. 

Based on how the event structures for the individual events are related in the dialogue context model, additional proxy relations can be put in place between the event nodes. Notably, if one event acts as a 
preparation for another event, there will be a "future preparation" relation. 

\subsubsection comsys_results Binding results

The binding monitor returns a structure \b ContextInfo indicating the results for binding linguistic content. ContextInfo provides information about supported and unsupported interpretations for a given 
packed logical form (by id). 

\code
	struct ContextInfo { 
		Id plfId; 
		sequence <InterpretationSupport> interpretations;
	}; // end ContextInfo
\endcode
\sa \c ComsysEssentials::ContextInfo

An \b InterpretationSupport provides information about a single relation (in a packed logical form), and provides a list of supported or unsupported interpretations. Support is indicated by a boolean flag: "true" means
the interpretation is supported, false means it is not. 
 
\code	
	struct InterpretationSupport { 
		Id plfId;
		string headNomVar;
		string depNomVar;
		string mode;
		boolean isSupported;
		sequence <string> LFids;		
	}; 
\endcode
\sa \c ComsysEssentials::InterpretationSupport

\section Components

\section Processes







*/
