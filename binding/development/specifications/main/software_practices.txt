/*! \page software_principles Software engineering principles

\section versioning Versioning

\sa <a href = "http://svnbook.red-bean.com/en/1.1/ch04s07.html">branches on svn</a>

Stable branch on SVN. The goal is to get all subsystems into the
stable branch. If a subsystem is not in the stable branch, it
will NOT be demonstrated. 

To get a subsystem into the stable branch, it needs to pass a barrier
which takes into account:

\li Documented: parameters options, compilation instruction, config files etc.
\li Testing (at least don't break anyting)
\li Bug reporting on bugzilla

Each component should have a leader who is responsible for the
development and testing etc.

\attention <b> Step 1: Stable version of year 3 system by 1 March</b>

Plan: Every week there will be a new stable branch tag. If there is
anything new or not doesn't matter. The routine will be that every
week, there is a new tag and everyone knows it. 

\subsection svn_ci Checking in in practice

\li Only check in when you are sure the code compiles. On your machine and on other 
\li Always update in the root before committing your code.
\li Communication. Use the mailing list to inform everyone.
\li Bugzilla can possibly help us to inform about bugs, bugfixes, feature requests etc. Nick will add cosy integration list as default email.
\li Check in frequently (minimum weekly) to avoid that subarchitectures start diverting

\section testing Testing

Isolated autometed tests: Example that works and examples that
shouldn't work. This makes sense only if the cost/pay-off ratio is
low. For example, for the grammar, it is easy enough to define
sentences that should or shouldn't be passed. For other subsystems,
they can only really be tested as part of running the full system.

\image html testing.jpg

\section tags Tags
\sa <a href = "http://svnbook.red-bean.com/en/1.1/ch04s06.html">svn tags</a>

Three level of stability tags: 
<ol>
<li>stable: weekly, the stable branch gets tagged</li>
<li>scenario stable: the whole system has been test on a full scenario nad gets tagged as scenario stable</li>
</ol>
Stability tags define points in time when the system was/is
stable. With these tags, it wil be possible to know how old the stable
branch is (never more than a week old). We also know that there is a
version that someone took throught the whole scenarion at some point.


Levels of tests:
<ol>
<li> Testing of stable branch: This is a barrier to be tagged 'scenario stable'</li>
<ol>
<li>Scenario test: A handful of times before the review demo</li>
<li>Subscenario test: As many as possible </li>
</ol>
<li> Testing of working branch(es): This is the barrier, everything below here should be tested before being added to stable branch</li>
<ol>
<li>Runtime tests on hardware: depends on subarchitecture</li>
<li>Runtime tests on different machines: as preparation for stable branch (but also try doing this yourself as frequently as possible)</li>
<li>Runtime tests on your own machine: all the time</li>
<li>Compile time tests: all the time</li>
</ol>
</ol>


\section error Error handling

\li assertions assertions assertions! Better have too many than too few!
\li catch exceptions judiciously!

\section components Components

Sanity checks of WM contents.

\subpage se_emails

\page se_emails Emails about testing etc

\section henrikj Henrik J. wrote:

1. Every time anyone runs your software, it is getting tested
(possibly in situations you never imagined). Our software is in fact
very WELL tested... that is not the problem, the problem is that we do
not fully take advantage of it. Make sure that this implicit test is
taken advantage of by making it part of your design:

1a. add assertions in your code to test pre and postconditions
(typically on what your process reads on WM). If an assertion fail
you'll know where it failed. and you can easily continue from that.
(http://java.sun.com/j2se/1.5.0/docs/guide/language/assert.html)

1b. add assertions! Just do it! They're cheap and do not make your
code cluttered (they may even make it more readable)

1c. Throw exceptions when you notice something that you suspected your
user could do wrong (slightly more informative than assertions)

1d. do not catch exceptions carelessly! That would eliminate the
efforts of 1c... alternatively, only throw runtime exceptions in 1c...
or do assertions! Let no error sneak you by.

1e. if you know that some particular input to a component will result
in an undefined behaviour throw an exception. (this is notoriously
difficult to do, but please try)

1f. in lack of detailed requirement specifications, use assertions to
make sure that external data are as YOU expect them to be according to
your idea of what others should provide you. How? Assertions! And if
an assertion fails, make contact with folks and start discussing
things.

2. In order to avoid some future errors, be strict and assume your
future users (i.e. including you) are going to be deliberately stupid.

2a. no public data members, ever (ok, we all cheat on this, bu it's a
good idea in general). If you know a data member shouldn't EVER be
changed after construction, then hide it!

2b. do not make implicit initiation of data members that control the
behaviour of your class. If you require the user to make a choice of a
parameter, make sure the program is aborted (e.g. with an assertion)
if the user has forgotten to do so. This way you will avoid that the
user by mistake overlooks a parameter (perhaps it wasa a public data
member)

2c. strictness applies to cast-file interpretation too. After some
refactoring of the code we'll be more strict here. My experience is
that a lot of error where due to silly mistake in the cast files...
too much copy-paste going on in them.

2d. if you do have a very precise idea how other components should be
interacting with yours (but have given up trying to make everyone read
that specification), make sure that the interaction indeed following
the protocol. How? Assertions!

2e. prefer compile time errors over runtime errors anytime! i.e., make
things private (and in c++ alse const) by default and protected
secondarily. (public should be used with a similar frequency as you
show yourself naked in public...)

2f. prefer informative crashes over undefined behaviour anytime. Don't
be shy to terminate the entire architecture when you discover sth
strange... If it's strange it shouldn't be there and it shouldn't be
ignored so go back and do it right...

3. make sure we all test the same stuff (and then some). We're on many
different platforms and we do indeed get lots of different behavious
on each machine. But we're also editing our own cast-files and do not
necessarily share them (partly because of all absolute paths in them)

3a. eliminate all absolute paths in cast-files...

3b. check in your cast files... noone will be upset as long as all
paths are relative...

3c. make a *limited* list of cast-files we all must run before
committing anything (with some exceptions and variations for folks
with/without cameras, Matlab etc) (cf. Aaron's list)

4. As MZ and Aaron said... pure automated testing is not feasible nor
effective in this case. But, there is one thing you can do. You can
add a component which has only one task: to go through the WM contents
and do assertions assertions assertions...

4a. This should not be necessary if you added assertions everywhere in
the ordinary components... but some extra sanity checks can't hurt if
it's not too expensive to add of course.

\section alen Alen wrote

Hello all,

as a former SW engineer with some experience in software development
processes I am following this debate with great interest.

Before I joined the Cosy project I had worked for a software company
with 700+ engineers. During this time I worked on quite a few projects,
from smaller to larger, external and internal. That company took (and
still takes) the proper implementation of software development processes
very seriously, partly because it began as a Hewlett Packard contractor,
which means that it had to integrate in their SW development machinery
thus completely inheriting their SW development processes. Later as it
grew larger (and also less HP dependent) the implementation of those
processes was necessary simply because its size, the number of projects
and the fluctuation between them.

One of my last tasks before quitting was to define and transition the
engineering and communication processes between our new partners in
Serbia to which we were outsourcing part of our management information
systems development.

Of course when I joined the Cosy project I was aware that most of the SW
development practices from the industry simply cannot be applied to
research world. But then again considering that engineering seems to be
an important part of this specific project (on which even the research
part relies a lot) for which we use very demanding technologies and is
very distributed, I think that some degree of engineering discipline and
organization is in order. I think a lot of us saw the need for that in
the weeks before the last review meeting. Of course we already missed
the best opportunity for introducing any of such processes, which is at
the beginning of the project, but I think we still can try to do
something (it might even be a useful experience for future projects).

Before I describe some of my concrete proposals let me spend a few words
on testing. Of course testing is a essential part of quality assurance
in any serious software project, but it is also a very costly way of QA
with a lot of requirements that have to be met already in the the
development phase. So perhaps it is not the best place to start when
thinking to improve the engineering quality, especially in our case
since we have a very limited possibilities for doing any serious
testing. Therefore I think the right place to start is the development
itself. I see some points where we could really improve ourselves with
relatively low cost and without seriously hindering the research freedom:

1. Better Version Control (configuration management) Policy

Of course even projects like ours cannot function without some kind
version control system and I think svn does its job very well, but I
think we could exploit better its potential. One of the major problems I
see is that we do not have separate development branches and release
tags. We don't even think in terms of releases (at least most of us),
and if we do, this is not reflected in version control. And if we do not
have that, we cannot even start to think about any kind of serious
testing. How will you tell from our current svn layout, which code is
tested and which is not? On the other hand we are a very distributed
project, often working unaware about what others are doing (sometimes
even within the same SA). Also, we have very different committing
practices: while some people's commits are quite rare, others use svn as
some kind of save option, often saving their daily work untested and
possibly half finished, often full of temporary hacks of very limited
purpose that they tend to forget about. Well in principle there is
noting wrong about different committing practices (as also Nick says in
one of the documents) and nobody is bugproof, the problem is people
updating somebody else's mess to their repositories. What we need is a
"safe svn heaven" that would in principle be functional and consistent
at any time (of course you can never reach perfection), so that people
could rely on. Of course we are not the first project with such
problems, that's why any serious version control system has tags
(labels) and branches.

My proposal obviously is to have separate development branches. They can
be based on SAs, functionality withing the SA, development sites, etc.
Of course this requires that we start to think in terms of releases. I
don't think it is feasible and necessary to have project wide releases,
it is enough to have minirelases within SAs or development site, etc.
Basically they should match the development branches. Releases should
just include short term goals (e.g. doable in two weeks), that somehow
fit together (I do not think this could hinder research freedom much).
After the release is implemented, a beta tag should be made that would
serve as the source for testing (whatever testing policy the site or SA
has). The creation of the beta tag should include two steps:

a) the tag should be created from the trunk (latest revision)

b) the branch code should be merged into tag (while in the most other
version control systems the tags just mark the revisions of individual
files that belong together, in svn there is no  real difference between
branches and tags, other than name, so you can freely commit to a tag)

In this way the release would be tested against the changes in the
trunk. Of course a very wise practice would be to periodically merge the
trunk into development branch (or just when new releases relevant to you
are merged into trunk), which would be basically equivalent to what we
do now by updating our repositories. In any case the development branch
should completely match the trunk at the beginning of each release cycle.

After the the tests are successfully completed, a release tag should be
made. This tag would obviously act as a merging point to the trunk, but
also as an easy way of downgrading your system's components, if
necessary (believe me, searching for the right revision to downgrade a
component to, based on the committing comments can be very time
consuming and painful).

So in this way we would exactly know which code on the repository is
supposed to be safe to use and how can we can safely upgrade or
downgrade our system's components. This system would also act as an
filter for various temporary solutions, hacks, workarounds, debugging
messages etc.,  which can be of course very useful to the developer, but
very annoying to the user.

Of course a very important part of this process is merging. I saw during
my SW developer career that engineers have problems in understanding how
version control works, particularly merging, but I also learned that it
is just enough for most them to know how to commit and update and
perhaps tag. It is even better that in a project more advanced version
control operations are restricted to just one or few engineers. In this
sense I would recommend that on each site there are 1-2 persons with the
permission for committing to their parts of the trunk. These persons
would actually perform all the merges. For some critical components
(e.g. in tools) the restriction could apply on the project level.

2. Defect Tracking System

One of the fundamental principles of testing is that (with the exception
of unit testing) it should be never performed by the developer (coder).
The best thing, if possible, is to have a team of dedicated testers
(that would also define the testcases and take care of the test system),
or at least it should be done by another developer (like I test yours,
you test mine). This means that a way of formalized communication is
necessary between tester and developer. This is best achieved via defect
tracking systems.

While we of course cannot afford  dedicated testers, we can still try to
apply this cross testing (probably just within the various sites). And,
as Henrik already mentioned, we could also exploit the fact that unlike
most of software projects, we are also our own customers. But for both
cases we do need a defect tracking tool. A very suitable solution for
our needs could be Bugzilla (www.bugzilla.org). With the defect tracking
system we could really make bug reporting more efficient. Instead of
writing emailing or skyping the responsible person, you would simply
report the bug to the defect tracking system, to the functional area you
have trouble with. The person responsible for that area would take care
of that bug entry and you would be able to monitor the progress. For the
developer this would mean that he(she) would be able to have all
reported problems regarding his component in one place, less chance of
forgetting about them, overlooking emails, etc. It can also be used as a
means of progress reporting for the development of new features. I
really think we need some kind of formalized communication like this,
Cosy lists are simply not enough.

Ok, in my view with this two points we could quickly and without much
cost improve our performance, without sacrificing the researching
freedom very much. In the same time I also see them as a fundamental
requirements for any kind of serious testing.

*/