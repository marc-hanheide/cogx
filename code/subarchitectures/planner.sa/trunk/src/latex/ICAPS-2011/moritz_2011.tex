

% File: formatting-instruction.tex
\documentclass[letterpaper]{article}
\usepackage{aaai}



\usepackage{times}
\usepackage{helvet}
\usepackage{courier}

\include{macros}

\nocopyright

\pdfinfo{
/Title (Switching in Continual Planning for Practical Robot Control)
/Subject (Proceedings of the 21st International Conference on Automated Planning and Scheduling)
/Author (NA)}
% The file aaai.sty is the style file for AAAI Press 
% proceedings, working notes, and technical reports.
%
%\title{Switching in Continual Planning for Practical Robot Control}
\title{A Continual Planner that Switches Between Serial and
  Decision-Theoretic Planning} 
\author{NA}
\setcounter{secnumdepth}{0}


\begin{document} 
\maketitle

\begin{abstract}

The problem of practical robot control poses many important challenges
to automated planning. 

Planning and plan execution must be robust, seamlessly accommodating
exogenous events and the underlying unpredictability of the
environment. 

Plans with a reasonable chance of exhibiting good behaviours must be
synthesised in a timely manner.

interleaved planning and execution. 

, reason about degrees of belief and uncertainty about the world, 

\end{abstract}

\section{Introduction}

We describe, \pcogx, the planning component in a robotic system that
continuously deliberates in a stochastic dynamic environment in order
to achieve objectives set by the user, and acquire knowledge about its
surroundings.
%%

Have a good understanding of the contents and function of rooms, as
well as the linguistic referrents to rooms, widgets, and their visual
qualities.

Having confidence in its beliefs about the linguistic terms for
spaces, and the function of those spaces.



sometimes difficult to predict, with exogenous events, such as the
changing of the objective, 





the speed and scalability of software for serial planning in
deterministic




We developed a first-order declarative language, called DTPDDL, for
describing domains of planning problems that correspond to POMDPs.  To
accompany that language we have implemented an information-state
\laostar\ procedure for solving problems expressed in DTPDDL. We have
an automated determinisation procedure that generates MAPL
descriptions given DTPDDL input. To accompany that, we have extended
MAPSIM to simulate DTPDDL instances, and modified \fastdownward\ so
that it can find useful serial executions given DTPDDL and MAPL models
of the problem at hand.




 called DTPDDL, along
the lines of PDDL for the partially observable case, an \laostar\
solution procedure, a determinisation of the DTPDDL problem in MAPL,
and modify the \fastdownward\ system to find high quality serial plans
.

We model the environment as a partially observable Markov decision
process. Although planning in that model is undecidable in
general~\cite{}, an optimal finite-horizon plan corresponds to a
contingent plan, that is a function mapping action-observation
histories to actions.

our continual planner is reactive, replanning whenever the underlying
domain and problem models change. For example, replanning occurs if
the motivational component alters the objectives, and if an assumed
outcome of a sensing action is not realised.

brittle sensing model, 

the evaluation of a fluent at a state is either known. Moreover, there
is a sensing process that run-time variables  after-which 

%% For $\prop \in \state$ we say proposition $\prop$ characterises state
%% $\state$. There is always a unique starting state $\state_0$. The goal
%% $\goal$ is a set of propositions, and we say that state $\state$ is a
%% goal state iff $\goal \subseteq \state$.

%%  To keep this exposition
%% simple, for any two distinct actions $\stochAction_i \neq
%% \stochAction_j$, if outcome $\detAct$ is a possibility for
%% $\stochAction_i$ then it cannot also be a possibility for
%% $\stochAction_j$ -- i.e., if $\detAct \in
%% \detActions(\stochAction_i)$ then $\detAct \not\in
%% \detActions(\stochAction_j)$.


%% The solution to a probabilistic planning problem is a contingency
%% plan. This consists of an assignment of actions to states at each
%% discrete timestep up to the planning horizon $n$. The optimal
%% contingency plan is one which prescribes actions to states that
%% maximise the probability that the goal is achieved within $n$ steps
%% from the starting state $\state_0$. For the purposes of this paper
%% we say a plan fails, i.e. achieves the goal with probability $0$, in
%% situations where it does not prescribe an action. Computing the
%% optimal plan for a problem is computationally intractable, and an
%% important direction for research in the field is to develop
%% heuristic mechanisms for generating small linear plans
%% quickly~\cite{littman:etal:98}

An outline of the paper is as follows. We describe POMDPs with a
propositionally factored representation of states and observations,
and describe how to evaluate plans when they correspond to
finite-state controllers.

\section{Propositional Decision-Theoretic Planning}


%% {\em runtime variables}
%% {\em decision variables}
%% {\em uninitialised variables}
%% {\em omitted variables}

We describe the partially observable propositional probabilistic
planning problem, with costs and rewards. The underlying process
dynamics are modelled in terms of a finite set of stochastic actions
$\stochActions$, deterministic outcomes $\detActions$, and
state-characterising propositions $\propositions$.

We model a process state $\state$ as the set of propositions that are
true of the state. Notationally, we have $\state \subseteq
\props$. State change is induced by application of actions. A
stochastic action $\stochAction \in \stochActions$ is applicable if
its precondition $\poss(\stochAction)$, a set of propositions, are
satisfied in the current state -- I.e., $\poss(\stochAction) \subseteq
\state$. We write $\stochActions(\state)$ for set of actions
applicable at state $\state$.  If action $\stochAction \in
\stochActions(\state)$ is applied at \state, nature chooses one
element amongst a small set of deterministic outcomes
$\detActions(\stochAction) \equiv \{\detAction_1, \ldots, \detAction_k
\}$. We denote $\mu_{\stochAction}(\detAction_i)$ the probability that
nature takes outcome $\detAct_i$, and for all \stochAction\ we require
$\sum_{\detAction_i \in \detActions(\stochAction)}
\mu_{\stochAction}(\detAction_i) = 1$. The chosen outcome has an
effect on the state given in terms of two lists of propositions. The
add-list $\add(\detAction)$ and delete-list
$\delete(\detAction)$.\footnote{If a proposition is in the add-list of
$\detAction$, then it cannot be in the delete-list and vice versa.}
If outcome $\detAction$ with $\add(\detAction) := [\prop_1,
..,\prop_n]$ and $\delete(\detAction) := [\prop_1, ..,\prop_m]$ is
chosen by nature when \stochAction\ is applied at state $\state$, then
the resultant state is $ ( \state \cup \add(\detAction) ) \backslash
\delete(\detAction)$ -- i.e., propositions from $\add(\detAction)$ are
added to $\state$, and those from $\delete(\detAction)$ are removed
from $\state$.

We are concerned with problems that feature partial
observability. These have a perceptual model given in terms of a
finite set of stochastic {\em senses} $\stochSenses$, deterministic
sensing outcomes $\detSenses$, and perceptual propositions
$\percepts$, called {\em percepts}. Here, an observation $\observ$ is
a set of percepts $\observ \subseteq \percepts$, and we denote
\observations\ the set of reachable observations. The underlying state
of the process cannot be observed directly, rather, senses
$\stochSense \in \stochSenses$ effect an observation $\observ \in
\observations$ that informs what should be believed about the state a
process is in. In detail, if $\action$ is applied effecting a
transition to a successor state $\state'$, then an observation occurs
according to the active senses $\stochSenses(\stochAction, \state')
\subseteq \stochSenses$. A percept is active, written $\stochSense \in
\stochSenses(\stochAction, \state')$, if the senses'
action-precondition, $\poss_\stochActions(\stochSense)$, is equal to
$\stochAction$, and the state-precondition $\poss_\states(\stochSense)
\subseteq \propositions$ is satisfied by the state $\state'$; In the
usual sense that $\poss_\states(\stochSense) \subseteq \state'$.
%%
When a sense is active, nature must choose exactly one outcome amongst
a small set of deterministic choices $\detSenses(\stochSense)
\equiv \{\detSense_1, \ldots, \detSense_k \}$, so that for each
$i$ we have $\detSense_i \subseteq \percepts$. The probability of
the $i^{th}$ element being chosen is given by
$\psi_{\stochSense}(\detSense_i)$, where $\sum_{\detSense_i \in
\detSenses(\stochSense)} \psi_{\stochSense}(\detSense_i) =
1$. The observation received by the agent corresponds to the union of
perceptual propositions from chosen elements of active senses.

A POMDP has a starting configuration that corresponds to a Bayesian
belief-state. Intuitively, this is the robot's subjective belief about
its environment. Formally, a belief-state $\bstate$ is a probability
distribution over process states. We write $\bstate(\state)$ to denote
the probability that the process is in $\state$ according to
$\bstate$, and $\bstate_0$ when discussing the starting
configuration. 

Finally, we make a number of fairly standard assumptions. First, that
action execution and sensing occurs instantaneously, and that only one
action can be applied at a plan-step. Second, it can arise that in
some propositional states an action is applicable, and in others it is
not. Moreover, a belief-state can assign non-zero probability to
states where that applicability holds, and states where it does
not. We differ
from~\citeauthor{younes:littman:04}~\citeyear{younes:littman:04},
because we forbid the execution of actions that are not applicable in
any state of the current belief.  Also,
unlike~\citeauthor{hoffmann:brafman:2006}~\citeyear{hoffmann:brafman:2006},
we do incorporate a PPDDL-like default semantics, supposing that when
an action \stochAction\ is executed at $\state_i$ where
$\bstate(\state_i) > 0$, that it has no effect on states $\state_i$ if
$\poss(\stochAction) \not\subseteq \state_i$ and $\bstate(\state_i) >
0$.


\subsection{Costs, Rewards, and Plan Evaluation}

Whereas until now we have considered a POMDP model that is factored in
terms of propositions and percepts, in order to discuss utilities and
policies it is convenient to consider the underlying decision process
in a flat format. This underlying decision process is given by the
tuple $\langle \states, \bstate_0, \actions, \transProb, \reward,
\observations, \obsDist \rangle$. Here $\bstate_0$ is the initial
belief-state, \states\ is the finite set of reachable propositional
states, \actions\ is the finite set of actions, and \observations\ is
the finite set of reachable observations (i.e., perceptual states).
Where $\state, \state' \in \states$, $a \in \actions$, from $\mu$ we
have a state transition function $\transProb(\state, \action,
\state')$ giving the probability of a transition from state $s$ to
$s'$ if $a$ is applied. For any $\state$ and $\action$ we have
$\sum_{\state' \in \states} \transProb(\state, \action, \state') = 1$.
%%
Function $\reward:\states \times \actions \to \Re$ is a bounded real
valued reward function.  Consequently there is a positive constant $c$
so that for all $\state \in \states$ and $\action \in \actions$,
$|\reward(\state, \action)| < c$.
%%
From $\psi$ we have that for each $\state \in \states$ and action
$\action \in \actions$, an observation $\observ \in \observations$ is
generated independently according to a probability distribution
$\obsDist(\state, \action)$. We denote $\obsDist_\observ(\state,
\action)$ the probability of getting observation $\observ$ in state
$\state$. For $\state$ and $\action$ we have $\sum_{\observ \in
\observations} \obsDist_\observ(\state, \action) = 1$.

Successive state estimation is available if we applying Bayes' rule.
Taking the current belief $\bstate$ as our {\em prior}, and supposing
action $\action$ is executed with perceptive outcome $\observ$, then
the probability that we are in $\state$ in the successive belief-state
$\bstate'$ is given by:

\begin{equation}\label{eq:revision}
\bstate'(\state) = \frac{\obsDist_\observ(\state, \action)
  \sum_{\state'\in \states} \transProb(\state', \action, \state) \bstate(\state') }{\pp{Pr}(\observ | \action, \bstate)}
\end{equation}

\noindent where $\pp{Pr}(\observ | \action, \bstate)$ is the
normalising factor, that is, the probability of getting observation
$\observ$ given we execute $\action$ in $\bstate$.

A general formalism for representing solutions to POMDPs is the
finite-state controller (FSC). This is a three-tuple $\langle \nodes,
\selection, \transition \rangle$ where: $\node \in \nodes$ is a set of
nodes, $\selection_\node(\action) = P(\action | \node)$, and
$\transition_\node(\action, \observation, \node') = P(\node'|\node,
\action, \observation)$. The value of state $\state$ at node $\node$
given a problem model, written $V_\node(\state)$, is:

\begin{equation}\label{eq:evaluation}
\begin{array}{lcl}
V_\node(\state) & = & \sum_{\action \in \actions}
\selection_\node(\action) \reward(\state, \action) \;\; + \vspace{1ex} \\

&& \hspace{-10ex} \beta \sum_{\action, \observation,
\state', \node'} \transition_\node(\action, \observation, \node')
\transProb(\state, \action, \state') \obsDist_\observ(\state',
\action) V_{\node'}(\state')
\end{array}
\end{equation}

\noindent The value of $\bstate$ according to a controller is
therefore:

\begin{equation} \label{eq:valueBelief}
V_{\pp{FSC}}(\bstate) = \max_{\node \in \nodes} \sum_{\state \in \states} \bstate(\state) V_\node(\state)
\end{equation}





\section{Serial Planning in Determinisations}

Goal expressions in the problem description ---for example
$(\pp{kval}~(\pp{location}~\pp{cornflakes}))$ expresses ``the robot
knows where the {\em cornflakes}--- are treated as control-knowledge
by the planner. Here, a plan is not considered valid unless in the
finial stet the goal condition is satisfied.


\section{Decision-Theoretic PDDL}

The modelling language of choice for planning in probabilistic
problems is the Probabilistic Planning Domain Definition
Language~\cite{younes:littman:04,younes:etal:2005}. PPDDL was used in
all 3 of the International Planning Competitions since 2004. A
variation on PDDL for describing domains with stochastic actions and
uncertain starting configurations, PPDDL is a declarative first-order
(a.k.a., relational) that facilitates factored description of domains
and problem instances. There are straightforward compilations from
problems expressed compactly in PPDDL to propositional representations
amenable to state-of-the-art planning procedures.  Because this
language cannot model domains that feature partial observability, for
our setting we have developed DTPDDL, an extension of PPDDL to partial
observability.


We suppose $\bstate_0$ is given in a factored tree format of the
form:

\def\oom{$^{\tt +}$}
\def\zom{$^*$}
\def\bump{\hspace{1cm}}
\def\req#1{$^{\tt #1}$}
\def\noteme#1{}%{[[#1]]}
\def\notecoauth#1{\ }
\def\notereader#1{[[#1]]}
\def\meta{$\uparrow\uparrow$}
\def\la{\langle}
\def\ra{\rangle}

\scriptsize
\begin{nopagebreak}\begin{tabtt}
\bump \= <observation-def> \bump\=::= \=(:obs\=erve <o-symbol> \+\+\+\+\\
                        %%{[}<extension-def>]  \\
                        :parameters (<typed list (variable)>)  \\
                        <o-def body>) \-\-\- \\
  <o-def body> \> ::= [:state <emptyOr (pre-GD)>] \\
  \> \> [:action <atomic action(term)> ] \\
    \> \> [:effect <emptyOr (o-effect)>] \\
  <o-symbol> \> ::= <name> \\
  <atomic action(t)> \> ::= (<action-symbol> t\zom) \\
  <o-effect> \> ::= (and <c-o-effect>\zom) \\
  <o-effect> \> ::= <c-o-effect> \\
  <c-o-effect> \> ::= \req{:conditional-effects} (when <GD>
                        <o-effect>) \\  
  <c-o-effect> \> ::= \req{:universal-effects} (forall (<typed list (var)>) <o-effect>) \\
  <c-o-effect> \> ::= \req{:probabilistic-effects} (probabilistic
  <prob> <o-effect>) \\
  <c-o-effect> \> ::=  \req{:probabilistic-effects}\req{:universal-unwinding} \\
%% 
\hspace{3ex}(probabilistic (for-each (<typed list (x)>) <s-f-head>  <o-effect> )) \\
  <c-o-effect> \> ::= <p-o-effect> \\
  <atomic o-formula(t)> \> ::= (<observation> t\zom) \\
  <p-o-effect> \> ::= <atomic o-formula(term)> \\
  <p-o-effect> \> ::= (not <atomic o-formula(term)>) \\
  <p-o-effect> \> ::= \req{:fluents}(<assign-op> <o-f-head> <o-f-exp>) \\
  <o-f-comp> \> ::= (<binary-comp> <o-f-exp> <o-f-exp>)\\
  <o-f-exp> \> ::= <number>\\
  <o-f-exp> \> ::= (<binary-op>  <o-f-exp> <o-f-exp>)\\
  <o-f-exp> \> ::= (<multi-op>  <o-f-exp> <o-f-exp>\oom)\\
  <o-f-exp> \> ::= (- <o-f-exp>)\\
  <o-f-exp> \> ::= <o-f-head>\\
  <o-f-head> \> ::= (<o-function-symbol> <term>\zom )\\
  <o-f-head> \> ::= <o-function-symbol>\\
\end{tabtt}\end{nopagebreak}
\noteme{<structure-def> ::= <attach-def>}
\normalsize

%% \small{
%% \begin{tabtt}
%% (:init  (and \=f_{11} f_{12} ..
%%  \> (probabilistic p_{}) 
%%  \> (probabilistic ..) ..) \\
%% \end{tabtt}
%% }

%% \[(:\pp{init} (and f_{11} f_{12} .. (\pp{probabilistic} p_) ))\]

\noindent Nodes of the tree have one of three types: (1) atomic nodes
are labelled with a state proposition, e.g., $(=
(\pp{location}~\pp{cornflakes}) \pp{kitchen})$, (2) probabilistic
nodes, have the form $(r_1, C_1;..;r_n, C_n)$, and (3) constructive
nodes, take the form of a set of probabilistic and atomic nodes. The
root node is conjunctive, and leaves are always atomic. A convenient
way to give semantics to the tree, is to treat nodes as if they were
actions. An atomic node is an action with a positive effect
corresponding to the node label, .
















For example, $(:\pp{predicates} (\pp{location} ?p - VisualObject))$
says that there is a unary predicate 




the effects of a {\em sense} schema are perceptual, whereas the
effects of an {\em operator} schema are over state propositions.


knowledge objectives and  sensory effects. 


probabilistic models of the sensing consequences of acting to
quantitatively capture the unreliability of sensing, and thus reason
precisely about the utility of a plan. 


planning in practical sized POMDPs is intractable. One approach is to
described the task hierarchically, thereby constrain and simplify the
policy search task. The approach ~\cite{}. 






\section{Evaluation}


\section{Discussion}

There have been a number of developments recently towards planning
under uncertainty using systems that were intended for serial planning
in deterministic problems.  Notably,
\system{FFR$_a$}~\cite{yoon:etal:2007}, the winning entry from the
probabilistic track of the 2004 International Planning Competition,
uses the fast satisficing procedure
\system{FF}~\cite{hoffmann:nebel:2001} to compute a serial plan and
corresponding execution trace. The resulting plan is then executed
until the observed trace deviates from the planned trace.

 ---i.e., the values of {\em
runtime variables} in the continual paradigm--- are treated , and
therefore plan for a specific eventuality, replanning from scratch if
something


More computationally expensive approaches in this vein combine
sampling strategies on valuations over {\em runtime variables} with
deterministic planning procedures. The outcome is typically a more
robust serial plan~\cite{yoon:etal:2008}, or contingent
plan~\cite{majercik:2006}.



%%
Recently the system
\system{Conformant-FF}~\cite{hoffmann:brafman:2006}, demonstrated how
conformant planning ---i.e., serial planning in unobservable worlds---
can be modelled as a deterministic problem, and therefore solved using
serial systems. In this conformant setting, advances have been towards
compact representations and lazy evaluations of belief-states.
 

\section{Concluding Remarks}

The switching continual planning system we have described serves as
the underlying planner for CogX. 


\bibliographystyle{aaai}
\bibliography{papers}

\end{document}


























\subsection{PPDDL}

We briefly discuss the description language PPDDL, a fairly
straight-forward extension of the Planning Domain Definition Language
for describing fully observable Markov decision processes.

\small{
\begin{tabtt}
(\= :action look-at-object \\
  \> :parameters \=(?r - robot ?v - visual-object\\
  \> \> ?l - label ?p - place) \\
  \> :precondition (and (= (is-in ?r) ?p) \\
  \> \> (= (label ?o) ?l) ) \\
  \> :effect (and (assign (reward) -3) ) ) \\
\end{tabtt}
}

\small{
\begin{tabtt}
(\= :observe visual-object \\
  \> :parameters \=(?r - robot ?v - visual-object\\
  \> \> ?l - label ?p - place) \\
  \> :execution (look-at-object ?r ?v ?l ?p) \\
  \> :effect (when (= (is-in ?o) ?p) (probabilistic 0.8)) \\
\end{tabtt}
}

\small{
\begin{tabtt}
(\= :sensor look-for-object \\
  \> :agent (?a - robot) \\
  \> :parameters (?l - label ?p - place) \\
  \> :variables (?o - visual-object) \\
  \> :precondition (\=and (= (is-in ?r) ?p) \\
  \> \> (= (label ?o) ?l) (assume (is-in ?o) ?p) ) \\
  \> :effect () \\
  \> :sense (= (is-in ?o) ?p) \\
\end{tabtt}
}

\subsection{MAPL}
