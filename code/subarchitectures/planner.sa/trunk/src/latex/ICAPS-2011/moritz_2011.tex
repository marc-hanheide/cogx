

% File: formatting-instruction.tex
\documentclass[letterpaper]{article}
\usepackage{aaai}



\usepackage{times}
\usepackage{helvet}
\usepackage{courier}

\include{macros}

\nocopyright

\pdfinfo{
/Title (Switching in Continual Planning for Practical Robot Control)
/Subject (Proceedings of the 21st International Conference on Automated Planning and Scheduling)
/Author (NA)}
% The file aaai.sty is the style file for AAAI Press 
% proceedings, working notes, and technical reports.
%
%\title{Switching in Continual Planning for Practical Robot Control}
\title{A Switching Planner for Combined Task and Observation Planning}
%\title{A Continual Planner that Switches Between Serial and
%  Decision-Theoretic Planning} 
\author{NA}
\setcounter{secnumdepth}{0}


\begin{document} 
\maketitle

\begin{abstract}

Realistic robot planning problems in uncertain environments often
require achieving tasks while also finding out about the
world. Because the world state cannot be observed directly, these
problems are naturally represented as partially observable Markov
decision problems (POMDPs). However, these are typically intractable
for realistic problems.

We present a \emph{switching planner} that employs fast sequential
planning to decide on the overall strategy, and uses a
decision-theoretic planner to solve the subproblems where partial
observability will significantly impact the quality of the plan. We
demonstrate the approach in a realistic robot exploration domain.

\end{abstract}

\section{Introduction}


Any (PO)MDP with stochastic actions can be compiled into an equivalent
deterministic-action POMDP with all the action uncertainty expressed
in the starting-state distribution~\cite{ng:Jordan:2000}. In our
setting, that of finite-horizon planning, this compilation yields a
finite flat representation of the POMDP. Therefore, although our
current implementation does not explicitly support it, our approach
generalises to POMDPs with stochastic action models.




We describe, \pcogx, the planning component in a robotic system that
continuously deliberates in a stochastic dynamic environment in order
to achieve objectives set by the user, and acquire knowledge about its
surroundings.
%%

Have a good understanding of the contents and function of rooms, as
well as the linguistic referrents to rooms, widgets, and their visual
qualities.

Having confidence in its beliefs about the linguistic terms for
spaces, and the function of those spaces.



sometimes difficult to predict, with exogenous events, such as the
changing of the objective, 





the speed and scalability of software for serial planning in
deterministic




We developed a first-order declarative language, called DTPDDL, for
describing domains of planning problems that correspond to POMDPs.  To
accompany that language we have implemented an information-state
\laostar\ procedure for solving problems expressed in DTPDDL. 

We have extended MAPSIM to simulate DTPDDL instances, and modified
\fastdownward\ so that it can find useful serial executions given
DTPDDL models of the problem at hand.




 called DTPDDL, along
the lines of PDDL for the partially observable case, an \laostar\
solution procedure, a determinisation of the DTPDDL problem in MAPL,
and modify the \fastdownward\ system to find high quality serial plans
.

We model the environment as a partially observable Markov decision
process. Although planning in that model is undecidable in
general~\cite{}, an optimal finite-horizon plan corresponds to a
contingent plan, that is a function mapping action-observation
histories to actions.

our continual planner is reactive, replanning whenever the underlying
domain and problem models change. For example, replanning occurs if
the motivational component alters the objectives, and if an assumed
outcome of a sensing action is not realised.

brittle sensing model, 

the evaluation of a fluent at a state is either known. Moreover, there
is a sensing process that run-time variables  after-which 

%% For $\prop \in \state$ we say proposition $\prop$ characterises state
%% $\state$. There is always a unique starting state $\state_0$. The goal
%% $\goal$ is a set of propositions, and we say that state $\state$ is a
%% goal state iff $\goal \subseteq \state$.

%%  To keep this exposition
%% simple, for any two distinct actions $\stochAction_i \neq
%% \stochAction_j$, if outcome $\detAct$ is a possibility for
%% $\stochAction_i$ then it cannot also be a possibility for
%% $\stochAction_j$ -- i.e., if $\detAct \in
%% \detActions(\stochAction_i)$ then $\detAct \not\in
%% \detActions(\stochAction_j)$.


%% The solution to a probabilistic planning problem is a contingency
%% plan. This consists of an assignment of actions to states at each
%% discrete timestep up to the planning horizon $n$. The optimal
%% contingency plan is one which prescribes actions to states that
%% maximise the probability that the goal is achieved within $n$ steps
%% from the starting state $\state_0$. For the purposes of this paper
%% we say a plan fails, i.e. achieves the goal with probability $0$, in
%% situations where it does not prescribe an action. Computing the
%% optimal plan for a problem is computationally intractable, and an
%% important direction for research in the field is to develop
%% heuristic mechanisms for generating small linear plans
%% quickly~\cite{littman:etal:98}

An outline of the paper is as follows. We describe POMDPs with a
propositionally factored representation of states and observations,
and describe how to evaluate plans when they correspond to
finite-state controllers.

\section{Switching Planner Outline}

Outline the high level approach here:
\begin{itemize}
\item Problem input (factored initial state, DTPDDL observations)
\item Determinisation
\item Problem generation for DT
\item Belief revision
\end{itemize}

\section{Determinisation}

\begin{itemize}
\item Generation of deterministic sensing models
\item Determinisation of the initial state
\item Compiling reward model into (soft-)goal model
\item Cost function in FD (maybe this could mentioned in the outline?)
\end{itemize}

\section{Subtask Generation}
\begin{itemize}
\item Goal generation from CP trace
\item Disconfirm actions
\item State pruning according to CP trace
\end{itemize}

\section{Belief revision}
Not sure if this needs its own section
\begin{itemize}
\item Convert inital state into bayesian network
\item Belief updates with observation nodes
\item Create new network for next planning runs
\end{itemize}

\section{Propositional Decision-Theoretic Planning}


%% {\em runtime variables}
%% {\em decision variables}
%% {\em uninitialised variables}
%% {\em omitted variables}

We describe the partially observable propositional probabilistic
planning problem, with costs and rewards. The underlying process
dynamics are modelled in terms of a finite set of stochastic actions
$\stochActions$, deterministic outcomes $\detActions$, and
state-characterising propositions $\propositions$.

We model a process state $\state$ as the set of propositions that are
true of the state. Notationally, we have $\state \subseteq
\props$. State change is induced by application of actions. A
stochastic action $\stochAction \in \stochActions$ is applicable if
its precondition $\poss(\stochAction)$, a set of propositions, are
satisfied in the current state -- I.e., $\poss(\stochAction) \subseteq
\state$. We write $\stochActions(\state)$ for set of actions
applicable at state $\state$.  If action $\stochAction \in
\stochActions(\state)$ is applied at \state, nature chooses one
element amongst a small set of deterministic outcomes
$\detActions(\stochAction) \equiv \{\detAction_1, \ldots, \detAction_k
\}$. We denote $\mu_{\stochAction}(\detAction_i)$ the probability that
nature takes outcome $\detAct_i$, and for all \stochAction\ we require
$\sum_{\detAction_i \in \detActions(\stochAction)}
\mu_{\stochAction}(\detAction_i) = 1$. The chosen outcome has an
effect on the state given in terms of two lists of propositions. The
add-list $\add(\detAction)$ and delete-list
$\delete(\detAction)$.\footnote{If a proposition is in the add-list of
$\detAction$, then it cannot be in the delete-list and vice versa.}
If outcome $\detAction$ with $\add(\detAction) := [\prop_1,
..,\prop_n]$ and $\delete(\detAction) := [\prop_1, ..,\prop_m]$ is
chosen by nature when \stochAction\ is applied at state $\state$, then
the resultant state is $ ( \state \cup \add(\detAction) ) \backslash
\delete(\detAction)$ -- i.e., propositions from $\add(\detAction)$ are
added to $\state$, and those from $\delete(\detAction)$ are removed
from $\state$.

We are concerned with problems that feature partial
observability. These have a perceptual model given in terms of a
finite set of stochastic {\em senses} $\stochSenses$, deterministic
sensing outcomes $\detSenses$, and perceptual propositions
$\percepts$, called {\em percepts}. Here, an observation $\observ$ is
a set of percepts $\observ \subseteq \percepts$, and we denote
\observations\ the set of reachable observations. The underlying state
of the process cannot be observed directly, rather, senses
$\stochSense \in \stochSenses$ effect an observation $\observ \in
\observations$ that informs what should be believed about the state a
process is in. In detail, if $\action$ is applied effecting a
transition to a successor state $\state'$, then an observation occurs
according to the active senses $\stochSenses(\stochAction, \state')
\subseteq \stochSenses$. A percept is active, written $\stochSense \in
\stochSenses(\stochAction, \state')$, if the senses'
action-precondition, $\poss_\stochActions(\stochSense)$, is equal to
$\stochAction$, and the state-precondition $\poss_\states(\stochSense)
\subseteq \propositions$ is satisfied by the state $\state'$; In the
usual sense that $\poss_\states(\stochSense) \subseteq \state'$.
%%
When a sense is active, nature must choose exactly one outcome amongst
a small set of deterministic choices $\detSenses(\stochSense)
\equiv \{\detSense_1, \ldots, \detSense_k \}$, so that for each
$i$ we have $\detSense_i \subseteq \percepts$. The probability of
the $i^{th}$ element being chosen is given by
$\psi_{\stochSense}(\detSense_i)$, where $\sum_{\detSense_i \in
\detSenses(\stochSense)} \psi_{\stochSense}(\detSense_i) =
1$. The observation received by the agent corresponds to the union of
perceptual propositions from chosen elements of active senses.

A POMDP has a starting configuration that corresponds to a Bayesian
belief-state. Intuitively, this is the robot's subjective belief about
its environment. Formally, a belief-state $\bstate$ is a probability
distribution over process states. We write $\bstate(\state)$ to denote
the probability that the process is in $\state$ according to
$\bstate$, and $\bstate_0$ when discussing the starting
configuration. 

Finally, we make a number of fairly standard assumptions. First, that
action execution and sensing occurs instantaneously, and that only one
action can be applied at a plan-step. Second, it can arise that in
some propositional states an action is applicable, and in others it is
not. Moreover, a belief-state can assign non-zero probability to
states where that applicability holds, and states where it does
not. We differ
from~\citeauthor{younes:littman:04}~(\citeyear{younes:littman:04}),
because we forbid the execution of actions that are not applicable in
any state of the current belief.  Also,
unlike~\citeauthor{hoffmann:brafman:2006}~(\citeyear{hoffmann:brafman:2006}),
we do incorporate a PPDDL-like default semantics, supposing that when
an action \stochAction\ is executed at $\state_i$ where
$\bstate(\state_i) > 0$, that it has no effect on states $\state_i$ if
$\poss(\stochAction) \not\subseteq \state_i$ and $\bstate(\state_i) >
0$.


\subsection{Costs, Rewards, and Plan Evaluation}

Whereas until now we have considered a POMDP model that is factored in
terms of propositions and percepts, in order to discuss utilities and
policies it is convenient to consider the underlying decision process
in a flat format. This underlying decision process is given by the
tuple $\langle \states, \bstate_0, \actions, \transProb, \reward,
\observations, \obsDist \rangle$. Here $\bstate_0$ is the initial
belief-state, \states\ is the finite set of reachable propositional
states, \actions\ is the finite set of actions, and \observations\ is
the finite set of reachable observations (i.e., perceptual states).
Where $\state, \state' \in \states$, $a \in \actions$, from $\mu$ we
have a state transition function $\transProb(\state, \action,
\state')$ giving the probability of a transition from state $s$ to
$s'$ if $a$ is applied. For any $\state$ and $\action$ we have
$\sum_{\state' \in \states} \transProb(\state, \action, \state') = 1$.
%%
Function $\reward:\states \times \actions \to \Re$ is a bounded real
valued reward function.  Consequently there is a positive constant $c$
so that for all $\state \in \states$ and $\action \in \actions$,
$|\reward(\state, \action)| < c$.
%%
From $\psi$ we have that for each $\state \in \states$ and action
$\action \in \actions$, an observation $\observ \in \observations$ is
generated independently according to a probability distribution
$\obsDist(\state, \action)$. We denote $\obsDist_\observ(\state,
\action)$ the probability of getting observation $\observ$ in state
$\state$. For $\state$ and $\action$ we have $\sum_{\observ \in
\observations} \obsDist_\observ(\state, \action) = 1$.

Successive state estimation is available if we applying Bayes' rule.
Taking the current belief $\bstate$ as our {\em prior}, and supposing
action $\action$ is executed with perceptive outcome $\observ$, then
the probability that we are in $\state$ in the successive belief-state
$\bstate'$ is given by:

\begin{equation}\label{eq:revision}
\bstate'(\state) = \frac{\obsDist_\observ(\state, \action)
  \sum_{\state'\in \states} \transProb(\state', \action, \state) \bstate(\state') }{\pp{Pr}(\observ | \action, \bstate)}
\end{equation}

\noindent where $\pp{Pr}(\observ | \action, \bstate)$ is the
normalising factor, that is, the probability of getting observation
$\observ$ given we execute $\action$ in $\bstate$.

A general formalism for representing solutions to POMDPs is the
finite-state controller (FSC). This is a three-tuple $\langle \nodes,
\selection, \transition \rangle$ where: $\node \in \nodes$ is a set of
nodes, $\selection_\node(\action) = P(\action | \node)$, and
$\transition_\node(\action, \observation, \node') = P(\node'|\node,
\action, \observation)$. The value of state $\state$ at node $\node$
given a problem model, written $V_\node(\state)$, is:

\begin{equation}\label{eq:evaluation}
\begin{array}{lcl}
V_\node(\state) & = & \sum_{\action \in \actions}
\selection_\node(\action) \reward(\state, \action) \;\; + \vspace{1ex} \\

&& \hspace{-10ex} \beta \sum_{\action, \observation,
\state', \node'} \transition_\node(\action, \observation, \node')
\transProb(\state, \action, \state') \obsDist_\observ(\state',
\action) V_{\node'}(\state')
\end{array}
\end{equation}

\noindent The value of $\bstate$ according to a controller is
therefore:

\begin{equation} \label{eq:valueBelief}
V_{\pp{FSC}}(\bstate) = \max_{\node \in \nodes} \sum_{\state \in \states} \bstate(\state) V_\node(\state)
\end{equation}





\section{Serial Planning in Determinisations}

Goal expressions in the problem description ---for example
$(\pp{kval}~(\pp{location}~\pp{cornflakes}))$ expresses ``the robot
knows where the {\em cornflakes}--- are treated as control-knowledge
by the planner. Here, a plan is not considered valid unless in the
finial stet the goal condition is satisfied.


\section{Decision-Theoretic PDDL}

The modelling language of choice for planning in probabilistic
problems is the Probabilistic Planning Domain Definition
Language~\cite{younes:littman:04,younes:etal:2005}. PPDDL was used in
all 3 of the International Planning Competitions since 2004. A
variation on PDDL for describing domains with stochastic actions and
uncertain starting configurations, PPDDL is a declarative first-order
(a.k.a., relational) language that facilitates factored description of
domains and problem instances. There are straightforward compilations
from problems expressed compactly in PPDDL to propositional
representations amenable to state-of-the-art planning procedures.
Because this language cannot model domains that feature partial
observability, for our setting we have developed DTPDDL, an extension
of PPDDL to partial observability.


Our mobile robot is able to look for a visual object, for example a
box of cornflakes, at a given place. We can model that deterministic
action using the following schema:


\def\oom{$^{\tt +}$}
\def\zom{$^*$}
\def\bump{\hspace{1cm}}
\def\req#1{$^{\tt #1}$}
\def\noteme#1{}%{[[#1]]}
\def\notecoauth#1{\ }
\def\notereader#1{[[#1]]}
\def\meta{$\uparrow\uparrow$}
\def\la{\langle}
\def\ra{\rangle}

\small{
\begin{tabtt}
(\= :action look-for-object \\
  \> :parameters \=(?r - robot ?v - visual-object\\
  \> \> ?l - location) \\
  \> :precondition (and (= (is-in ?r) ?l) ) \\
  \> :effect (and (assign (reward) -3) ) ) \\
\end{tabtt}
}

\noindent That is, the robot can always look for a visual object at
its current location. The only effect this has on the state, is that
the robot incurs a penalty, $3$, that corresponds to the {\em cost} of
performing a visual search. Instances of this operator are useful,
because their application activates an the following sense schema:

\small{
\begin{tabtt}
(\= :sense vision \\
  \> :parameters \=(?r - robot ?v - visual-object\\
  \> \> ?l - location) \\
  \> :execution (look-for-object ?r ?v ?l) \\
  \> :effect ( and (when (= (is-in ?v) ?l) \\
  \> \bump (probabilistic 0.8 \\
  \> \bump (= (observe-is-in ?v) ?l))) \\
  \> (when (not (= (is-in ?v) ?l)) \\
  \> \bump (probabilistic 0.1 \\
  \> \bump (= (observe-is-in ?v) ?l))))) \\
\end{tabtt}
}

\noindent Here, the {\em execution} clause is a lifted description of
the sense action-precondition -- I.e,
$(\pp{look-for-object}~\pp{R2D2}~\pp{box}~\pp{desk}) =
\poss_\stochActions(\pp{vision}~\pp{R2D2}~\pp{box}~\pp{desk})$. No
other conditions guard the activation of this sense, therefore if
$(\pp{look-for-object} \pp{Dora} \pp{cornflakes} \pp{bedroom})$ is
executed, we suppose there is a $0.8$ chance of observing the
cornflakes if they are in the $\pp{bedroom}$, and otherwise a $0.1$ of
observing them.\footnote{We have simplified the schemata for
illustrative purposes. In practice, the probabilities of a particular
observation here should be parametrised by the type of visual object
the robot is searching for, and category of the location.}




\scriptsize
\begin{nopagebreak}\begin{tabtt}
<observation-def> \=::= \=(:sense <observation symbol> \\
                        \> \> :parameters (<typed list (variable)>)  \\
                        \> \> <o-def body>) \\
  <o-symbol> \> ::= <name> \\
  <o-def body> \> ::= [:precondition <GD>] \\
  \> \> [:execution <atomic action(term)> ] \\
    \> \> [:effect <o-effect>] \\
  <atomic action(t)> \> ::= (<action symbol> t\zom) \\
  <o-effect> \> ::= (and <c-o-effect>\zom) \\
  <o-effect> \> ::= <c-o-effect> \\
  <c-o-effect> \> ::= \req{:probabilistic-effects} (probabilistic <prob> <o-effect>) \\
  <c-o-effect> \> ::= <p-o-effect> \\
  <atomic o-formula(t)> \> ::= (<observation> t\zom) \\
  <p-o-effect> \> ::= <atomic o-formula(term)> \\
  <p-o-effect> \> ::= (not <atomic o-formula(term)>) \\
  <o-f-comp> \> ::= (<binary-comp> <o-f-exp> <o-f-exp>)\\
  <o-f-exp> \> ::= <number>\\
  <o-f-exp> \> ::= (- <o-f-exp>)\\
  <o-f-exp> \> ::= <o-f-head>\\
  <o-f-head> \> ::= (<o-function-symbol> <term>\zom )\\
  <o-f-head> \> ::= <o-function-symbol>\\
\end{tabtt}\end{nopagebreak}
\noteme{<structure-def> ::= <attach-def>}
\normalsize



We suppose $\bstate_0$ is given in a factored tree format of the
form:


%% \small{
%% \begin{tabtt}
%% (:init  (and \=f_{11} f_{12} ..
%%  \> (probabilistic p_{}) 
%%  \> (probabilistic ..) ..) \\
%% \end{tabtt}
%% }

%% \[(:\pp{init} (and f_{11} f_{12} .. (\pp{probabilistic} p_) ))\]

\noindent Nodes of the tree have one of three types: (1) atomic nodes
are labelled with a state proposition, e.g., $(=
(\pp{location}~\pp{cornflakes}) \pp{kitchen})$, (2) probabilistic
nodes, have the form $(r_1, C_1;..;r_n, C_n)$, and (3) constructive
nodes, take the form of a set of probabilistic and atomic nodes. The
root node is conjunctive, and leaves are always atomic. A convenient
way to give semantics to the tree, is to treat nodes as if they were
actions. An atomic node is an action with a positive effect
corresponding to the node label, .
















For example, $(:\pp{predicates} (\pp{location} ?l - VisualObject))$
says that there is a unary predicate 




the effects of a {\em sense} schema are perceptual, whereas the
effects of an {\em operator} schema are over state propositions.


knowledge objectives and  sensory effects. 


probabilistic models of the sensing consequences of acting to
quantitatively capture the unreliability of sensing, and thus reason
precisely about the utility of a plan. 


planning in practical sized POMDPs is intractable. One approach is to
described the task hierarchically, thereby constrain and simplify the
policy search task. The approach ~\cite{}. 

\section{Evaluation}


\section{Discussion}

There have been a number of developments recently towards planning
under uncertainty using systems that were intended for serial planning
in deterministic problems.  Notably,
\system{FFR$_a$}~\cite{yoon:etal:2007}, the winning entry from the
probabilistic track of the 2004 International Planning Competition,
uses the fast satisficing procedure
\system{FF}~\cite{hoffmann:nebel:2001} to compute a serial plan and
corresponding execution trace. The resulting plan is then executed
until the observed trace deviates from the planned trace.

 ---i.e., the values of {\em
runtime variables} in the continual paradigm--- are treated , and
therefore plan for a specific eventuality, replanning from scratch if
something


More computationally expensive approaches in this vein combine
sampling strategies on valuations over {\em runtime variables} with
deterministic planning procedures. The outcome is typically a more
robust serial plan~\cite{yoon:etal:2008}, or contingent
plan~\cite{majercik:2006}.



%%
Recently the systems
\system{Conformant-FF}~\cite{hoffmann:brafman:2006} and
$T_0$~\cite{palacios:geffner:2009}, demonstrated how conformant
planning ---i.e., serial planning in unobservable worlds--- can be
modelled as a deterministic problem, and therefore solved using serial
systems. In this conformant setting, advances have been towards
compact representations and lazy evaluations of beliefs.
 

\section{Concluding Remarks}

The switching continual planning system we have described serves as
the underlying planner for CogX. 





From an automated planning perspective, the problem of practical
mobile robot control poses important and contrary challenges.
%%
On the one hand, planning and execution monitoring must be
lightweight, robust, timely, and should span the lifetime of the
robot. Those processes must seamlessly accommodate exogenous events,
changing objectives, and the underlying unpredictability of the
environment.
%%
On the other hand, robot planning should perform computationally
expensive reasoning about contingencies, and possible revisions of
subjective belief according to quantitatively modelled uncertainty in
acting and sensing. 

In this paper we address these challenges, developing a continual
planner that switches between sequential and decision-theoretic
planning. Given a POMDP model of the environment, serial planning is
responsible for an initial deterministic sequential plan and
complementary runtime evolution of the decision process. That plan is
executed until an assumption about a runtime proposition

posted by a motivational component of the underlying robotic
architecture. 

contingent sensory plans that are tailored to current
objectives.

In this paper we present an approach to continual planing that uses
two planning systems. The first 

 to a distinct class of
challenges. We suppose 
%%
The underlying environment is modelled as a POMDP. We use the fast
classical satisficing system FastDownward to find a deterministic
sequential plan and complementary runtime evolution of that
process. This corresponds to a generalisation of replanning in
probabilistic planning to problems with partial observability.

Interaction between the serial planner and execution proceeds
more-or-less analogously to popular replanning approaches

Addressing these challenges in a monolithic framework, we present a
{\em switching} continual planner, that uses the fast serial
satisficing procedure FastDownward to perform net-benefit
%%
makes reasonable assumptions about the evolution of the runtime state
given a POMDP model of the environment. 

contingent sensory plans that are tailored to current
objectives.


The decision-theoretic planner is able to tailor sensory processing on
a robot platform to the current objective, while FastDownward  quickly 


In this paper we develop a continual planing system that uses two
planning systems. The first, is a state-of-the-art domain independent
planner for deterministic problems. The second is a information-state
contingency planning the information-state space of 


Continual planning is a powerful technique that goes some way to
addressing those challenges. That approach interleaves planning and
execution, deliberately postponing planing for contingencies unless
they eventuate during execution. 


computing a single serial plan and
eventuality



To these challenges it seems a continual planning approach is best, 

where the runtime state evolves during plan execution 


interleaved planning and execution. 

the latter is able to tailor sensory processing on a robot platform,
in order that it.

{\em ad-hoc} 

, reason about degrees of belief and uncertainty about the world, 

probabilistic sequential decision making in practical sized problems
is intractable.

quantitative probabilistic models of the perception and action .


\bibliographystyle{aaai}
\bibliography{papers}

\end{document}


























\subsection{PPDDL}

We briefly discuss the description language PPDDL, a fairly
straight-forward extension of the Planning Domain Definition Language
for describing fully observable Markov decision processes.

\small{
\begin{tabtt}
(\= :action look-at-object \\
  \> :parameters \=(?r - robot ?v - visual-object\\
  \> \> ?l - label ?l - location) \\
  \> :precondition (and (= (is-in ?r) ?l) \\
  \> \> (= (label ?o) ?l) ) \\
  \> :effect (and (assign (reward) -3) ) ) \\
\end{tabtt}
}

\small{
\begin{tabtt}
(\= :observe visual-object \\
  \> :parameters \=(?r - robot ?v - visual-object\\
  \> \> ?l - label ?l - location) \\
  \> :execution (look-at-object ?r ?v ?l ?l) \\
  \> :effect (when (= (is-in ?o) ?l) (probabilistic 0.8)) \\
\end{tabtt}
}

\small{
\begin{tabtt}
(\= :sensor look-for-object \\
  \> :agent (?a - robot) \\
  \> :parameters (?l - label ?l - location) \\
  \> :variables (?o - visual-object) \\
  \> :precondition (\=and (= (is-in ?r) ?l) \\
  \> \> (= (label ?o) ?l) (assume (is-in ?o) ?l) ) \\
  \> :effect () \\
  \> :sense (= (is-in ?o) ?l) \\
\end{tabtt}
}

\subsection{MAPL}
