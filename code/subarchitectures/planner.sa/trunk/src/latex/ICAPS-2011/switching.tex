

We now describe a continual planning approach that switches between
sequential and decision-theoretic planning sessions. 


In this work, the problem at hand is a POMDP described in DTPDDL. From
hereon, we restrict our attention to POMDPs with deterministic
actions. Therefore it is worthwhile noting that any POMDP with
stochastic actions can be compiled into an equivalent
deterministic-action POMDP, where all the original action uncertainty
is expressed in the starting-state
distribution~\cite{ng:Jordan:2000}. In our setting, that of
finite-horizon planning, such a compilation yields a finite flat
representation of the original POMDP, only with deterministic
actions. 


PLEASE STICK A PARAGRAPH HERE, (SENSIBLY) REQUESTED BY MORITZ, ABOUT
HOW WE DO BELIEF REVISION ACROSS SWITCHING.

\Omit{
  Therefore, although our current implementation does not
  explicitly support it, our approach generalises to POMDPs with
  stochastic action models.
}

\subsection{Sequential Planning in POMDPs}


Our approach uses sequential planning to compute a linear plan, and
one trace of that plan's execution. The latter encapsulates
assumptions about: (1) the true underlying state, (2) how the plan's
execution will progress, and (3) the possibility of the agent
eventually holding strong beliefs about the truth values of specific
state propositions. Sequential planning proceeds in a continual manner
until the verification of an assumption about a runtime state
proposition is scheduled. At that point the system switches to a
decision-theoretic planner that is used to tailor sensory processing
by the robot to test the validity of the assumptions made so far.

In a deterministic-action POMDP, all the uncertainty in state and
action is expressed in the $(\pp{:init})$ declaration. Our approach
uses the structure of that, as it occurs in the problem description,
do define a set of state-assumptions available to sequential planning.
%%
Writing \#\ if the value of a proposition is unspecified in an
assumption, for our $\pp{DORA}$ example we have that the following
assumptions are available:

\small
\begin{tabular}{cccc}
\hline
Probability & (is-in DORA)  & (is-in box)  & (is-in cup) \\
\hline
.24 & kitchen & office & office \\
.06 & kitchen & kitchen & office \\
.56 & kitchen & office & kitchen \\
.14 & kitchen & kitchen & kitchen \\
.7 & kitchen & \# &  kitchen\\
.3 & kitchen & \# & office \\
.8 & kitchen & office & \# \\
.2 & kitchen & kitchen & \# \\
1.0 & kitchen & \# & \# \\
\hline
\end{tabular}
\normalsize

\noindent A valid assumption in our setting corresponds to a {\em
relaxed} visitation of the root term of $(:init)$. In this relaxed
case, a conjunctive term is visited iff its atomic subterms are
visited, and zero or more of its immediate probabilistic subterms are
visited. The starting state, $\state_0$, for sequential planning is
the unique abstract state with probability $1.0$ -- E.G., for
$\pp{DORA}$ $s_0 \equiv \{(= (is-in DORA) kitchen)\}$.

In order to make assumptions available to the sequential planner, we
add one {\em assumptive} action $\assumptiveS{i}$ to the problem for
each element, $p_i (T_i)$, of each probabilistic term from
$(:init)$. The physics of these actions is as
follows. $\assumptiveS(i)$ can be executed if no $\assumptiveS(j)$,
$j \neq i$, has been executed from the same probabilistic term, and,
either $(\pp{probabilistic}~..p_i~(T_i)..)$ is in the root conjunct,
or it occurs in $T_k$ for some executed $\assumptiveS(k)$.
%%
Executing $\assumptiveS(i)$ in $s$ effects a transition to a
successor state $s^{T_i}$ with probability $p_i$, and $s^\bot$ with
probability $1 - p_i$. Here, $s^{T_i}$ is the union of $s$ and atomic
terms from $T_i$, and $s^\bot$ is a sink state -- I.e.,
$\forall\action\in\actions$ $\reward(\state^\bot, \action) = 0$ and
$\transProb(\state^\bot, \action, \state^\bot) = 1$.

Because $\state_0$ is abstract, and because actions can have
preconditions that are not uniformly true in the underlying
(resp. abstract) belief-state, we must make two further adjustments to
the problem posed to the sequential planner. First, if
$\action\in\actions$ has a precondition that is not true with
probability $1$, then before it is executed blindly, the sequential
continual planning switches to a decision-theoretic session, discussed in
detail below. Overall, by scheduling $\action$ the sequential planner
makes a naive assumption about the possibility of the agent eventually
``committing'' to the truth values of specific state
propositions. Thus, the role of decision-theoretic plans is to verify
the validity of assumptions about the runtime state in sequential
planning. Second, if the truth value of $\prop$ is not known, and no
assumption is made about it in the current abstract state, then if
$\prop \in
\poss(\action)$ we have that action $\action$ is not applicable.

%% Because $\state_0$ is abstract, and because actions can have
%% preconditions that are not uniformly true in the underlying
%% (resp. abstract) belief-state, we must make two further adjustments to
%% the problem posed to the sequential planner. First, if
%% $\action\in\actions$ has a precondition that is not true with
%% probability $1$, then before it is executed blindly, the sequential
%% continual planning switches to a decision-theoretic mode, discussed in
%% detail below. Overall, by scheduling $\action$ the sequential planner
%% makes a naive assumption about the possibility of the agent eventually
%% committing ---for the purpose of the current task--- to the truth
%% values of specific state propositions. Second, if the truth value of
%% $\prop$ is not known, and no assumption is made about it in the
%% current abstract state, then if $\prop \in \poss(\action)$ we have
%% that action $\action$ is not applicable.

%% In other words, the value of a proposition in an abstract
%% state is either {\em true}, {\em false}, or {\em unspecified} (i.e.,
%% \#). In the last case actions whose execution is preconditioned on the
%% term being truth functional are not available.


Finally, we come to valuing sequential plans, and therefore giving a
criteria for the sequential planner to optimise. Where $p_i$ is the
probability that the $i^{th}$ sequenced action does not transition to
$s^\bot$, we value a trace as follows:

\begin{equation}\label{eq:tracevalue}
V(s_0, a_0, s_1, a_1,.., s_N) =  \prod_{i=1..N-1} p_i \sum_{i=1..N-1} \reward(\state_i, \action_i)
\end{equation}

\noindent The optimal trace given a plan, i.e., the non-assumptive
actions in sequence, therefore has value:

\[
V^* = \max_{p_1, .., p_n} \prod_{i=1..N-1} p_i \sum_{i=1..N-1}
\reward(\state_i, \action_i),
\]

\noindent which is equal to the maximal contribution a trace of the plan can
make to Eq~\ref{eq:expectedvalue}. In other words, writing $V_i$ for
the value of the $i^{th}$ trace of the plan expressed in $s_0, a_0,
s_1, a_1,.., s_N$, we have

\[
V^* = \arg\min_{V_i} \Expect \bigg{[} 
\sum_{t=0}^{N-1}  \reward(\bstate_t, \pp{PLAN}_t) \mid \pp{PLAN}, \bstate_0
\bigg{]} - V_i.
\]

In goal directed problems, where the only non-zero reward is received
at the first transition to a goal state, the Eq~\ref{eq:tracevalue}
criteria corresponds exactly to that of the infamous degenerate
hindsight system
\system{FFR$_a$}~\cite{yoon:etal:2007}. Form the perspective of
gradient-based reinforcement learning systems, such as William's
REINFORCE and GPOMDP variants for planning~\cite{olivier:doug:2009},
under reasonable assumptions the optimal trace identifies a maximal
gradient step from a uniformly random soft-max policy. Finally, it is
worth clarifying that although the above discussion has been in terms
of a trace of length $N$, there is nothing in our approach that
artificially limits the length of candidate sequential plans.


\subsection{Decision-Theoretic Planning in Abstractions}

In a decision-theoretic session, the switching planner solves a sensing
problem in an abstract processes defined in terms of the assumptive
actions in the current sequential plan, and the $(\pp{:init})$ block
from the original problem description. 

The focus for decision-theoretic planning is on sensing, rather than
the achievement of utility according to the given objectives. We
therefor give the process in this session a different reward model, which
reflects the value of performing sensing actions in the context of the
trace proposed for the preceding sequential deliberation. In detail,
all positive rewards from the initial POMDP are removed. Then, for
each $\assumptiveS{i}$ action scheduled in the current sequential
plan, we have a {\em dual} $\assumptiveDT{i}$ so that
$\forall\state\in\states$:

\[
\reward(\state, \assumptiveDT{i}) = \bigg\{ \begin{array}{ll}
\$(T_i) & \pp{if}~\;\;T_i \not\subseteq \state \\
-\$(T_i) & \pp{otherwise} \\
\end{array}
\]

\noindent Where $\$(T_i)$ is a positive numeric quantity which
captures the reward the agent receives for correctly jettisoning a
false assumption. The added dual actions are propositionally trivial
-- I.e., $\forall\state\in\states$ $\poss(\assumptiveDT{i})
\subseteq \state$ and $\assumptiveDT{i}(\state) \equiv \state$.  Also, for the action,
$\action_i$, whose scheduled execution triggered the switch to the
decision-theoretic session, we include another propositionally trivial
action $\actions.\poss(\action_i)$ with the reward property:

\[
\reward(\state, \actions.\poss(\action_i)) = \bigg\{ \begin{array}{ll}
\$(\poss(\action_i)) & \pp{if}~\;\; \poss(\action_i) \subseteq \state \\
-\$(\poss(\action_i)) & \pp{otherwise} \\
\end{array}
\]


For the management of switching, execution of either a
$\assumptiveDT{i}$ action, or $\actions.\poss(\action_i)$, returns
control to the sequential planner. At that point, if a dual is
executed, then the sequential planner must replan with $\bstate_0$
equal to the current belief. Otherwise, if $\actions.\poss(\action_i)$
is executed, then the current sequential plan is executed until
further sensing is scheduled, or to completion. In our project the
rewards, i.e., function $\$:2^\propositions\to\Re$, for the POMDP
posed to decision-theoretic planning are sourced from a motivational
component of our robotic architecture.

The starting belief-state for the abstract process is given in terms
of a {\em relaxed} $(\pp{:init})$ block.  This is constructed by first
constructing a tree which only includes terms from the {\em relaxed}
visitation characterised by the {\em assumptive} actions in the
current sequential plan. For example, if $\pp{DORA}$'s plan is:

\[
\begin{array}{l}
\actions^{\circ}(.8;(=(\pp{is-in}~\pp{box})\pp{office}));\\
(\pp{look-for-object}~\pp{DORA}~\pp{box}~\pp{office});\\
(\pp{report-is-in}~\pp{box}~\pp{office})
\end{array}
\]

\noindent Then the {\em relaxed} block will be,

\small
\begin{tabtt}
(\=:init (= (is-in DORA) kitchen) \+ \\
       (probabilistic \=.8 (= (is-in box) office)  \\
		      \>.2 (= (is-in box) kitchen))) \\
\end{tabtt}
\normalsize

\noindent and therefore, the belief-state $\bstate_0$ is:

\small
\begin{tabular}{cccc}
\hline
Probability & (is-in DORA)  & (is-in box)  & (is-in cup) \\
\hline
.8 & kitchen & office & \# \\
.2 & kitchen & kitchen & \# \\
\hline
\end{tabular}
\normalsize

\noindent At this point, decision-theoretic planning would proceed in an
abstraction of the environment that does not contain a $\pp{cup}$,
where the inclusion (exclusion) of POMDP actions is the same as we
defined it for the sequential session.

In a second step, we iteratively refine the {\em relaxed} block by
adding terms from the original block while the resulting state space
remains of a practicable size. Of course, it is expensive to measure
size exactly, thus in practice we estimate it to be
$2^{|\propositions^\#|}$, where $\propositions^\#$ is the set of
propositions in the abstraction induced by the proposed tree. Terms
are proposed for addition because they are probabilistic, and because
they were omitted in the {\em relaxed} visitation characterised by
{\em assumptive} actions in the sequential plan. Each iteration, we
add one candidate term with the largest number of parent terms already
in the candidate tree.








