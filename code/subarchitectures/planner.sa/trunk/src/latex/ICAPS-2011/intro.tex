Planning for agents acting in partially observable environments with
stochastic actions is extremely computationally
expensive \cite{mdp-complexity}. Even relatively small partially
observable Markov decision problems (POMDPs), represented by tens of
propositional variables, produce state spaces far larger than existing
optimal or close to optimal planners can handle. For the easier
problem of planning in completely observable stochastic domains, an
alternative approach has been to use classical sequential planners and
replan when actions have unplanned-for
outcomes \cite{yoon:etal:2007}. However, these approaches aren't so
useful in partially observable domains as replanning is triggered when
the system ends up in a state other than the desired one, and this
state can't in general be determined.

In this paper we describe, \pcogx, the planning component in a robotic
system that continuously deliberates in a stochastic dynamic
environment in order to achieve objectives set by the user, and
acquire knowledge about its surroundings. This domain has the
characteristics of a POMDP in that the state is not perfectly known,
and state information is gained by the robot through the use of
unreliable sensing actions. However, the state representation is
propositional, and even for relatively simple problems is too large
for a POMDP solver to be applied.

Inspired by the replanning approaches used successfully for MDP
planning \cite{yoon:etal:2007,yoon:etal:2008} we propose an approach
that uses a replanning sequential planner to build the majority of the
plan, but uses a decision-theoretic planner on small subproblems where
reasoning about observations is required to find good plans. We refer
to the complete system as a {\em switching planner}. The approach is
not optimal, particularly as it is based on a satisficing sequential
planner, but it performs better than the sequential planner alone and
is fast enough to be used onboard the robot for decision-making. As an
illustration, consider a robot searching for a box that it knows is
often found in offices. The sequential planner might produce a plan
that assumes that {\sc Room a} is an office, that the box is in {\sc
Room a}, and that the box is in location 1 in {\sc Room a}, and then
goes to {\sc Room a}, goes to location 1 and searches for the
box. When the plan gets executed and the search begins, the
decision-theoretic planner is called to conduct the search, and at
this point may retract the last assumption and build a plan to search
the whole of {\sc Room a} for the box. If this plan fails, it will do
so by disproving one of the assumptions (for example, that {\sc Room
a} is an office), which will then lead to replanning by the sequential
planner based on the newly discovered knowledge.

To represent planning problems of this kind, we have developed a
first-order declarative language, called decision-theoretic planning
domain definition language (DTPDDL), for describing domains of
planning problems that correspond to POMDPs. We have created a system
that automatically constructs from this a determinised planning domain
for the sequential planner that includes actions that allow
assumptions to be made about the values of imperfectly known state
variables. These are then used to restrict the state space and set the
reward function for the decision-theoretic planner, which can operate
with the original representation of the domain.

The sequential planner we use is Continual
Planning \cite{brenner:nebel:jaamas09} (CP). CP is based
on \fastdownward \cite{fast-downward}. CP already has the capability
to replan, and also allows agent knowledge to be represented
explicitly, so we can write actions that gain the agent knowledge. For
the decision-theoretic planner we use an information-state
\laostar\ procedure that uses the solution to the equivalent MDP as a
heuristic. For reasons of space, we will not describe this system in
detail here.

The switching planner approach gives us some important advantages in
our robotic domain. Firstly, the main source of uncertainty in the
domain is unreliable observations, rather than stochasticity in the
action effects, and this approach is particularly suited to problems
that combine a deterministic task planning problem with observations
as the decision-theoretic planner is only invoked to determine the
values of variables. Secondly, the replanning makes the system very
robust to changing goals and the discovery of new facts about the
world, both of which are features of our domain.

%To accompany
%that language we have implemented an information-state
%\laostar\ procedure for solving problems expressed in DTPDDL. 

%%

%Have a good understanding of the contents and function of rooms, as
%well as the linguistic referrents to rooms, widgets, and their visual
%qualities.

%Having confidence in its beliefs about the linguistic terms for
%spaces, and the function of those spaces.



%sometimes difficult to predict, with exogenous events, such as the
%changing of the objective, 





%the speed and scalability of software for sequential planning in
%deterministic




%% We developed a first-order declarative language, called DTPDDL, for
%% describing domains of planning problems that correspond to POMDPs.  To
%% accompany that language we have implemented an information-state
%% \laostar\ procedure for solving problems expressed in DTPDDL. 

%% We have extended MAPSIM to parse and simulate DTPDDL problems, and
%% modified \fastdownward\ so that it can find useful sequential executions
%% given DTPDDL models of the problem at hand.




%%  called DTPDDL, along
%% the lines of PDDL for the partially observable case, an \laostar\
%% solution procedure, a determinisation of the DTPDDL problem in MAPL,
%% and modify the \fastdownward\ system to find high quality sequential plans
%% .

%% We model the environment as a partially observable Markov decision
%% process. Although planning in that model is undecidable in
%% general~\cite{}, an optimal finite-horizon plan corresponds to a
%% contingent plan, that is a function mapping action-observation
%% histories to actions.

%% our continual planner is reactive, replanning whenever the underlying
%% domain and problem models change. For example, replanning occurs if
%% the motivational component alters the objectives, and if an assumed
%% outcome of a sensing action is not realised.

%% brittle sensing model, 

%% the evaluation of a fluent at a state is either known. Moreover, there
%% is a sensing process that run-time variables  after-which 

%% For $\prop \in \state$ we say proposition $\prop$ characterises state
%% $\state$. There is always a unique starting state $\state_0$. The goal
%% $\goal$ is a set of propositions, and we say that state $\state$ is a
%% goal state iff $\goal \subseteq \state$.

%%  To keep this exposition
%% simple, for any two distinct actions $\stochAction_i \neq
%% \stochAction_j$, if outcome $\detAct$ is a possibility for
%% $\stochAction_i$ then it cannot also be a possibility for
%% $\stochAction_j$ -- i.e., if $\detAct \in
%% \detActions(\stochAction_i)$ then $\detAct \not\in
%% \detActions(\stochAction_j)$.


%% The solution to a probabilistic planning problem is a contingency
%% plan. This consists of an assignment of actions to states at each
%% discrete timestep up to the planning horizon $n$. The optimal
%% contingency plan is one which prescribes actions to states that
%% maximise the probability that the goal is achieved within $n$ steps
%% from the starting state $\state_0$. For the purposes of this paper
%% we say a plan fails, i.e. achieves the goal with probability $0$, in
%% situations where it does not prescribe an action. Computing the
%% optimal plan for a problem is computationally intractable, and an
%% important direction for research in the field is to develop
%% heuristic mechanisms for generating small linear plans
%% quickly~\cite{littman:etal:98}

An outline of the paper is as follows. We begin by describing POMDPs with a
propositionally factored representation of states and observations,
and therelationship between this and a flat POMDP representation. Next
we describe our DTPDDL language using a running example of the robot
looking for a box. Then we describe the switching planner, including
how the sequential domain is produced and a plan is generated, and how
this then provides the input for the decision-theoretic
planner. Finally we provide an evaluation of the system on the example
robot domain, and finish with a short survey of related work and some
concluding remarks.
