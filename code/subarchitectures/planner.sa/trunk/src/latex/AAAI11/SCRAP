
We describe our approach that switches between sequential and
contingent sessions. As a continual planning approach, it proceeds by
interleaving planning and execution in a deterministic-action POMDP
described in DTPDDL. During a sequential session, planning is
performed by a ``classical'' system,\footnote{That is, a planner
designed to solve fully observable deterministic tasks.}  and
execution proceeds according to the {\em trace} computed by that
system. Taking the form of a classical plan, the trace specifies a
sequence of POMDP actions that achieve the agent's objectives in a
deterministic approximation, i.e., {\em determinisation}, of the
problem at hand. More precisely, the trace is an interleaved sequence
of POMDP actions and {\em assumptive} actions. The latter correspond
to assumptions the planner makes about the truth value of propositions
-- e.g. that a box of cornflakes is located in the kitchen at the
third plan step. They are called {\em applicability} assumptions if
the trace includes an action $\action$ that is not applicable with
probability $1$ at the belief-state
\bstate\ that the system is projected to be in when \action\ is
scheduled for execution, i.e., $\exists\state\in\states\;
\bstate(\state) > 0$ and $\poss(\action)\not\subseteq\state$. By
scheduling
\action,  the serial planner makes an assumption about the
observability of the precondition $\poss(\action)$.



Our approach always begins with a sequential session. Non-assumptive
actions from the trace are executed in sequence until the
applicability of the next scheduled action is uncertain. We denote
that action by \switchAction.  A contingent session then begins that
tailors sensory processing by the agent to determine whether the
assumptions made in the trace hold. 
%%
We add {\em disconfirm} and {\em confirm} actions to the model, so
the session is encouraged to judge the assumptions made in the
trace. On execution of one of those actions, control is returned to
the sequential session that continues at the current%%{\em underlying}
belief-state.
%%
%% For each assumption we add a {\em
%% disconfirm} action to the POMDP whose execution is rewarding
%% (resp. costly) if the assumption is false (resp. true). We also add
%% one {\em confirm} action that is rewarding (resp. costly) to execute
%% if $\poss(\switchAction)$ is true (resp. false). If execution of the
%% contingent plan applies a disconfirm action, then a new sequential
%% session begins at the {\em underlying} belief-state. If the confirm
%% action is executed, the sequential session is resumed, and
%% \switchAction\ is executed. Otherwise, control rests in the continual
%% session.
%%
Because contingent planning is only practical in relatively small
POMDPs, contingent sessions plan in an abstract decision process
determined by the current trace. This abstraction is constructed by
first omitting all propositions that do not feature in the trace, and
by then iteratively refining the model while the result is of a
practicable size.

%%  A DT
%% session then begins which tailors sensory processing to determine
%% whether the assumptions made in the trace hold, or which otherwise
%% acts to achieve the overall objectives.



%% the base planner changes depending on our robot's
%% subjective degrees of belief (i.e., POMDP belief-state), and progress
%% in plan execution.

%% {\em Fast Downward}~\cite{fast-downward}

%% %%
%% When the underlying planner is a {\em sequential}/linear planner,
%% i.e., a {\em classical} planner, we say planning is in a sequential
%% {\em session}, and otherwise it is in a {\em decision-theoretic} (DT)
%% session. Finally, planning is continual in the usual sense that,
%% whatever the session, plans are adapted and rebuilt online in reaction
%% to changes to the planning model (e.g. when objectives are modified,
%% or when our robot's path is obstructed by a door being closed). By
%% autonomously mixing these two types of sessions our robot is able to
%% be robust and responsive to changes in its environment
%% \emph{and} make appropriate decisions in the face of uncertainty.



%% For sequential planning the operators from the POMDP model are
%% available with semantics that accommodate the above abstraction as
%% follows: In the deterministic model a proposition $\prop$ can be
%% thought to have a ternary interpretation at a state \state, as either
%% {\em true}, written $\prop\in\state$, {\em false}, written
%% $\prop\not\in\state$, or {\em unspecified}, with a slight abuse of
%% notation written $\prop\#\in\state$. For example, in $\pp{DORA}$
%% $(=(\pp{is-in}~\pp{box})\pp{office})\#\in\state_0$. For POMDP actions
%% $\action\in\actions$ in the deterministic model, if
%% $\prop \in \poss(\action)$, or if $\prop$ is the subject of a positive
%% or negative effect of $\action$, then $\action$ is not applicable
%% in \state\ if $\prop\#\in\state$. 

%% Addressing now the switching semantics of action execution. I
%% %% In detail, we halt the sequential
%% session at \action\ if $\exists
%% \state$ s.t. $\bstate(\state)>0$, 
%% $\prop\in\poss(\action)$ , and $\prop\not\in\state$. According to the
%% semantics of action execution there must be a $\state'$ s.t.
%% $\bstate(\state')>0$ and $\prop\in\state'$. Otherwise, \action\ must
%% not have been scheduled at \bstate.


%% then $\reward(\state,\action)=10$, and
%% otherwise if $\prop\not\in\state'$ then
%% $\reward(\state',\action)=-10$.


%% A sequential session uses a classical planner to compute a trace. The
%% latter encapsulates assumptions about: (1) the true underlying state,
%% (2) how execution will progress, and (3) the possibility of the agent
%% eventually holding strong beliefs about the truth values of specific
%% state propositions. Here we describe the deterministic planning
%% problem, derived from the DTPDDL model, that admits plans which
%% correspond to traces.

%% In a deterministic-action POMDP all the uncertainty in state and
%% action is expressed in the $(\pp{:init})$ declaration. Our approach
%% uses the structure of that, as it occurs in the problem description,
%% to define a set of state-assumptions available to sequential planning.
%% %%
%% Writing \#\ if the value of a proposition is unspecified, for
%% $\pp{DORA}$ we have the following assumptions:



We add additional constraints that ensure no action from the original
problem model can be sequenced unless the starting states, or an
assumptive action, has 

sequenced in a plan 

: (1) that a proposition cannot be
assumed both true and false, and (2) . 

Additionally, an
assumption about a proposition $\prop$ can only be made once, and must
be sequenced before any POMDP actions that have $\prop$ in their
precondition or effect.


%%
In other words, an assumptive action
must be applied to make a truth assignment to \prop, and then that
truth assignment can be modified by the DTPDDL domain operators.


%% %%
%% Lastly, we have that if $\action$ has a precondition that is not true
%% with probability $\theta$, a given threshold, in the underlying belief
%% $\bstate$, then immediately before it is executed blindly the system
%% switches to a contingent session (described below).

%% that although the above criteria has been in terms of a
%% trace of length $N$, that is not a system parameter.


%% From the perspective of
%% gradient-based reinforcement learning systems, such as William's
%% REINFORCE and GPOMDP variants for planning~\cite{olivier:doug:2009},
%% under reasonable assumptions the optimal trace identifies a maximal
%% gradient step from a uniformly random soft-max policy.


The added dual actions are propositionally trivial --
i.e., $\forall\state\in\states$ $\poss(\assumptiveDT{i})
\subseteq \state$ and $\assumptiveDT{i}(\state) \equiv \state$.  



%%  Rather than invoking a contingent session when a
%% switching action $\switchAction$ is scheduled for execution, the
%% baseline simply executes one action that can trigger a sensing outcome
%% determined by the precondition of $\switchAction$, and replans in the
%% resulting belief-state.

 
% Our implementation is able
% to use several underlying planning systems.
%% We have extended MAPSIM so that it can parse DTPDDL, and perform
%% successive estimation of the underlying belief-state using
%% \system{dlib-ml}~\cite{king:2009} for inference.  In this evaluation
%% we use our own version of Fast Downward~\cite{fast-downward} for
%% sequential sessions. We have extended that system to support actions
%% with success probabilities. In our evaluation Fast Downward is run
%% with the cyclic causal graph heuristic using an A* search or weighted
%% A* (with weight 5). We use A* for easy problems and WA* for more
%% difficult problems where A* is ineffective. Contingent sessions use
%% our own forward search procedure.




%% \Omit{ We then perform multiple tests with different limits for the
%%   belief-space size in contingent sessions.  Higher limits should
%%   cause longer planning times but be beneficial to plan quality as
%%   more contingencies can be taken in to account by the POMDP planner.
%% }

%% executes switching actions and replans in the
%% resulting belief state.
 
%% but instead of
%% creating an observation problem for the decision theoretic planner it
%% will just execute one sensing action -- assuming that this action will
%% confirm its assumption.

In order to evaluate our system, we have also implemented a {\em
baseline} approach in MAPSIM. Rather than invoking a contingent
session when a switching action $\switchAction$ is scheduled for
execution, the baseline simply executes one action that can trigger a
sensing outcome determined by the precondition of $\switchAction$, and
replans in the resulting belief-state.

%% Figure \ref{fig:results-quality} shows the average costs of the
%% executed plans as well as the percentage of solvable tasks that were
%% actually solved by the planner. 


%% :
%%while the resulting plans are still
%% longer on average, the impact on the number of solved tasks was much
%% smaller than for the {\em baseline} system.  

%% Less aggressive abstraction of the initial configuration results in
%% longer runtimes, with little to be gained in terms of plan quality. We
%% conclude that in our scenario it is worthwhile being assumptively
%% aggressive.  



%% We find that abstraction has little impact on plan
%% costs and success rate in our scenario. Increasing the size of the
%% initial abstract belief beyond 50 states rarely pays off, because
%% while additional information may facilitate better plans, the indirect
%% sensing tests report the same result. Here, we only really see an
%% impact in terms of the runtime of contingent planning.


% We believe that a part of the improvement is due to the segmentation of
% the plan into several subtask, essentially performing hierarchical
% planning. Especially when the continual planner performs badly this is
% a huge gain.

%%% Local Variables: 
%%% mode: latex
%%% TeX-master: "moritz_2011"
%%% End: 

They contain rooms and corridors, that are 

To test our approach we use a robot exploration domain based on a
scenario from our physical robotic system. Here, a robot is exploring
an office or living environment, and trying to report the locations of
objects to their owner. Basic types are {\tt rooms}, {\tt places} and
{\tt objects}. Places are topological map nodes that occur in rooms.
Objects, and also the robot, are always located at a place. The robot
can move around the rooms via connections between places given by a
{\tt connected} predicate. Each room has a, possibly unknown, {\em
  category} (e.g. kitchen, office, living room). Certain objects are
more likely to be located in rooms of a particular category.  For
example, a box of cornflakes is more likely to be located in the
kitchen than the office. The robot can look for an object at a place
by executing a {\tt look-for-object} action. Executing {\tt
  look-for-object} results in a noisy sensing outcome, sometimes
indicating a positive perception if the object is there. We have that
some objects are harder to detect than others. Finally, if the robot
is in the presence of a human, it can ask what type of room it is
currently in, however conducting a dialogue is more costly than
running a vision algorithm (cost of 8 vs costs of 3).


%%  Therefore, an instantaneous positive detection of an object is
%% not proof positive of it being there.

We evaluated our switching planner against the {\em baseline} in several
tasks with the number of rooms ranging from 3 to 6. We examine the
impact of sensing reliability, experimenting using sensor models that
are: ({\em reliable}) $0.9$ chance the object is perceived if it is
present, ({\em semi-reliable}) chance is 0.65, and ({\em noisy})
chance is 0.4. The continual planning times are reported in
Figure~\ref{fig:results-time}, and the quality data in
Figure~\ref{fig:results-quality}. Here, in switching runs we examine
the performance of our system where $b_0$ allocates non-zero
probability to between 20 and 200 abstract states during contingent
sessions. 


%% Because the switching utility model 

%% Using a satisficing, optimistic serial planner in sequential sessions
%% makes optimising for the expected reward difficult. Only reasoning
%% about single traces, it is optimistic wrt the remaining costs to a
%% goal and pessimistic regarding the probability of reaching it. By
%% setting the rewards to non-extremal values has little effect on that
%% system. Therefore, we do not consider expected utility in our
%% experiments. Instead, we report average costs of the plans and the
%% success rate (as a ratio between solved and solvable problems).




%% From an automated planning perspective, the problem of practical
%% mobile robot control poses important and contrary challenges.
%% %%
%% On the one hand, planning and execution monitoring must be
%% lightweight, robust, timely, and should span the lifetime of the
%% robot. Those processes must seamlessly accommodate exogenous events,
%% changing objectives, and the underlying unpredictability of the
%% environment.
%% %%
%% On the other hand, robot planning should perform computationally
%% expensive reasoning about contingencies, and possible revisions of
%% subjective belief according to quantitatively modelled uncertainty in
%% acting and sensing. 
