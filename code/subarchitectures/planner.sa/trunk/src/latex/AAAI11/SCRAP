
%% When progress according to that strategy is uncertain, a DT sessions
%% proceeds in a small {\em abstract} process, in order sense . 

%%  in order to
%% (in-)validate 

%%  in order to sense
%% whether the {\em sequential} strategy is good, and otherwise pursue
%% the goals.

%% Because DT planning is not practical in large problems, those sessions
%% plan for a small {\em abstract} processes that typically sequential
%% plan and underlying belief state. 


%% Our system is domain independent, taking domain and problem
%% descriptions in a first-order declarative language we have developed,
%% called the decision-theoretic planning domain definition language
%% (DTPDDL). In this paper, we restrict our attention to deterministic
%% action goal-oriented POMDPs\footnote{We note that POMDPs with
%% stochastic actions can be compiled into equivalent
%% deterministic-action POMDPs, where all the original action uncertainty
%% is expressed in the starting-state
%% distribution~\cite{ng:Jordan:2000}.} -- I.e., where a finite horizon
%% optimal contingent plan exists.  
%%
%% From these descriptions of POMDPs, we
%% automatically construct a deterministic model for the sequential
%% planner. That model includes actions which correspond to making
%% assumptions about the values of imperfectly known state
%% variables. Assumptions scheduled by the sequential system are used to
%% propose a pragmatic abstract belief-state space to the
%% decision-theoretic system, and to modify the reward function, so that
%% system might pursue sensing related to those assumptions.

%% The switching planner provides important advantages in our
%% mobile robot domain. First, the significant source of uncertainty in the
%% domain is the unreliability of observations and this approach is
%% particularly suited to problems that combine a deterministic task
%% planning problem with observations because the decision-theoretic planner
%% is only invoked to deal with state uncertainty.
%% %determine a characteristic of the underlying state. 
%% Secondly, replanning makes the system robust to
%% changing objectives and discoveries about the world,
%% both of which feature in our domain.



%% The remainder of the paper proceeds as follows. We begin by describing
%% related work. Next we describe our DTPDDL language with example
%% declarations from a mobile robot exploration task. Then we describe
%% the switching planner, how sequential planning proceeds in our problem
%% models, and how this then provides the input for the
%% decision-theoretic planner. Finally we provide an evaluation of the
%% system on the example robot domain, and then provide some concluding
%% remarks and future directions.



%% BEGIN SCRAPPY


%% The initial abstract process, let's suppose, has two states. The state
%% where everything assumed is true, and the null state. This has a KL
%% divergence from the :init term, because we can suppose it uniformly
%% chooses one concrete state, given the abstract :init states.

%% As you add propositions to the abstract :init term, then the KL
%% divergence decreases. 


%% END SCRAPPY

%%

%% In our work we take a concrete step towards addressing all the
%% challenges we outlined. We have developed a {\em switching}
%% domain-independent planning system that operates according to the
%% continual planning paradigm.  It uses first-order By autonomously mixing these two
%% types of sessions our robot is able to be robust and responsive to
%% changes in its environment
%% \emph{and} make appropriate decisions in the face of uncertainty.





%% This domain features partial observability, particularly
%% because the state is not perfectly known, and state information is
%% gained by the robot through the use of noisy sensing
%% actions. 
%% %Using a propositionally factored state representation, for
%% For
%% interesting sized tasks the corresponding POMDP model is too large for a
%% POMDP solver to be applied directly. Inspired by the replanning
%% approaches used successfully for MDP
%% planning~\cite{yoon:etal:2007}, we propose a continual
%% planning approach that uses a classical planning system to compute a
%% reasonably valuable trace in the model, and then uses a
%% decision-theoretic planner on small subproblems where reasoning about
%% observations might be useful. We refer to the complete system as a
%% {\em switching continual planner}. The approach is not optimal,
%% particularly as it relies on the results of satisficing sequential planning
%% directly. It does nevertheless perform better than a purely sequential
%% replanner, and is fast enough to be used for real-time decision-making
%% on a mobile robot.



%% The underlying planning procedures are {\em classical}, i.e.,
%% sequential, planning system.





%% Planning for agents acting in partially observable environments with
%% stochastic actions is extremely computationally
%% expensive~\cite{mdp-complexity}. Even relatively small partially
%% observable Markov decision problems (POMDPs), represented by tens of
%% propositional variables, have state spaces far larger than existing
%% optimal or close to optimal planners can handle. For the easier
%% problem of planning in completely observable stochastic domains, an
%% alternative has been to use classical sequential planners and
%% replan when actions have unplanned-for outcomes~\cite{yoon:etal:2007}.
%% Execution monitoring tracks properties of the state that determine
%% whether further planning is required. For partially observable
%% domains, only probabilistic information about such properties will, in
%% general, be available.



%% However, these approaches rely
%% crucially on being able to determine properties of the current state
%% in order to decide to replan. For partially observable domains, only
%% probabilistic information about these properties will in general be
%% available.

%% Switching gives us some important advantages in our robotic
%% domain. First, the significant source of uncertainty in the domain is
%% the unreliability of observations, and this approach is particularly
%% suited to problems that combine a deterministic task planning problem
%% with observations as the decision-theoretic planner is often invoked
%% to determine a characteristic of the underlying state. Secondly,
%% replanning makes our system robust to changing objectives and the
%% discovery of new facts about the world.


%% As an illustration, consider a robot searching for a box that it knows
%% is often found in offices. The sequential planner might produce a plan
%% that assumes that {\sc Room a} is an office, that the box is in {\sc
%% Room a}, and that the box is in location 1 in {\sc Room a}. Given
%% these assumptions, the plan might be to go to {\sc Room a}, go to
%% location 1 and search for the box. When the plan gets executed and the
%% search begins the decision-theoretic planner is called since searching
%% involves making observations. The decision-theoretic planner might
%% retract the last assumption but leave the others, and build a plan to
%% search the whole of {\sc Room a} for the box. If this plan fails to
%% find the box, it will disprove the assumption that the box is in {\sc
%% Room a}, which will then lead to replanning by the sequential planner
%% based on the newly discovered knowledge.


%To accompany
%that language we have implemented an information-state
%\laostar\ procedure for solving problems expressed in DTPDDL. 

%%

%Have a good understanding of the contents and function of rooms, as
%well as the linguistic referrents to rooms, widgets, and their visual
%qualities.

%Having confidence in its beliefs about the linguistic terms for
%spaces, and the function of those spaces.



%sometimes difficult to predict, with exogenous events, such as the
%changing of the objective, 





%the speed and scalability of software for sequential planning in
%deterministic




%% We developed a first-order declarative language, called DTPDDL, for
%% describing domains of planning problems that correspond to POMDPs.  To
%% accompany that language we have implemented an information-state
%% \laostar\ procedure for solving problems expressed in DTPDDL. 

%% We have extended MAPSIM to parse and simulate DTPDDL problems, and
%% modified \fastdownward\ so that it can find useful sequential executions
%% given DTPDDL models of the problem at hand.




%%  called DTPDDL, along
%% the lines of PDDL for the partially observable case, an \laostar\
%% solution procedure, a determinisation of the DTPDDL problem in MAPL,
%% and modify the \fastdownward\ system to find high quality sequential plans
%% .

%% We model the environment as a partially observable Markov decision
%% process. Although planning in that model is undecidable in
%% general~\cite{}, an optimal finite-horizon plan corresponds to a
%% contingent plan, that is a function mapping action-observation
%% histories to actions.


%% our continual planner is reactive, replanning whenever the underlying
%% domain and problem models change. For example, replanning occurs if
%% the motivational component alters the objectives, and if an assumed
%% outcome of a sensing action is not realised.

%% brittle sensing model, 

%% the evaluation of a fluent at a state is either known. Moreover, there
%% is a sensing process that run-time variables  after-which 

%% For $\prop \in \state$ we say proposition $\prop$ characterises state
%% $\state$. There is always a unique starting state $\state_0$. The goal
%% $\goal$ is a set of propositions, and we say that state $\state$ is a
%% goal state iff $\goal \subseteq \state$.

%%  To keep this exposition
%% simple, for any two distinct actions $\stochAction_i \neq
%% \stochAction_j$, if outcome $\detAct$ is a possibility for
%% $\stochAction_i$ then it cannot also be a possibility for
%% $\stochAction_j$ -- i.e., if $\detAct \in
%% \detActions(\stochAction_i)$ then $\detAct \not\in
%% \detActions(\stochAction_j)$.


%% The solution to a probabilistic planning problem is a contingency
%% plan. This consists of an assignment of actions to states at each
%% discrete timestep up to the planning horizon $n$. The optimal
%% contingency plan is one which prescribes actions to states that
%% maximise the probability that the goal is achieved within $n$ steps
%% from the starting state $\state_0$. For the purposes of this paper
%% we say a plan fails, i.e. achieves the goal with probability $0$, in
%% situations where it does not prescribe an action. Computing the
%% optimal plan for a problem is computationally intractable, and an
%% important direction for research in the field is to develop
%% heuristic mechanisms for generating small linear plans
%% quickly~\cite{littman:etal:98}

%%  which can operate
%% with the original representation of the domain.


%% The sequential planner we use is based
%% on \fastdownward~\cite{fast-downward}. We add the capability to replan
%% ~\cite{brenner:nebel:jaamas09}, and also allow agent
%% knowledge to be represented explicitly, so we can write actions that
%% gain the agent knowledge. LEASE CITE MICHAEL's WORK AS THE BASIS FOR
%% THIS. For the decision-theoretic planner we have implemented our own
%% forward search in the belief-state space.

%% the significant source of uncertainty in the
%% domain is the unreliability of observations and this approach is
%% particularly suited to problems that combine a deterministic task
%% planning problem with observations because the decision-theoretic planner
%% is only invoked to deal with state uncertainty.

%% The switching planner provides important advantages in our
%% mobile robot domain.



%% BEGIN SCRAPPY

%% deal with POND and the Perseus language


%% For example, the robot usually knows exactly where it is located on a
%% topological map, and therefore there are tight constraints about what
%% regions of the map can be observed, and what regions of the map can be
%% moved to in an atomic step.

%% Moreover, we use this domain description to model the action physics
%% of abstract processes posed to decision-theoretic planning
%% sessions. Here, we forbid the planner from considering courses of
%% action that are not {\em known} to be possible to alleviate the
%% computational burden planning in the abstract process.

%% END SCRAPPY



%% define the value of a trace
%% $\state_0, \action_0, \state_1, \action_1,.., \state_N$ from the
%% deterministic model to equal:

%% \small
%% \begin{equation}\label{eq:tracevalue}
%% V(\state_0, \action_0, \state_1, \action_1,.., \state_N) =  \prod_{i=1..N-1} \prob_i \sum_{i=1..N-1} \reward(\state_i, \action_i)
%% \end{equation}
%% \normalsize

%% \noindent The
%% optimal trace given a plan, i.e., the sequence of non-assumptive
%% actions from the trace, therefore has value:


We describe our approach that switches between sequential and
contingent sessions. As a continual planning approach, it proceeds by
interleaving planning and execution in a deterministic-action POMDP
described in DTPDDL. During a sequential session, planning is
performed by a ``classical'' system,\footnote{That is, a planner
designed to solve fully observable deterministic tasks.}  and
execution proceeds according to the {\em trace} computed by that
system. Taking the form of a classical plan, the trace specifies a
sequence of POMDP actions that achieve the agent's objectives in a
deterministic approximation, i.e., {\em determinisation}, of the
problem at hand. More precisely, the trace is an interleaved sequence
of POMDP actions and {\em assumptive} actions. The latter correspond
to assumptions the planner makes about the truth value of propositions
-- e.g. that a box of cornflakes is located in the kitchen at the
third plan step. They are called {\em applicability} assumptions if
the trace includes an action $\action$ that is not applicable with
probability $1$ at the belief-state
\bstate\ that the system is projected to be in when \action\ is
scheduled for execution, i.e., $\exists\state\in\states\;
\bstate(\state) > 0$ and $\poss(\action)\not\subseteq\state$. By
scheduling
\action,  the serial planner makes an assumption about the
observability of the precondition $\poss(\action)$.



Our approach always begins with a sequential session. Non-assumptive
actions from the trace are executed in sequence until the
applicability of the next scheduled action is uncertain. We denote
that action by \switchAction.  A contingent session then begins that
tailors sensory processing by the agent to determine whether the
assumptions made in the trace hold. 
%%
We add {\em disconfirm} and {\em confirm} actions to the model, so
the session is encouraged to judge the assumptions made in the
trace. On execution of one of those actions, control is returned to
the sequential session that continues at the current%%{\em underlying}
belief-state.
%%
%% For each assumption we add a {\em
%% disconfirm} action to the POMDP whose execution is rewarding
%% (resp. costly) if the assumption is false (resp. true). We also add
%% one {\em confirm} action that is rewarding (resp. costly) to execute
%% if $\poss(\switchAction)$ is true (resp. false). If execution of the
%% contingent plan applies a disconfirm action, then a new sequential
%% session begins at the {\em underlying} belief-state. If the confirm
%% action is executed, the sequential session is resumed, and
%% \switchAction\ is executed. Otherwise, control rests in the continual
%% session.
%%
Because contingent planning is only practical in relatively small
POMDPs, contingent sessions plan in an abstract decision process
determined by the current trace. This abstraction is constructed by
first omitting all propositions that do not feature in the trace, and
by then iteratively refining the model while the result is of a
practicable size.

%%  A DT
%% session then begins which tailors sensory processing to determine
%% whether the assumptions made in the trace hold, or which otherwise
%% acts to achieve the overall objectives.



%% the base planner changes depending on our robot's
%% subjective degrees of belief (i.e., POMDP belief-state), and progress
%% in plan execution.

%% {\em Fast Downward}~\cite{fast-downward}

%% %%
%% When the underlying planner is a {\em sequential}/linear planner,
%% i.e., a {\em classical} planner, we say planning is in a sequential
%% {\em session}, and otherwise it is in a {\em decision-theoretic} (DT)
%% session. Finally, planning is continual in the usual sense that,
%% whatever the session, plans are adapted and rebuilt online in reaction
%% to changes to the planning model (e.g. when objectives are modified,
%% or when our robot's path is obstructed by a door being closed). By
%% autonomously mixing these two types of sessions our robot is able to
%% be robust and responsive to changes in its environment
%% \emph{and} make appropriate decisions in the face of uncertainty.



%% For sequential planning the operators from the POMDP model are
%% available with semantics that accommodate the above abstraction as
%% follows: In the deterministic model a proposition $\prop$ can be
%% thought to have a ternary interpretation at a state \state, as either
%% {\em true}, written $\prop\in\state$, {\em false}, written
%% $\prop\not\in\state$, or {\em unspecified}, with a slight abuse of
%% notation written $\prop\#\in\state$. For example, in $\pp{DORA}$
%% $(=(\pp{is-in}~\pp{box})\pp{office})\#\in\state_0$. For POMDP actions
%% $\action\in\actions$ in the deterministic model, if
%% $\prop \in \poss(\action)$, or if $\prop$ is the subject of a positive
%% or negative effect of $\action$, then $\action$ is not applicable
%% in \state\ if $\prop\#\in\state$. 

%% Addressing now the switching semantics of action execution. I
%% %% In detail, we halt the sequential
%% session at \action\ if $\exists
%% \state$ s.t. $\bstate(\state)>0$, 
%% $\prop\in\poss(\action)$ , and $\prop\not\in\state$. According to the
%% semantics of action execution there must be a $\state'$ s.t.
%% $\bstate(\state')>0$ and $\prop\in\state'$. Otherwise, \action\ must
%% not have been scheduled at \bstate.


%% then $\reward(\state,\action)=10$, and
%% otherwise if $\prop\not\in\state'$ then
%% $\reward(\state',\action)=-10$.


%% A sequential session uses a classical planner to compute a trace. The
%% latter encapsulates assumptions about: (1) the true underlying state,
%% (2) how execution will progress, and (3) the possibility of the agent
%% eventually holding strong beliefs about the truth values of specific
%% state propositions. Here we describe the deterministic planning
%% problem, derived from the DTPDDL model, that admits plans which
%% correspond to traces.

%% In a deterministic-action POMDP all the uncertainty in state and
%% action is expressed in the $(\pp{:init})$ declaration. Our approach
%% uses the structure of that, as it occurs in the problem description,
%% to define a set of state-assumptions available to sequential planning.
%% %%
%% Writing \#\ if the value of a proposition is unspecified, for
%% $\pp{DORA}$ we have the following assumptions:



We add additional constraints that ensure no action from the original
problem model can be sequenced unless the starting states, or an
assumptive action, has 

sequenced in a plan 

: (1) that a proposition cannot be
assumed both true and false, and (2) . 

Additionally, an
assumption about a proposition $\prop$ can only be made once, and must
be sequenced before any POMDP actions that have $\prop$ in their
precondition or effect.


%%
In other words, an assumptive action
must be applied to make a truth assignment to \prop, and then that
truth assignment can be modified by the DTPDDL domain operators.


%% %%
%% Lastly, we have that if $\action$ has a precondition that is not true
%% with probability $\theta$, a given threshold, in the underlying belief
%% $\bstate$, then immediately before it is executed blindly the system
%% switches to a contingent session (described below).

%% that although the above criteria has been in terms of a
%% trace of length $N$, that is not a system parameter.


%% From the perspective of
%% gradient-based reinforcement learning systems, such as William's
%% REINFORCE and GPOMDP variants for planning~\cite{olivier:doug:2009},
%% under reasonable assumptions the optimal trace identifies a maximal
%% gradient step from a uniformly random soft-max policy.


The added dual actions are propositionally trivial --
i.e., $\forall\state\in\states$ $\poss(\assumptiveDT{i})
\subseteq \state$ and $\assumptiveDT{i}(\state) \equiv \state$.  



%%  Rather than invoking a contingent session when a
%% switching action $\switchAction$ is scheduled for execution, the
%% baseline simply executes one action that can trigger a sensing outcome
%% determined by the precondition of $\switchAction$, and replans in the
%% resulting belief-state.

 
% Our implementation is able
% to use several underlying planning systems.
%% We have extended MAPSIM so that it can parse DTPDDL, and perform
%% successive estimation of the underlying belief-state using
%% \system{dlib-ml}~\cite{king:2009} for inference.  In this evaluation
%% we use our own version of Fast Downward~\cite{fast-downward} for
%% sequential sessions. We have extended that system to support actions
%% with success probabilities. In our evaluation Fast Downward is run
%% with the cyclic causal graph heuristic using an A* search or weighted
%% A* (with weight 5). We use A* for easy problems and WA* for more
%% difficult problems where A* is ineffective. Contingent sessions use
%% our own forward search procedure.




%% \Omit{ We then perform multiple tests with different limits for the
%%   belief-space size in contingent sessions.  Higher limits should
%%   cause longer planning times but be beneficial to plan quality as
%%   more contingencies can be taken in to account by the POMDP planner.
%% }

%% executes switching actions and replans in the
%% resulting belief state.
 
%% but instead of
%% creating an observation problem for the decision theoretic planner it
%% will just execute one sensing action -- assuming that this action will
%% confirm its assumption.

In order to evaluate our system, we have also implemented a {\em
baseline} approach in MAPSIM. Rather than invoking a contingent
session when a switching action $\switchAction$ is scheduled for
execution, the baseline simply executes one action that can trigger a
sensing outcome determined by the precondition of $\switchAction$, and
replans in the resulting belief-state.

%% Figure \ref{fig:results-quality} shows the average costs of the
%% executed plans as well as the percentage of solvable tasks that were
%% actually solved by the planner. 


%% :
%%while the resulting plans are still
%% longer on average, the impact on the number of solved tasks was much
%% smaller than for the {\em baseline} system.  

%% Less aggressive abstraction of the initial configuration results in
%% longer runtimes, with little to be gained in terms of plan quality. We
%% conclude that in our scenario it is worthwhile being assumptively
%% aggressive.  



%% We find that abstraction has little impact on plan
%% costs and success rate in our scenario. Increasing the size of the
%% initial abstract belief beyond 50 states rarely pays off, because
%% while additional information may facilitate better plans, the indirect
%% sensing tests report the same result. Here, we only really see an
%% impact in terms of the runtime of contingent planning.


% We believe that a part of the improvement is due to the segmentation of
% the plan into several subtask, essentially performing hierarchical
% planning. Especially when the continual planner performs badly this is
% a huge gain.

%%% Local Variables: 
%%% mode: latex
%%% TeX-master: "moritz_2011"
%%% End: 

They contain rooms and corridors, that are 

To test our approach we use a robot exploration domain based on a
scenario from our physical robotic system. Here, a robot is exploring
an office or living environment, and trying to report the locations of
objects to their owner. Basic types are {\tt rooms}, {\tt places} and
{\tt objects}. Places are topological map nodes that occur in rooms.
Objects, and also the robot, are always located at a place. The robot
can move around the rooms via connections between places given by a
{\tt connected} predicate. Each room has a, possibly unknown, {\em
  category} (e.g. kitchen, office, living room). Certain objects are
more likely to be located in rooms of a particular category.  For
example, a box of cornflakes is more likely to be located in the
kitchen than the office. The robot can look for an object at a place
by executing a {\tt look-for-object} action. Executing {\tt
  look-for-object} results in a noisy sensing outcome, sometimes
indicating a positive perception if the object is there. We have that
some objects are harder to detect than others. Finally, if the robot
is in the presence of a human, it can ask what type of room it is
currently in, however conducting a dialogue is more costly than
running a vision algorithm (cost of 8 vs costs of 3).


%%  Therefore, an instantaneous positive detection of an object is
%% not proof positive of it being there.

We evaluated our switching planner against the {\em baseline} in several
tasks with the number of rooms ranging from 3 to 6. We examine the
impact of sensing reliability, experimenting using sensor models that
are: ({\em reliable}) $0.9$ chance the object is perceived if it is
present, ({\em semi-reliable}) chance is 0.65, and ({\em noisy})
chance is 0.4. The continual planning times are reported in
Figure~\ref{fig:results-time}, and the quality data in
Figure~\ref{fig:results-quality}. Here, in switching runs we examine
the performance of our system where $b_0$ allocates non-zero
probability to between 20 and 200 abstract states during contingent
sessions. 


%% Because the switching utility model 

%% Using a satisficing, optimistic serial planner in sequential sessions
%% makes optimising for the expected reward difficult. Only reasoning
%% about single traces, it is optimistic wrt the remaining costs to a
%% goal and pessimistic regarding the probability of reaching it. By
%% setting the rewards to non-extremal values has little effect on that
%% system. Therefore, we do not consider expected utility in our
%% experiments. Instead, we report average costs of the plans and the
%% success rate (as a ratio between solved and solvable problems).




%% From an automated planning perspective, the problem of practical
%% mobile robot control poses important and contrary challenges.
%% %%
%% On the one hand, planning and execution monitoring must be
%% lightweight, robust, timely, and should span the lifetime of the
%% robot. Those processes must seamlessly accommodate exogenous events,
%% changing objectives, and the underlying unpredictability of the
%% environment.
%% %%
%% On the other hand, robot planning should perform computationally
%% expensive reasoning about contingencies, and possible revisions of
%% subjective belief according to quantitatively modelled uncertainty in
%% acting and sensing. 





%% In detail, for each excluded atomic term, we compute the {\em
%% entropy} of the corresponding proposition, {\em conditional} on the
%% active assumptions. The {\em conditional entropy} measure
%% is higher the more uncertain that proposition is given truth
%% assignments to propositions that are the subject of active
%% assumptions. Propositions that in the first place were excluded are
%% iteratively added to the problem posed to the conditional session in
%% increasing order according to that measure.


%% In particular, we compute the {\em conditional entropy} of
%% each excluded proposition given the active assumptions. Propositions
%% are candidates for addition in increasing order according to that
%% measure.

%% Of course, it is expensive to measure size exactly,
%% and in practise we estimate this to be $2^{|\propositions^\#|}$, where
%% $\propositions^\#$ is the set of propositions in the abstraction that
%% were not assigned the {\em unknown} value \#. Terms are proposed for
%% addition because they are probabilistic, and because they were omitted
%% in the {\em relaxed} visitation characterised by {\em assumptive}
%% actions in the sequential plan. In each iteration, we add one
%% candidate term with the largest number of parent terms already in the
%% candidate tree.


 Figure \ref{fig:abstraction} shows the belief-state of a more
detailed version of our previous example. The diamonds represent
probabilistic terms of the state description while circles represent
conjunctive or atomic terms.


The starting belief-state for the abstract process is given in terms
of a {\em relaxed} $(\pp{:init})$ declaration. We first construct a
tree which only includes terms from the {\em relaxed} visitation
characterised by the active {\em assumptive} actions in the current
sequential plan. Figure \ref{fig:abstraction} shows the belief-state
of a more detailed version of our previous example. The diamonds
represent probabilistic terms of the state description while circles
represent conjunctive or atomic terms. 