
We now describe our {\em switching} planning system that operates
according to the continual planning paradigm, given a description of
the current problem and domain in DTPDDL. The system {\em
switches}, in the sense that planning proceeds in interleaved
sessions, in which the base planner is either {\em sequential}/linear
or {\em decision-theoretic}.
%%
During a sequential session, a rewarding {\em trace} of a possible
execution is computed using a cost-optimising satisficing planner
which trades action costs, goal rewards, and determinacy.
%%
Formatted as a linear plan, the trace specifies a sequence of actions
that achieves some reachable goals following a deterministic
approximation of the problem at hand.
%%
Structurally, a trace is a sequence of elements that are either: (i) ground
actions from the DTPDDL description of the world, or (ii) atomic {\em
assumptions}, modelled as deterministic actions, made about the truth
value of facts that can only be determined at runtime (e.g., that a
box of cornflakes is located on the corner bench in the kitchen).
%%
The system always begins in a sequential session, so that plan
execution proceeds by applying DTPDDL actions from the trace, in
sequence, until the outcome of executing the next scheduled action is
too uncertain according to a threshold parameter~(here, 95\%). In that
eventuality a DT session commences, which tailors sensory processing
to determine whether the assumptions made in the trace hold, or which
otherwise acts to achieve the goals.


Because online DT planning in large problems is impractical (we seek
response times in seconds), a DT session plans for an abstract process
determined by the current trace and underlying belief-state. That
abstract problem is characterised by a limited number of propositions,
chosen due to their {\em relevance} to the action whose scheduled
execution triggered the switch to the DT session.
%%the subproblem the DT session is asked tosolve.
% MOG: Describe this in detail in the appropriate subsection
%  This
% abstraction is constructed by first excluding all propositions (and
% related actions) that are not true, or {\em assumed } true, of states
% in the trace, then adding them back, using as an heuristic the entropy
% of the trace assumptions conditional on a candidate
% proposition. Propositions are added, one at a time, until the number
% of states in the initial belief-state reaches a given
% threshold.\footnote{Our experiments are performed using a number of
% different threshold parameters.} 
To that abstract model we also add {\em disconfirm} and {\em confirm}
actions that the DT session can schedule in order to judge an atomic
assumption in the trace. Those actions yield a relatively small reward
if the corresponding judgement is true (or small penalty
otherwise). If a judgement action is scheduled for execution the DT
session is terminated, and control is returned to a new sequential
session.

Whatever the session type, our continual planner maintains a factored
representation of successive belief-states by performing belief
revision. As an internal representation of the $(\pp{:init})$
declaration, we keep a tree-shaped Bayesian network which gets updated
whenever an action is performed, or an observation received. That
belief-state representation is used: (1) as the source of candidate
determinisations for sequential planning, (2) in determining when to
switch to a DT session, and (3) as a mechanism to guide construction
of an abstract process for DT sessions.

\subsection{Sequential Sessions}

As we only consider deterministic-action POMDPs, all state
uncertainty is expressed in the $(\pp{:init})$ declaration. This
declaration is used by our approach to define the starting state for
sequential sessions, and the set of state-assumptions available to
sequential planning.  Writing \#\ if the value of a proposition is
unspecified, taking the $(\pp{:init})$ example from the previous
section, we have the following assumptions:

\small
\begin{tabular}{cccc}
\hline
Probability & (is-in R2D2)  & (is-in box)  & (is-in cup) \\
\hline
%% .24 & kitchen & office & office \\
%% .06 & kitchen & kitchen & office \\
%% .56 & kitchen & office & kitchen \\
%% .14 & kitchen & kitchen & kitchen \\
.7 & kitchen & \# &  kitchen\\
.3 & kitchen & \# & office \\
.8 & kitchen & office & \# \\
.2 & kitchen & kitchen & \# \\
1.0 & kitchen & \# & \# \\
\hline
\end{tabular}
\normalsize

\noindent Each assumption corresponds to one distinct {\em
relaxed} visitation of the root term. Here, a conjunctive term is
visited iff its atomic subterms are visited, and zero or one of its
immediate probabilistic subterms are visited. For a sequential
session, the starting state corresponds to an assumption (usually
unique) with probability $1$.\footnote{To simplify the discussion, we
suppose that the root term of $(\pp{:init})$ lists all atoms that are
necessarily true.} In our example, that starting state is:

\small
\[
\begin{array}{l}
\state_0 \equiv \{(=(\pp{is-in}~\pp{R2D2})~\pp{kitchen}),\\
\;\;(=(\pp{is-in}~\pp{box})~\#), (=(\pp{is-in}~\pp{cup})~\#)\}.
\end{array}
\]
\normalsize

Encapsulating state-assumptions, we annotate the problem posed during
a sequential session with an \emph{assumptive action} $\assumptiveS{i}$ for
each element $\prob_i (T_i)$, of each probabilistic term from
$(\pp{:init})$. Here, $\assumptiveS{i}$ can be executed if no
$\assumptiveS{j}$, $j \neq i$, has been executed from the same
probabilistic term, and, either
$(\pp{probabilistic}~..\prob_i~(T_i)..)$ is in the root conjunct, or
it occurs in $T_k$ for some executed $\assumptiveS{k}$.
%%
We also add constraints that forbid scheduling of
assumptions about facts after actions with preconditions or effects
that mention those facts. For example, the robot cannot assume it is
plugged into a power source immediately after it unplugs itself.
%%
Executing $\assumptiveS{i}$ in a state $\state$ effects a transition
to a successor state $\state^{T_i}$ with probability $\prob_i$, and
$\state^\bot$ with probability $1 - \prob_i$. Here, $\state^{T_i}$ is
the union of $\state$ with atomic terms from $T_i$. State
$\state^\bot$ is an added sink.


We now describe the optimisation criteria used during sequential
sessions. Where $\prob_i$ is the probability that the $i^{th}$
sequenced action, $\action_i$, from a trace of state-action pairs
$\langle \state_0, \action_0,\state_1, \action_1,.., \state_N \rangle$
does not transition to $\state^\bot$, we have that the optimal trace
has value:

\small
\[
V^* = \max_N \max_{\state_0, \action_0,.., \state_N} \prod_{i=1..N-1} \prob_i \sum_{i=1..N-1}
\reward(\state_i, \action_i),
\]
\normalsize

\noindent  Here,  $\reward(\state_i, \action_i)$ is the instantaneous
reward ---i.e., can be negative for action costs, or positive for goal
achievement--- received for executing action $\action_i$ in state
$\state_i$. Finally, it is worth clarifying that in practise we do not
artificially limit the length of traces that the sequential planner
can consider. Moreover, for problems we consider there is always a
finite optimal trace.

\subsection{DT Sessions}

When an action is scheduled whose outcome is uncertain according to
the underlying belief-state, the planner switches to a DT
session. That plans for {\em small} abstract processes defined
according to the action that triggered the DT session, the assumptive
actions in the proceeding trace, and the current
belief-state. Targeted sensing is encouraged by augmenting the reward
model to reflect a heuristic value of knowing the truth about
assumptions. In detail, all rewards from the underlying problem are
retained. Additionally, for each relevant assumptive action
$\assumptiveS{i}$ in the current trace, we have a {\em disconfirm
action} $\assumptiveDT{i}$ so that for all states $\state$:

\small
\[
\reward(\state, \assumptiveDT{i}) = \bigg\{ \begin{array}{ll}
\$(T_i) & \pp{if}~\;\;T_i \not\subseteq \state \\
\hat\$(T_i) & \pp{otherwise} \\
\end{array}
\]
\normalsize

\noindent where $\$(T_i)$ (resp. $\hat\$(T_i)$) is a 
relatively small positive (negative) numeric quantity\footnote{Small
relative to rewards from the original process model.} which captures
the utility the agent receives for correctly (incorrectly) rejecting
an assumption.
%%
In terms of action physics, a disconfirm action can only be executed
once, and otherwise is modelled as self-transformation.
%%
We only consider {\em relevant} assumptions when constructing the
abstract model.  If \switchAction\ is the action that switched the
system to a DT session, then an assumption $\assumptiveS{i}$ is {\em
relevant} if it is necessary for the outcome of \switchAction\ to be
determined.  Continuing our simplified example, if the trace is:

\small
\[
\begin{array}{l}
\actions^{\circ}(.8;(=(\pp{is-in}~\pp{box})\pp{office}));\\
\actions^{\circ}(.3;(=(\pp{is-in}~\pp{cup})\pp{kitchen}));\\
(\pp{look}~\pp{box}~\pp{office});
(\pp{look}~\pp{cup}~\pp{kitchen});\\
(\pp{report}~\pp{box}~\pp{office}); 
(\pp{report}~\pp{cup}~\pp{kitchen})
\end{array}
\]
\normalsize

\noindent Taking the switching action \switchAction\ to be
$(\pp{look}~\pp{box}~\pp{office})$, we have that
$\actions^{\circ}(.3;(=(\pp{is-in}~\pp{cup})\pp{kitchen}))$ is not
relevant for this session, and therefore we exclude the corresponding
disconfirm action from the abstract decision process.

Given \switchAction, we also include another once-only self-transition
action $\actions.\poss(\switchAction)$, the \emph{confirmation action}
with the reward property:

\[
\reward(\state, \actions.\poss(\switchAction)) = \bigg\{ \begin{array}{ll}
\$(\poss(\switchAction)) & \pp{if}~\;\; \poss(\switchAction) \subseteq \state \\
\hat\$(\poss(\switchAction)) & \pp{otherwise} \\
\end{array}
\]

Execution of either a disconfirmation or the confirmation action
returns control to a sequential session, which then continues from the
underlying belief-state.

Turning to the detail of (dis-)confirmation rewards, in our integrated
system these are sourced from a motivational subsystem. In this paper,
for $\assumptiveDT{i}$ actions we set $\$(x)$ to be a small positive
constant, and have $\hat\$(x)= - \$(x)(1 - \prob) /
\prob$ where $\prob$ is the probability that $x$ is true. For
$\actions.\poss(\switchAction)$ actions we have $\hat\$(x)= -
\$(x)\prob/(1-\prob)$.



\Omit{
%%
Finally, 
If an
assumption was rejected, we prohibit that sequential session from
making it again.
}


%%  the size of the state space
%% is determined by two factors: the number of non-zero states in the
%% initial belief-state, and the number of actions. Determining
%% action reachability is intractable, as is determining their relevance
%% to goal achievement.

%% As we are aiming to keep the decision theoretic session fast, we try
%% to keep the state space of the abstract problem as small as
%% possible. In a deterministic-action POMDP, the state space size is
%% determined by two factors: the number of non-zero states in the
%% initial belief-state and the number of executable actions. As
%% determining the relevance of an action for a given goal in in general
%% a hard problem, we concentrate on keeping the number of uncertain
%% facts as low as possible without losing important information about
%% the sensing goal.

In order to guarantee fast DT sessions, planning occurs for a small
abstract problem. In our setting, where the underlying decision
process has a deterministic action model, a significant cause of
problem largeness is due to the number of states that occur with
non-zero probability in the underlying belief-state. In posing a small
abstract process for a DT session, we construct an abstract
belief-state that is chiefly characterised by facts related to the
current trace. In that abstraction, instances of operator or sense
declarations whose specification mentions an excluded fact are not
available.
%%
In detail, we first construct an $(\pp{:init})$ declaration which only
contains relevant assumptions from the trace -- e.g.,
(Fig.\ref{fig:abstraction-b}) gives an example, where diamonds are
probabilistic terms, and circles are atomic and/or conjunctive.
%%
For each excluded atom, we compute the {\em entropy} of the active
assumptions, {\em conditional} on that atom. Intuitively, lower
entropy indicates an atom gives better information about
assumptions. Facts are iteratively added to the belief-state in
increasing order according to that measure until the belief space size
reaches a predefined limit (Fig.\ref{fig:abstraction-c}).

%%  or no more atoms that reduce the entropy
%% can be added 

\begin{figure}[h!]
  \centering
  \tikzstyle{tree} = [sibling distance=4.5mm]
  \tikzstyle{toplevel} = [grow'=right, sibling distance=22mm]
  \tikzstyle{seclevel} = [sibling distance=9mm]
  \tikzstyle{pnode} = [diamond, draw=black, minimum size=2.5mm]
  \tikzstyle{cnode} = [circle, draw=black, minimum size=3mm]
  \tikzstyle{assumption} = [solid, very thick, draw=black]
  \tikzstyle{selected} = [solid, draw=black]
  \tikzstyle{unused} = [densely dashed, draw=black!40]
  %% \subfloat[The initial belief-state with the assumptions made by the continual
  %%   planner in bold.]{
  %%     \label{fig:abstraction-a}
  %%     \begin{tikzpicture}[
  %%   level 3/.style={tree}]
  %%   \node[pnode, assumption] (cat) at (1,1) {} [toplevel]
  %%     child[seclevel] {node[cnode, assumption] (office) {}
  %%       child {node [pnode, assumption] (box) {}
  %%         child {node [cnode] (boxp1) {}}
  %%         child {node [cnode, assumption] (boxp2) {}}
  %%       }
  %%       child {node [pnode] (cup) {} 
  %%         child {node [cnode] (cupp1) {}}
  %%         child {node [cnode] (cupp2) {}}
  %%       }
  %%     }
  %%     child {node[cnode] (kitchen) {}
  %%       child {node [pnode] (box2) {} 
  %%         child {node [cnode] (box2p1) {}}
  %%         child {node [cnode] (box2p2) {}}
  %%       }
  %%     };
  %%  \tiny
  %%  \draw[assumption] (cat) -- (office) -- (box) -- (boxp2);
  %%  \node[above=0 of cat] {$\pp{(category room1)}$};
  %%  \node[below=0 of kitchen] {$\pp{kitchen}$};
  %%  \node[below=0 of office] {$\pp{office}$};
  %%  \node[below=0 of box] {$\pp{(is-in box)}$};
  %%  \node[below=0 of box2] {$\pp{(is-in box)}$};
  %%  \node[below=0 of cup] {$\pp{(is-in cup)}$};
  %%  \node[right=0 of boxp1] {$\pp{place1}$};
  %%  \node[right=0 of boxp2] {$\pp{place2}$};
  %%  \node[right=0 of box2p1] {$\pp{place1}$};
  %%  \node[right=0 of box2p2] {$\pp{place2}$};
  %%  \node[right=0 of cupp1] {$\pp{place1}$};
  %%  \node[right=0 of cupp2] {$\pp{place2}$};
  %%   % \node[node] (kitchen) right of (cat) {};
  %% \end{tikzpicture}}
\qquad
  \subfloat[Removing facts that are not part of an assumption.]{
    \label{fig:abstraction-b}
    \begin{tikzpicture}[
    level 3/.style={tree}]
    \node[pnode, assumption] (cat) at (1,1) {} [toplevel, unused]
      child[seclevel] {node[cnode, assumption] (office) {}
        child {node [pnode, assumption] (box) {}
          child {node [cnode, unused] (boxp1) {}}
          child {node [cnode, assumption] (boxp2) {}}
        }
        child {node [pnode, unused] (cup) {} 
          child {node [cnode, unused] (cupp1) {}}
          child {node [cnode, unused] (cupp2) {}}
        }
      }
      child {node[cnode, selected] (kitchen) {}
        child {node [pnode, selected] (box2) {} 
          child {node [cnode, unused] (box2p1) {}}
          child {node [cnode, selected] (box2p2) {}}
        }
      };
   \draw[assumption] (cat) -- (office) -- (box) -- (boxp2);
   \draw[selected] (cat) -- (kitchen) -- (box2) -- (box2p2);
   \tiny
   \node[above=0 of cat] {$\pp{(category room1)}$};
   \node[below=0 of office] {$\pp{office}$};
   \node[below=0 of box] {$\pp{(is-in box)}$};
   \node[below=0 of box2] {$\pp{(is-in box)}$};
   \node[right=0 of boxp2] {$\pp{place2}$};
   \node[right=0 of box2p2] {$\pp{place2}$};
    % \node[node] (kitchen) right of (cat) {};
  \end{tikzpicture}}
\vspace{2mm}
  \subfloat[Refinement, by adding relevant facts.]{
    \label{fig:abstraction-c}
    \begin{tikzpicture}[
    level 3/.style={tree}]
    \node[pnode, assumption] (cat) at (1,1) {} [toplevel, unused]
      child[seclevel] {node[cnode, assumption] (office) {}
        child {node [pnode, assumption] (box) {}
          child {node [cnode, selected] (boxp1) {}}
          child {node [cnode, assumption] (boxp2) {}}
        }
        child {node [pnode, unused] (cup) {} 
          child {node [cnode, unused] (cupp1) {}}
          child {node [cnode, unused] (cupp2) {}}
        }
      }
      child {node[cnode, selected] (kitchen) {}
        child {node [pnode, selected] (box2) {} 
          child {node [cnode, selected] (box2p1) {}}
          child {node [cnode, selected] (box2p2) {}}
        }
      };
   \draw[assumption] (cat) -- (office) -- (box) -- (boxp2);
   \draw[selected] (box) -- (boxp1);
   \draw[selected] (cat) -- (kitchen) -- (box2) -- (box2p2);
   \draw[selected] (box2) -- (box2p1);
   \tiny
   \node[above=0 of cat] {$\pp{(category room1)}$};
   \node[below=0 of office] {$\pp{office}$};
   \node[below=0 of box] {$\pp{(is-in box)}$};
   \node[below=0 of box2] {$\pp{(is-in box)}$};
   \node[right=0 of boxp1] {$\pp{place1}$};
   \node[right=0 of boxp2] {$\pp{place2}$};
   \node[right=0 of box2p1] {$\pp{place1}$};
   \node[right=0 of box2p2] {$\pp{place2}$};
    % \node[node] (kitchen) right of (cat) {};
  \end{tikzpicture}}
  
  \caption{Abstract belief-state, and refinement.}
\label{fig:abstraction}
\end{figure}

% When limiting the number of actions, we aim to include those that are
% possibly required to a) reach the goal or b) to directly observe an
% atom that is part of the abstract belief state. In general, checking
% this for an action is \textsc{PSPACE} complete for \textsc{STRIPS}
% planning. 

% We approximate the set of relevant actions as follows: Let
% \relevantAtoms\ be the set of atoms that either occur in the goal
% condition \poss(\switchAction) or in a precondition \poss($a$) of an
% action $a$ that can directly sense any atom in the abstract belief
% state. We construct a \emph{relaxed planning graph} with
% \relevantAtoms\ as goal atoms from which we extract a relaxed
% plan. 



%% In the first step, we remove all facts that are not part of an
%% assumption (Fig. \ref{fig:abstraction-b}). At this point, the session
%% would proceed in an abstraction of the environment that does not
%% contain $\pp{place1}$, the $\pp{cup}$ or a $\pp{kitchen}$. 
%% %%
%% In a second step, we iteratively refine the relaxed declaration by
%% adding terms from the original statement of $(\pp{:init})$ while the
%% number of abstract states in $\bstate_0$ that occur with non-zero
%% probability according to that refined declaration remains of a
%% practicable size. In detail, 







%%% Local Variables: 
%%% mode: latex
%%% TeX-master: "aaai11"
%%% End: 
