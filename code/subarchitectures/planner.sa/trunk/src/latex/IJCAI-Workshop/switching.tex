
We now describe our {\em switching} planning system that operates
according to the continual planning paradigm. The system {\em
switches} in the sense that planning and plan execution proceed in
interleaved sessions in which the {\em base planner} is either {\em
sequential} or {\em decision-theoretic}.
%%
The first session is sequential, and begins when a DTPDDL description
of the current problem and domain are posted to the system.  During a
sequential session a serial plan is computed that corresponds to one
execution-{\em trace} in the underlying decision-process. That trace
is a reward-giving sequence of process actions and {\em assumptive}
actions. Each {\em assumptive} action corresponds to an assertion
about the truth of a fact that is unknown at plan time -- e.g. that a
box of cornflakes is located on the corner bench in the kitchen. The
trace specifies a plan and characterises a {\em deterministic
approximation} (see~\cite{yoon:etal:2008}) of the underlying process
in which that plan is valuable. Traces are computed by a
cost-optimising classical planner which trades off action costs, goal
rewards, and determinacy. Execution of a trace proceeds according to
the process actions in the order that they appear in the trace. If,
according to the underlying belief-state, the outcome of the next
action scheduled for execution is not predetermined above a threshold
(here 95\%), then the system switches to a DT session.


Because online DT planning is impractical for the size of problem we
are interested in, DT sessions plan in a small abstract problem
defined in terms of the trace from the proceeding sequential
session. This abstract state-space is characterised by a limited
number of propositions, chosen due to their {\em relevance} to the
action whose scheduled execution triggered the switch to the DT
session.
%%
To allow the DT planner to judge assumptions from the trace, we add
{\em disconfirm} and {\em confirm} actions to the problem for each of
them. Those yield a relatively small reward/penalty if the
corresponding judgement is true/false. If a judgement action is
scheduled for execution, then the DT session is terminated, and
a new sequential session begins.

%% Because online DT planning is impractical for the size of problem we
%% are interested in, when a DT session begins we use the trace from the
%% preceeding sequential session to restrict the state space. The state space is characterised by a limited number of propositions,
%% chosen due to their {\em relevance} to the action whose scheduled
%% execution triggered the switch to the DT session.
%% %The state
%% %space is constructed by first removing all propositions (and related
%% %actions) that are not true or assumed true of states in the trace,
%% %then adding them back as long as the number of states does not exceed
%% %a size threshold\footnote{Our experiments are performed using a number
%% %of different threshold parameters.}  Propositions to add back are
%% %chosen using as a heuristic the entropy of the trace assumptions
%% %conditional on the candidate proposition. 
%% To allow the DT planner to
%% determine the truth of the assumptions in the trace, we add {\em
%% disconfirm} and {\em confirm} actions to the problem for each of
%% them. These actions yield a relatively small reward/penalty if the
%% corresponding judgement is true/false. If a judgement action is
%% scheduled for execution the DT session is terminated, and control is
%% returned to a sequential session.

Whatever the session type, our continual planner maintains a factored
representation of successive belief-states.
% by performing belief revision. 
As an internal representation of the $(\pp{:init})$
declaration, we keep a tree-shaped Bayesian network which gets updated
whenever an action is performed, or an observation received. That
belief-state representation is used: (1) as the source of candidate
determinisations for sequential planning, (2) in determining when to
switch to a DT session, and (3) as a mechanism to guide construction
of an abstract process for DT sessions.

\subsection{Sequential Sessions}

As we only consider deterministic-action POMDPs, all state
uncertainty is expressed in the $(\pp{:init})$ declaration. This
declaration is used by our approach to define the starting state for
sequential sessions, and the set of state-assumptions available to
sequential planning.  Writing \#\ if the value of a proposition is
unspecified, taking the $(\pp{:init})$ example from the previous
section, we have the following assumptions:

\tiny
\begin{tabular}{cccc}
\hline
Probability & (is-in R2D2)  & (is-in box)  & (is-in cup) \\
\hline
%% .24 & kitchen & office & office \\
%% .06 & kitchen & kitchen & office \\
%% .56 & kitchen & office & kitchen \\
%% .14 & kitchen & kitchen & kitchen \\
.7 & kitchen & \# &  kitchen\\
.3 & kitchen & \# & office \\
.8 & kitchen & office & \# \\
.2 & kitchen & kitchen & \# \\
1.0 & kitchen & \# & \# \\
\hline
\end{tabular}
\normalsize

\noindent Each assumption corresponds to one distinct {\em
relaxed} visitation of the root term. Here, a conjunctive term is
visited iff its atomic subterms are visited, and zero or one of its
immediate probabilistic subterms are visited. For a sequential
session, the starting state corresponds to an assumption (usually
unique) with probability $1$.\footnote{Without a loss of generality,
we assume conjunctive terms list atoms that are true with
probability 1 according to a visitation, and that actions do not
have negative preconditions.}  In our example, that starting state is:

\vspace{-1ex}

\tiny
\[
\begin{array}{l}
\state_0 \equiv \{(=(\pp{is-in}~\pp{R2D2})~\pp{kitchen}),\\
\;\;(=(\pp{is-in}~\pp{box})~\#), (=(\pp{is-in}~\pp{cup})~\#)\}.
\end{array}
\]
\normalsize

\vspace{-1ex}

To represent state assumptions we augment the problem posed during
a sequential session with an \emph{assumptive action} $\assumptiveS{i}$ for
each element $\prob_i (T_i)$, of each probabilistic term from
$(\pp{:init})$. Here, $\assumptiveS{i}$ can be executed if no
$\assumptiveS{j}$, $j \neq i$, has been executed from the same
probabilistic term, and, either
$(\pp{probabilistic}~..\prob_i~(T_i)..)$ is in the root conjunct, or
it occurs in $T_k$ for some executed $\assumptiveS{k}$.
%%
We also add constraints that forbid scheduling of
assumptions about facts after actions with preconditions or effects
that mention those facts. For example, the robot cannot assume it is
plugged into a power source immediately after it unplugs itself.
%%
Executing $\assumptiveS{i}$ in a state $\state$ effects a transition
to a successor state $\state^{T_i}$ with probability $\prob_i$, and
$\state^\bot$ with probability $1 - \prob_i$. Here in addition to
respecting the above constraints, $\state^{T_i}$ is the union of
$\state$ with atomic terms from $T_i$. State $\state^\bot$ is an added
sink.


We now describe the optimisation criteria used during sequential
sessions. Let $\prob_i$ be the probability that the $i^{th}$ sequenced
action, $\action_i$, from a trace of state-action pairs
$\langle \state_0, \action_0,\state_1, \action_1,.., \state_N \rangle$
does not transition to $\state^\bot$, and
$\reward(\state_i, \action_i)$ be the instantaneous reward received
for executing action $\action_i$ in state $\state_i$. The optimal
trace has value:

%We now describe the optimisation criteria used during sequential
%sessions. Where $\prob_i$ is the probability that the $i^{th}$
%sequenced action, $\action_i$, from a trace of state-action pairs
%$\langle \state_0, \action_0,\state_1, \action_1,.., \state_N \rangle$
%does not transition to $\state^\bot$, we have that the optimal trace
%has value:

\tiny
\[
V^* = \max_N \max_{\state_0, \action_0,.., \state_N} \prod_{i=1..N-1} \prob_i \sum_{i=1..N-1}
\reward(\state_i, \action_i),
\]
\normalsize

%\noindent  Here,  $\reward(\state_i, \action_i)$ is the instantaneous
%reward ---i.e., can be negative for action costs, or positive for goal
%achievement--- received for executing action $\action_i$ in state
%$\state_i$. Finally, it is worth clarifying that in practise we do not
%artificially limit the length of traces that the sequential planner
%can consider. Moreover, for problems we consider there is always a
%finite optimal trace.

\subsection{DT Sessions}

During a sequential session, when an action is scheduled whose outcome
is uncertain according to the underlying belief-state, the planner
switches to a DT session. That plans for {\em small} abstract
processes defined according to the action that triggered the DT
session, the assumptive actions in the proceeding trace, and the
current belief-state. Targeted sensing is encouraged by augmenting the
reward model to reflect a heuristic value of knowing the truth about
assumptions. In detail, all rewards from the underlying problem are
retained. Additionally, for each relevant assumptive action
$\assumptiveS{i}$ in the current trace, we have a {\em disconfirm
action} $\assumptiveDT{i}$ so that for all states $\state$:

\vspace{-1ex}
\tiny
\[
\reward(\state, \assumptiveDT{i}) = \bigg\{ \begin{array}{ll}
\$(T_i) & \pp{if}~\;\;T_i \not\subseteq \state \\
\hat\$(T_i) & \pp{otherwise} \\
\end{array}
\]
\normalsize

\vspace{-1ex}

\noindent where $\$(T_i)$ (resp. $\hat\$(T_i)$) is a
small positive (negative) numeric quantity which captures
the utility the agent receives for correctly (incorrectly) rejecting
an assumption.
%%
In terms of action physics, a disconfirm action can only be executed
once, and otherwise is modelled as a self-transformation.
%%
We only consider {\em relevant} assumptions when constructing the
abstract model.  If \switchAction\ is the action that switched the
system to a DT session, then an assumption $\assumptiveS{i}$ is {\em
relevant} if it is necessary for the outcome of \switchAction\ to be
determined.  Continuing our simplified example, if the trace is:

\tiny
\[
\begin{array}{l}
\actions^{\circ}(.8;(=(\pp{is-in}~\pp{box})\pp{office}));\\
\actions^{\circ}(.3;(=(\pp{is-in}~\pp{cup})\pp{kitchen}));\\
(\pp{look}~\pp{box}~\pp{office});
(\pp{look}~\pp{cup}~\pp{kitchen});\\
(\pp{report}~\pp{box}~\pp{office}); 
(\pp{report}~\pp{cup}~\pp{kitchen})
\end{array}
\]
\normalsize

\noindent Taking the switching action \switchAction\ to be
$(\pp{look}~\pp{box}~\pp{office})$, we have that
$\actions^{\circ}(.3;(=(\pp{is-in}~\pp{cup})\pp{kitchen}))$ is not
relevant for this session, and therefore we exclude the corresponding
disconfirm action from the abstract decision process.

Given \switchAction, we also include another once-only self-transition
action $\actions.\poss(\switchAction)$, a \emph{confirmation action}
with the reward property:

\[
\reward(\state, \actions.\poss(\switchAction)) = \bigg\{ \begin{array}{ll}
\$(\poss(\switchAction)) & \pp{if}~\;\; \poss(\switchAction) \subseteq \state \\
\hat\$(\poss(\switchAction)) & \pp{otherwise} \\
\end{array}
\]

\noindent Here, \poss(\switchAction) is the list of positive literals
that comprise the precondition of \switchAction. Execution of
either a disconfirmation or the confirmation action returns control to
a sequential session, which then continues from the underlying
belief-state.

Turning to the detail of (dis-)confirmation rewards, in our integrated
system these are sourced from a motivational subsystem. In this paper,
for $\assumptiveDT{i}$ actions we set $\$(x)$ to be a small positive
constant, and have $\hat\$(x)= - \$(x)(1 - \prob) /
\prob$ where $\prob$ is the probability that $x$ is true. For
$\actions.\poss(\switchAction)$ actions we have $\hat\$(x)= -
\$(x)\prob/(1-\prob)$.



\Omit{
%%
Finally, 
If an
assumption was rejected, we prohibit that sequential session from
making it again.
}


%%  the size of the state space
%% is determined by two factors: the number of non-zero states in the
%% initial belief-state, and the number of actions. Determining
%% action reachability is intractable, as is determining their relevance
%% to goal achievement.

%% As we are aiming to keep the decision theoretic session fast, we try
%% to keep the state space of the abstract problem as small as
%% possible. In a deterministic-action POMDP, the state space size is
%% determined by two factors: the number of non-zero states in the
%% initial belief-state and the number of executable actions. As
%% determining the relevance of an action for a given goal in in general
%% a hard problem, we concentrate on keeping the number of uncertain
%% facts as low as possible without losing important information about
%% the sensing goal.

In order to guarantee fast DT sessions, planning occurs for a small
abstract problem. In our setting, where the underlying decision
process has a deterministic action model, a significant cause of
problem largeness is due to the number of states that occur with
non-zero probability in the underlying belief-state. We construct an
abstract belief-state that is chiefly characterised by facts related
to the current trace. In that abstraction, instances of operator or
sense declarations whose specification mentions an excluded fact are
not available.
%%
In detail, we first construct an $(\pp{:init})$ declaration which
contains the root term, and relevant assumptions from the trace---e.g., (Fig.\ref{fig:abstraction-b}) gives an example fragment, where
diamonds are probabilistic terms, and circles are atomic and/or
conjunctive.
%%
For each excluded atom, we compute the {\em entropy} of the {\em
relevant}
assumptions, {\em conditional} on that atom. Intuitively, lower
entropy indicates an atom gives better information about
assumptions. Facts are iteratively added to the belief-state in
increasing order according to that measure until the number of
abstract states in the initial belief reach predefined limit (Fig.\ref{fig:abstraction-c}).

%%  or no more atoms that reduce the entropy
%% can be added 

\vspace{-.5cm}
\begin{figure}[h!]
  \centering
  \tikzstyle{tree} = [sibling distance=4.5mm]
  \tikzstyle{toplevel} = [grow'=right, sibling distance=22mm]
  \tikzstyle{seclevel} = [sibling distance=9mm]
  \tikzstyle{pnode} = [diamond, draw=black, minimum size=2.5mm]
  \tikzstyle{cnode} = [circle, draw=black, minimum size=3mm]
  \tikzstyle{assumption} = [solid, very thick, draw=black]
  \tikzstyle{selected} = [solid, draw=black]
  \tikzstyle{unused} = [densely dashed, draw=black!40]
  %% \subfloat[The initial belief-state with the assumptions made by the continual
  %%   planner in bold.]{
  %%     \label{fig:abstraction-a}
  %%     \begin{tikzpicture}[
  %%   level 3/.style={tree}]
  %%   \node[pnode, assumption] (cat) at (1,1) {} [toplevel]
  %%     child[seclevel] {node[cnode, assumption] (office) {}
  %%       child {node [pnode, assumption] (box) {}
  %%         child {node [cnode] (boxp1) {}}
  %%         child {node [cnode, assumption] (boxp2) {}}
  %%       }
  %%       child {node [pnode] (cup) {} 
  %%         child {node [cnode] (cupp1) {}}
  %%         child {node [cnode] (cupp2) {}}
  %%       }
  %%     }
  %%     child {node[cnode] (kitchen) {}
  %%       child {node [pnode] (box2) {} 
  %%         child {node [cnode] (box2p1) {}}
  %%         child {node [cnode] (box2p2) {}}
  %%       }
  %%     };
  %%  \tiny
  %%  \draw[assumption] (cat) -- (office) -- (box) -- (boxp2);
  %%  \node[above=0 of cat] {$\pp{(category room1)}$};
  %%  \node[below=0 of kitchen] {$\pp{kitchen}$};
  %%  \node[below=0 of office] {$\pp{office}$};
  %%  \node[below=0 of box] {$\pp{(is-in box)}$};
  %%  \node[below=0 of box2] {$\pp{(is-in box)}$};
  %%  \node[below=0 of cup] {$\pp{(is-in cup)}$};
  %%  \node[right=0 of boxp1] {$\pp{place1}$};
  %%  \node[right=0 of boxp2] {$\pp{place2}$};
  %%  \node[right=0 of box2p1] {$\pp{place1}$};
  %%  \node[right=0 of box2p2] {$\pp{place2}$};
  %%  \node[right=0 of cupp1] {$\pp{place1}$};
  %%  \node[right=0 of cupp2] {$\pp{place2}$};
  %%   % \node[node] (kitchen) right of (cat) {};
  %% \end{tikzpicture}}
\qquad
  \subfloat[Removing facts that are not part of an assumption.]{
    \label{fig:abstraction-b}
    \begin{tikzpicture}[
    level 3/.style={tree}]
    \node[pnode, assumption] (cat) at (1,1) {} [toplevel, unused]
      child[seclevel] {node[cnode, assumption] (office) {}
        child {node [pnode, assumption] (box) {}
          child {node [cnode, unused] (boxp1) {}}
          child {node [cnode, assumption] (boxp2) {}}
        }
        child {node [pnode, unused] (cup) {} 
          child {node [cnode, unused] (cupp1) {}}
          child {node [cnode, unused] (cupp2) {}}
        }
      }
      child {node[cnode, selected] (kitchen) {}
        child {node [pnode, selected] (box2) {} 
          child {node [cnode, unused] (box2p1) {}}
          child {node [cnode, selected] (box2p2) {}}
        }
      };
   \draw[assumption] (cat) -- (office) -- (box) -- (boxp2);
   \draw[selected] (cat) -- (kitchen) -- (box2) -- (box2p2);
   \tiny
   \node[above=0 of cat] {$\pp{(category room1)}$};
   \node[below=0 of office] {$\pp{office}$};
   \node[below=0 of box] {$\pp{(is-in box)}$};
   \node[below=0 of box2] {$\pp{(is-in box)}$};
   \node[right=0 of boxp2] {$\pp{place2}$};
   \node[right=0 of box2p2] {$\pp{place2}$};
    % \node[node] (kitchen) right of (cat) {};
  \end{tikzpicture}}
\vspace{2mm}
  \subfloat[Refinement, by adding relevant facts.]{
    \label{fig:abstraction-c}
    \begin{tikzpicture}[
    level 3/.style={tree}]
    \node[pnode, assumption] (cat) at (1,1) {} [toplevel, unused]
      child[seclevel] {node[cnode, assumption] (office) {}
        child {node [pnode, assumption] (box) {}
          child {node [cnode, selected] (boxp1) {}}
          child {node [cnode, assumption] (boxp2) {}}
        }
        child {node [pnode, unused] (cup) {} 
          child {node [cnode, unused] (cupp1) {}}
          child {node [cnode, unused] (cupp2) {}}
        }
      }
      child {node[cnode, selected] (kitchen) {}
        child {node [pnode, selected] (box2) {} 
          child {node [cnode, selected] (box2p1) {}}
          child {node [cnode, selected] (box2p2) {}}
        }
      };
   \draw[assumption] (cat) -- (office) -- (box) -- (boxp2);
   \draw[selected] (box) -- (boxp1);
   \draw[selected] (cat) -- (kitchen) -- (box2) -- (box2p2);
   \draw[selected] (box2) -- (box2p1);
   \tiny
   \node[above=0 of cat] {$\pp{(category room1)}$};
   \node[below=0 of office] {$\pp{office}$};
   \node[below=0 of box] {$\pp{(is-in box)}$};
   \node[below=0 of box2] {$\pp{(is-in box)}$};
   \node[right=0 of boxp1] {$\pp{place1}$};
   \node[right=0 of boxp2] {$\pp{place2}$};
   \node[right=0 of box2p1] {$\pp{place1}$};
   \node[right=0 of box2p2] {$\pp{place2}$};
    % \node[node] (kitchen) right of (cat) {};
  \end{tikzpicture}}
  
  \caption{Abstract belief-state, and refinement.}
\label{fig:abstraction}
\end{figure}

% When limiting the number of actions, we aim to include those that are
% possibly required to a) reach the goal or b) to directly observe an
% atom that is part of the abstract belief state. In general, checking
% this for an action is \textsc{PSPACE} complete for \textsc{STRIPS}
% planning. 

% We approximate the set of relevant actions as follows: Let
% \relevantAtoms\ be the set of atoms that either occur in the goal
% condition \poss(\switchAction) or in a precondition \poss($a$) of an
% action $a$ that can directly sense any atom in the abstract belief
% state. We construct a \emph{relaxed planning graph} with
% \relevantAtoms\ as goal atoms from which we extract a relaxed
% plan. 



%% In the first step, we remove all facts that are not part of an
%% assumption (Fig. \ref{fig:abstraction-b}). At this point, the session
%% would proceed in an abstraction of the environment that does not
%% contain $\pp{place1}$, the $\pp{cup}$ or a $\pp{kitchen}$. 
%% %%
%% In a second step, we iteratively refine the relaxed declaration by
%% adding terms from the original statement of $(\pp{:init})$ while the
%% number of abstract states in $\bstate_0$ that occur with non-zero
%% probability according to that refined declaration remains of a
%% practicable size. In detail, 







%%% Local Variables: 
%%% mode: latex
%%% TeX-master: "aaai11"
%%% End: 
