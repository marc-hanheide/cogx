\section{Evaluation}\label{sec:evaluation}

To evaluate our progress toward building a cognitive system capable of reasoning about CDSRs, we conducted the an experiment focusing on the following questions:
\begin{itemize}
\item{Are anchor points able to encode context-dependent spatial regions?}
\item{When provided with a base representation containing a labelled CDSR, how well does our approach identify the CDSR in a given target?}
\end{itemize}

\subsection{Materials}

We evaluated our approach on six classrooms (two simulated and four real) and two simulated studio apartments. The simulated rooms were based on real-life counterparts. For each room we manually encoded the appropriate CDSRs that could be represented by our approach. For the classrooms these were the front and back, and the front and back rows of desk. For the studios these were the kitchen, office and living areas. These manually encoded regions were used as the base CDSRs for analogical transfers, and can be considered the training data for our evaluation. 

To determine how people define CDRSs, we asked three na\"ive users to draw polygons for each region type for each room. This task was performed using a webpage (link removed for blind review) shown in the inset in Figure~\ref{fig:ug40} on which each user was presented with an image of the real room plus the map data generated by the robot (as in Figure~\ref{fig:rooms}) onto which the drawing could be done. The user-defined polygons define the \textit{target regions} against which we evaluate our transfers.

We consider a \textit{problem instance} to be a room and a sought CDSR type. For each room for which we have a human-labelled target region of the sought type, we generate an \textit{inferred region} using analogical transfer. To assess the quality of the transfer, we calculate precision ($p$, the proportion of the inferred region that overlaps with the target region) and recall ($r$, the proportion of the target region that overlaps with the inferred region) as follows:

\begin{equation}
	p=\frac{area(inferred\ region \cap target\ region)}{area(inferred\ region)}
\end{equation}
\begin{equation}
	r=\frac{area(inferred\ region \cap target\ region)}{area(target\ region)}
\end{equation}

Using this approach we generate results showing the matches between each of the following pairs of regions: the transferred region and the appropriate target region; the manually annotated CDSR in the target and appropriate target region; and the region for the whole room and the target region. 
 Results comparing transferred and target regions measure how well the output of our system matches user expectations. Results comparing the manual annotations to the target regions measure how well the anchor point-based regions we chose match the users' regions (who were not constrained to anchor points). Results comparing whole rooms to target regions provide a baseline performance for comparison. 


\subsection{Results}
\begin{table*}
	\center
\begin{tabular}{|c|c|c|}
\hline
Inferred & Manually Encoded & Entire Room \\
\hline
$\bar{p}$=.45 $\sigma$=.37, $\bar{r}$=.45 $\sigma$=.38 & $\bar{p}$=.71 $\sigma$=.29, $\bar{r}$=.66 $\sigma$=.24  & $\bar{p}$=.17 $\sigma$=.11, $\bar{r}$=.98 $\sigma$=.05  \\
\hline
\end{tabular}
\caption{Overall Performance}
  \label{tab:overall}	

\end{table*}

\from{klenk}{I guess we should rotate the above table. And it has been awhile since I wrote academic, so feel free to cut the boring text below}

Table~\ref{tab:overall} summarizes the results by combining each problem instance and user-defined target region. Both precision and recall for the inferred regions is .45 with a standard deviation of .37 and .38 respectively. When comparing the manually encoded regions against each target instance, we calculate a mean precision of .71 ($\sigma$=.29) and recall of .66 ($\sigma$=.24). The region defined by the room results in a precision of .17 ($\sigma$=.11) and recall of .98 ($\sigma$=.05).

% these are sensible results, nice

\begin{table*}
	\center
\begin{tabular}{|c|c|c|c|}
\hline
Region & Inferred & Manually Encoded & Entire Room \\
\hline
Front & $\bar{p}$=.33 $\sigma$=.34, $\bar{r}$=.52 $\sigma$=.40 & $\bar{p}$=.60 $\sigma$=.29, $\bar{r}$=.82 $\sigma$=.19  & $\bar{p}$=.15 $\sigma$=.09, $\bar{r}$=1 $\sigma$=0  \\
\hline
Back & $\bar{p}$=.45 $\sigma$=.35, $\bar{r}$=.56 $\sigma$=.40 & $\bar{p}$=.66 $\sigma$=.25, $\bar{r}$=.84 $\sigma$=.17  & $\bar{p}$=.11 $\sigma$=.06, $\bar{r}$=.99 $\sigma$=.03  \\
\hline
Front Rows & $\bar{p}$=.62 $\sigma$=.36, $\bar{r}$=.22 $\sigma$=.21 & $\bar{p}$=.82 $\sigma$=.30, $\bar{r}$=.50 $\sigma$=.11  & $\bar{p}$=.22 $\sigma$=.08, $\bar{r}$=1 $\sigma$=0  \\
\hline
Back Rows & $\bar{p}$=.74 $\sigma$=.28, $\bar{r}$=.43 $\sigma$=.23 & $\bar{p}$=.80 $\sigma$=.29, $\bar{r}$=.43 $\sigma$=.26  & $\bar{p}$=.19 $\sigma$=.06, $\bar{r}$=1 $\sigma$=0  \\
\hline
Kitchen & $\bar{p}$=.23 $\sigma$=.03, $\bar{r}$=.09 $\sigma$=.02 & $\bar{p}$=.78 $\sigma$=.20, $\bar{r}$=.71 $\sigma$=.13  & $\bar{p}$=.16 $\sigma$=.02, $\bar{r}$=.92 $\sigma$=.13  \\
\hline
Office & $\bar{p}$=.05 $\sigma$=.01, $\bar{r}$=.06 $\sigma$=.06 & $\bar{p}$=.78 $\sigma$=.29, $\bar{r}$=.55 $\sigma$=.20  & $\bar{p}$=.08 $\sigma$=.03, $\bar{r}$=.93 $\sigma$=.06  \\
\hline
Living Room & $\bar{p}$=0 $\sigma$=0, $\bar{r}$=0 $\sigma$=0 & $\bar{p}$=.63 $\sigma$=.34, $\bar{r}$=.54 $\sigma$=.13  & $\bar{p}$=.35 $\sigma$=.22, $\bar{r}$=.96 $\sigma$=.06  \\
\hline
\end{tabular}
\caption{Performance by Region Type}
  \label{tab:region}	
\end{table*}

Table~\ref{tab:region} separates the results by CDSR type. The mean precision for the transferred regions ranged from .74 for the back rows to 0 for the living room of the studio apartment. Comparing manually encoded against  target regions resulted in a minimum mean precision of .60. This  occurred for the front of the classroom. The whole room precision, which is directly proportionally to the size of the target region, varied from .08 for the office to .35 for the living area. 


%(3 :transfer-p (0.5649091160864536d0 . 0.3824746399624735d0)  :transfer-r (0.4264200799557943d0 . 0.34846457388485863d0) :self-p   (0.8290901572528112d0 . 0.19630847005535487d0) :self-r
%  (0.6113880445364026d0 . 0.22432810023132582d0) :room-p
%  (0.20903207305733315d0 . 0.10291204447091876d0) :room-r
%  (0.9960679285191474d0 . 0.011022662977174953d0))
% (2 :transfer-p (0.4496053675639215d0 . 0.36845133415546233d0)
%  :transfer-r (0.4174204919721308d0 . 0.3697742376406773d0) :self-p
%  (0.7556540963495391d0 . 0.2672139547143468d0) :self-r
%  (0.6252647119318083d0 . 0.23778540411202373d0) :room-p
%  (0.17891943994602752d0 . 0.11273973943366206d0) :room-r
%  (0.95742701954372d0 . 0.08432884603590181d0))
% (1 :transfer-p (0.3360804531954755d0 . 0.3364697224078454d0)
%  :transfer-r (0.5074731366878895d0 . 0.4231938085019586d0) :self-p
%  (0.5493333659045643d0 . 0.33190756014975015d0) :self-r
%  (0.7596761147032691d0 . 0.25387231566349d0) :room-p
%  (0.12277614149421091d0 . 0.10953121808467986d0) :room-r
%  (0.9946881982552835d0 . 0.011992196508589958d0))

\subsection{Discussion}

These results support the hypothesis that anchor points can provide a symbolic representation on top of sensor data for context-dependent spatial regions, and, when combined with qualitative spatial relations, they facilitate learning from a single example through analogical transfer. The relatively high manually encoded precisions and recalls indicate that the defined anchor points are a reasonable starting place for a symbolic representation. The lower scores typically resulted from target regions which were farther away from objects. While the inferred regions did not score as well as the manually encoded regions, they were considerably more precise than the room regions. The only case were transfer was not possible was between the two studio's living room cases. The objects which define the living area in one studio (sofa, TV, and bookshelves) are significantly different than the objects in the other (desk, bed, and sofa), as are the studios in general.

