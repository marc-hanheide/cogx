

We describe our continual planning approach that switches between
sequential and contingent sessions. Continual planning and execution
proceeds in a deterministic-action POMDP described in DTPDDL. During a
sequential session, planning is performed by a ``classical''
system,\footnote{That is, a system designed to solve fully observable
deterministic tasks.}  and execution proceeds according to the {\em
trace} computed by that system. Taking the form of a classical plan,
the {\em trace} specifies a sequence of POMDP actions that achieve the
agent's objectives in a deterministic approximation, i.e., {\em
determinisation}, of the problem at hand. More precisely, the trace is
an interleaved sequence of POMDP actions and {\em assumptive}
actions. The latter correspond to assumptions the planner makes about
the truth value of {\em runtime state variables} -- e.g. that a box of
cornflakes is located in the kitchen at the third plan step. Another
kind of assumption, called an {\em applicability} assumption, is made
when the trace includes an action $\action$ that is not applicable
with probability $1$ at the belief-state
\bstate\ that the system is projected to be in when \action\ is executed, 
i.e., $\exists\state\in\states
\bstate(\state) > 0$ and $\poss(\action)\not\subseteq\state$. By
scheduling
\action,  the serial planner makes an assumption about the
observability of the precondition $\poss(\action)$.

Our approach always begins with a sequential session. Non-assumptive
actions from the trace are executed until the applicability of the
next scheduled action, here \action, is questionable. A contingent
session then begins, which tailors sensory processing by the agent to
perceive whether the assumptions made in the trace hold. For each
active assumption in the trace, we add a {\em disconfirm} action to
the POMDP whose execution is rewarding (resp. costly) if the
assumption is false (resp. true). We also add one {\em confirm} action
that is rewarding (resp. costly) to execute if $\poss(\action)$ is
true (resp. false). If execution of the contingent plan applies a
disconfirm action, then a new sequential session begins at the current
underlying belief-state. If the confirm action is executed, the
sequential session is resumed, and \action\ is executed. Otherwise,
control rests in the continual session.
%%
Finally, because contingent planning is only practical in relatively
small POMDPs, contingent sessions proceed in an abstract decision
process determined by the current trace. That abstract process is
constructed by first omitting all variables from the model that are
not featured in the trace, and by then iteratively refining that
abstraction while the resulting model is of a practicable size.


PLEASE STICK A PARAGRAPH HERE, (SENSIBLY) REQUESTED BY MORITZ, ABOUT
HOW WE DO BELIEF REVISION ACROSS SWITCHING.

\Omit{
  Therefore, although our current implementation does not
  explicitly support it, our approach generalises to POMDPs with
  stochastic action models.
}

\subsection{Sequential Planning in POMDPs}


Our approach uses sequential planning to compute a linear plan, and
one trace of that plan's execution. The latter encapsulates
assumptions about: (1) the true underlying state, (2) how the plan's
execution will progress, and (3) the possibility of the agent
eventually holding strong beliefs about the truth values of specific
state propositions.

In a deterministic-action POMDP, all the uncertainty in state and
action is expressed in the $(\pp{:init})$ declaration. Our approach
uses the structure of that, as it occurs in the problem description,
to define a set of state-assumptions available to sequential planning.
%%
Writing \#\ if the value of a proposition is unspecified in an
assumption, for our $\pp{DORA}$ example we have the following
assumptions:

\small
\begin{tabular}{cccc}
\hline
Probability & (is-in DORA)  & (is-in box)  & (is-in cup) \\
\hline
.24 & kitchen & office & office \\
.06 & kitchen & kitchen & office \\
.56 & kitchen & office & kitchen \\
.14 & kitchen & kitchen & kitchen \\
.7 & kitchen & \# &  kitchen\\
.3 & kitchen & \# & office \\
.8 & kitchen & office & \# \\
.2 & kitchen & kitchen & \# \\
1.0 & kitchen & \# & \# \\
\hline
\end{tabular}
\normalsize

\noindent A valid assumption in our setting corresponds to a {\em
relaxed} visitation of the root term of $(\pp{:init})$. In this relaxed
case, a conjunctive term is visited iff its atomic subterms are
visited, and zero or more of its immediate probabilistic subterms are
visited. The starting state, $\state_0$, for sequential planning is
the unique abstract state with probability $1.0$ -- e.g., for
$\pp{DORA}$:

\[
\begin{array}{l}
\state_0 \equiv \{(=(\pp{is-in}~\pp{DORA})~\pp{kitchen}),\\
\;\;(=(\pp{is-in}~\pp{box})~\#), (=(\pp{is-in}~\pp{cup})~\#)\}.
\end{array}
\]

In order to make assumptions available to the sequential planner, we
add one {\em assumptive} action $\assumptiveS{i}$ to the problem for
each element, $p_i (T_i)$, of each probabilistic term from
$(\pp{:init})$. The physics of these actions is as
follows. $\assumptiveS{i}$ can be executed if no $\assumptiveS{j}$,
$j \neq i$, has been executed from the same probabilistic term, and,
either $(\pp{probabilistic}~..p_i~(T_i)..)$ is in the root conjunct,
or it occurs in $T_k$ for some executed $\assumptiveS{k}$.
%%
Executing $\assumptiveS{i}$ in $s$ effects a transition to a successor
state $s^{T_i}$ with probability $p_i$, and $s^\bot$ with probability
$1 - p_i$. Here, $s^{T_i}$ is the union of $s$ with atomic terms from
$T_i$ and conflicting propositions from $s$ removed. For example, if
the action adds $(=(\pp{is-in}~\pp{box})~\pp{office})$, then it must
delete $(=(\pp{is-in}~\pp{box})~\#)$. State $s^\bot$ is a sink, where
$\forall\action\in\actions$ $\reward(\state^\bot, \action) = 0$ and
$\transProb(\state^\bot, \action, \state^\bot) = 1$.

Because $\state_0$ is abstract, and because actions can have
preconditions that are not uniformly true in the underlying
(resp. abstract) belief-state, we must make two further adjustments to
the problem posed to the sequential planner. First, if
$\action\in\actions$ has a precondition that is not true with
probability $1$ in $\bstate_0$, then before it is executed blindly,
the sequential continual planning switches to a decision-theoretic
session, discussed in detail below. Overall, by scheduling $\action$
the sequential planner makes a naive assumption about the possibility
of the agent eventually ``committing'' to the truth values of specific
state propositions in $\poss(\action)$. The role of decision-theoretic
plans is to examine the validity of assumptions about the runtime
state of sequential planning. Second, if in the current state \state\
the value of $\prop$ is $\#$ ---because a truth value is not specified
in $\state_0$ and no assumption is made about it in the plan
to \state--- then if $\prop \in \poss(\action)$, or if $\prop$ is the
subject of a positive or negative effect of $\action$, we have that
action $\action$ is not applicable in \state.

Finally, we come to valuing sequential plans, and therefore giving a
criteria for the sequential planner to optimise. Where $p_i$ is the
probability that the $i^{th}$ sequenced action does not transition to
$s^\bot$, we value a trace as follows:

\begin{equation}\label{eq:tracevalue}
V(s_0, a_0, s_1, a_1,.., s_N) =  \prod_{i=1..N-1} p_i \sum_{i=1..N-1} \reward(\state_i, \action_i)
\end{equation}

\noindent The optimal trace given a plan, i.e., the non-assumptive
actions in sequence, therefore has value:

\[
V^* = \max_{p_1, .., p_n} \prod_{i=1..N-1} p_i \sum_{i=1..N-1}
\reward(\state_i, \action_i),
\]

\noindent which is equal to the maximal contribution a trace of the plan can
make to Eq~\ref{eq:expectedvalue}. %% In other words, writing $V_i$ for
%% the value of the $i^{th}$ trace of the plan expressed in $s_0, a_0,
%% s_1, a_1,.., s_N$, we have
%%
%% \[
%% V^* = \arg\min_{V_i} \Expect \bigg{[} 
%% \sum_{t=0}^{N-1}  \reward(\bstate_t, \pp{PLAN}_t) \mid \pp{PLAN}, \bstate_0
%% \bigg{]} - V_i.
%% \]
%%
In goal directed problems, where the only non-zero reward is received
at the first transition to a goal state, the Eq~\ref{eq:tracevalue}
criteria corresponds exactly to that of
\system{FFR$_a$}~\cite{yoon:etal:2007}. From the perspective of
gradient-based reinforcement learning systems, such as William's
REINFORCE and GPOMDP variants for planning~\cite{olivier:doug:2009},
under reasonable assumptions the optimal trace identifies a maximal
gradient step from a uniformly random soft-max policy. Finally, it is
worth clarifying that although the above discussion has been in terms
of a trace of length $N$, there is nothing in our approach that
artificially limits the length of candidate sequential plans.


\subsection{Decision-Theoretic Planning in Abstractions}

In a decision-theoretic session, the switching planner solves a sensing
problem in an abstract processes defined in terms of the assumptive
actions in the current sequential plan, and the $(\pp{:init})$ declaration
from the original problem description. 

The focus for decision-theoretic planning is on sensing, rather than
the achievement of utility according to the given objectives. We
therefor give the process in this session a different reward model, which
reflects the value of performing sensing actions in the context of the
trace proposed for the preceding sequential deliberation. In detail,
all positive rewards from the initial POMDP are removed. Then, for
each $\assumptiveS{i}$ action scheduled in the current sequential
plan, we have a {\em dual} $\assumptiveDT{i}$ so that
$\forall\state\in\states$:

\[
\reward(\state, \assumptiveDT{i}) = \bigg\{ \begin{array}{ll}
\$(T_i) & \pp{if}~\;\;T_i \not\subseteq \state \\
-\$(T_i) & \pp{otherwise} \\
\end{array}
\]

\noindent Where $\$(T_i)$ is a positive numeric quantity which
captures the reward the agent receives for correctly jettisoning a
false assumption. The added dual actions are propositionally trivial
-- i.e., $\forall\state\in\states$ $\poss(\assumptiveDT{i})
\subseteq \state$ and $\assumptiveDT{i}(\state) \equiv \state$.  Also, for the action,
$\action_i$, whose scheduled execution triggered the switch to the
decision-theoretic session, we include another propositionally trivial
action $\actions.\poss(\action_i)$ with the reward property:

\[
\reward(\state, \actions.\poss(\action_i)) = \bigg\{ \begin{array}{ll}
\$(\poss(\action_i)) & \pp{if}~\;\; \poss(\action_i) \subseteq \state \\
-\$(\poss(\action_i)) & \pp{otherwise} \\
\end{array}
\]


For the management of switching, execution of either a dual action of
the form $\assumptiveDT{i}$, or the $\actions.\poss(\action_i)$,
returns control to the sequential planner. At that point, if a dual is
executed, then the sequential planner must replan with $\bstate_0$
equal to the current belief. Otherwise, if $\actions.\poss(\action_i)$
is executed, then the current sequential plan is executed until
further sensing is scheduled, or to completion. In our project the
rewards, i.e., function $\$:2^\propositions\to\Re$, for the POMDP
posed to decision-theoretic planning are sourced from a motivational
component of our robotic architecture.

The starting belief-state for the abstract process is given in terms
of a {\em relaxed} $(\pp{:init})$ declaration.  We first construct a
tree which only includes terms from the {\em relaxed} visitation
characterised by the {\em assumptive} actions in the current
sequential plan. For example, if $\pp{DORA}$'s plan is:

\[
\begin{array}{l}
\actions^{\circ}(.8;(=(\pp{is-in}~\pp{box})\pp{office}));\\
(\pp{look-for-object}~\pp{DORA}~\pp{box}~\pp{office});\\
(\pp{report-is-in}~\pp{box}~\pp{office})
\end{array}
\]

\noindent Then the {\em relaxed} declaration will be,

\small
\begin{tabtt}
(\=:init (= (is-in DORA) kitchen) \+ \\
       (probabilistic \=.8 (= (is-in box) office)  \\
		      \>.2 (= (is-in box) kitchen))) \\
\end{tabtt}
\normalsize

\noindent and therefore, the belief-state $\bstate_0$ is:

\small
\begin{tabular}{cccc}
\hline
Probability & (is-in DORA)  & (is-in box)  & (is-in cup) \\
\hline
.8 & kitchen & office & \# \\
.2 & kitchen & kitchen & \# \\
\hline
\end{tabular}
\normalsize

\noindent At this point, decision-theoretic planning would proceed in an
abstraction of the environment that does not contain a $\pp{cup}$,
where the inclusion (exclusion) of POMDP actions is the same as we
defined it for the sequential session.

In a second step, we iteratively refine the above{\em relaxed}
declaration by adding terms from the original statement of
$(\pp{:init})$ while the resulting state space remains of a
practicable size. Of course, it is expensive to measure size exactly,
and in practice we estimate this to be $2^{|\propositions^\#|}$, where
$\propositions^\#$ is the set of propositions in the abstraction that
were not assigned the {\em unknown} value \#. Terms are proposed for
addition because they are probabilistic, and because they were omitted
in the {\em relaxed} visitation characterised by {\em assumptive}
actions in the sequential plan. In each iteration, we add one
candidate term with the largest number of parent terms already in the
candidate tree.








