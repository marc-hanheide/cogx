

% File: formatting-instruction.tex
\documentclass[letterpaper]{article}
\usepackage{aaai}



\usepackage{times}
\usepackage{helvet}
\usepackage{courier}

\include{macros}%

\nocopyright

\pdfinfo{
/Title (Switching in Continual Planning for Practical Robot Control)
/Subject (Proceedings of the 21st International Conference on Automated Planning and Scheduling)
/Author (NA)}
% The file aaai.sty is the style file for AAAI Press 
% proceedings, working notes, and technical reports.
%
%\title{Switching in Continual Planning for Practical Robot Control}
\title{A Switching Planner for Combined Task and Observation Planning}
%\title{A Continual Planner that Switches Between Sequential and
%  Decision-Theoretic Planning} 
\author{NA}
\setcounter{secnumdepth}{0}


\begin{document} 
\maketitle

\begin{abstract}

Realistic robot planning problems in uncertain environments often
require achieving tasks while also finding out about the
world. Because the world state cannot be observed directly, these
problems are naturally represented as partially observable Markov
decision problems (POMDPs). However, these are typically intractable
for realistic problems.

We present a \emph{switching planner} that employs fast sequential
planning to decide on the overall strategy, and uses a
decision-theoretic planner to solve the subproblems where partial
observability will significantly impact the quality of the plan. We
demonstrate the approach in a realistic robot exploration domain.

\end{abstract}

\section{Moritz's Initial Outline}

\subsection{Switching Planner Outline}

Outline the high level approach here:
\begin{itemize}
\item Problem input (factored initial state, DTPDDL observations)
\item Determinisation
\item Problem generation for DT
\item Belief revision
\end{itemize}

\subsection{Determinisation}

\begin{itemize}
\item Generation of deterministic sensing models
\item Determinisation of the initial state
\item Compiling reward model into (soft-)goal model
\item Cost function in FD (maybe this could mentioned in the outline?)
\end{itemize}

\subsection{Subtask Generation}
\begin{itemize}
\item Goal generation from CP trace
\item Disconfirm actions
\item State pruning according to CP trace
\end{itemize}

\subsection{Belief revision}
Not sure if this needs its own section
\begin{itemize}
\item Convert inital state into bayesian network
\item Belief updates with observation nodes
\item Create new network for next planning runs
\end{itemize}

\section{Introduction}


Any POMDP with stochastic actions can be compiled into an equivalent
deterministic-action POMDP with all the action uncertainty expressed
in the starting-state distribution~\cite{ng:Jordan:2000}. In our
setting, that of finite-horizon planning, this compilation yields a
finite flat representation of the POMDP. Therefore, although our
current implementation does not explicitly support it, our approach
generalises to POMDPs with stochastic action models.




We describe, \pcogx, the planning component in a robotic system that
continuously deliberates in a stochastic dynamic environment in order
to achieve objectives set by the user, and acquire knowledge about its
surroundings.
%%

Have a good understanding of the contents and function of rooms, as
well as the linguistic referrents to rooms, widgets, and their visual
qualities.

Having confidence in its beliefs about the linguistic terms for
spaces, and the function of those spaces.



sometimes difficult to predict, with exogenous events, such as the
changing of the objective, 





the speed and scalability of software for sequential planning in
deterministic




We developed a first-order declarative language, called DTPDDL, for
describing domains of planning problems that correspond to POMDPs.  To
accompany that language we have implemented an information-state
\laostar\ procedure for solving problems expressed in DTPDDL. 

We have extended MAPSIM to parse and simulate DTPDDL problems, and
modified \fastdownward\ so that it can find useful sequential executions
given DTPDDL models of the problem at hand.




 called DTPDDL, along
the lines of PDDL for the partially observable case, an \laostar\
solution procedure, a determinisation of the DTPDDL problem in MAPL,
and modify the \fastdownward\ system to find high quality sequential plans
.

We model the environment as a partially observable Markov decision
process. Although planning in that model is undecidable in
general~\cite{}, an optimal finite-horizon plan corresponds to a
contingent plan, that is a function mapping action-observation
histories to actions.

our continual planner is reactive, replanning whenever the underlying
domain and problem models change. For example, replanning occurs if
the motivational component alters the objectives, and if an assumed
outcome of a sensing action is not realised.

brittle sensing model, 

the evaluation of a fluent at a state is either known. Moreover, there
is a sensing process that run-time variables  after-which 

%% For $\prop \in \state$ we say proposition $\prop$ characterises state
%% $\state$. There is always a unique starting state $\state_0$. The goal
%% $\goal$ is a set of propositions, and we say that state $\state$ is a
%% goal state iff $\goal \subseteq \state$.

%%  To keep this exposition
%% simple, for any two distinct actions $\stochAction_i \neq
%% \stochAction_j$, if outcome $\detAct$ is a possibility for
%% $\stochAction_i$ then it cannot also be a possibility for
%% $\stochAction_j$ -- i.e., if $\detAct \in
%% \detActions(\stochAction_i)$ then $\detAct \not\in
%% \detActions(\stochAction_j)$.


%% The solution to a probabilistic planning problem is a contingency
%% plan. This consists of an assignment of actions to states at each
%% discrete timestep up to the planning horizon $n$. The optimal
%% contingency plan is one which prescribes actions to states that
%% maximise the probability that the goal is achieved within $n$ steps
%% from the starting state $\state_0$. For the purposes of this paper
%% we say a plan fails, i.e. achieves the goal with probability $0$, in
%% situations where it does not prescribe an action. Computing the
%% optimal plan for a problem is computationally intractable, and an
%% important direction for research in the field is to develop
%% heuristic mechanisms for generating small linear plans
%% quickly~\cite{littman:etal:98}

An outline of the paper is as follows. We describe POMDPs with a
propositionally factored representation of states and observations,
and describe how to evaluate plans when they correspond to
finite-state controllers.

\section{Propositional Decision-Theoretic Planning}


%% {\em runtime variables}
%% {\em decision variables}
%% {\em uninitialised variables}
%% {\em omitted variables}

We describe the partially observable propositional probabilistic
planning problem, with costs and rewards. The underlying process
dynamics are modelled in terms of a finite set of stochastic actions
$\stochActions$, deterministic outcomes $\detActions$, and
state-characterising propositions $\propositions$.

We model a process state $\state$ as the set of propositions that are
true of the state. Notationally, we have $\state \subseteq
\props$. State change is induced by application of actions. A
stochastic action $\stochAction \in \stochActions$ is applicable if
its precondition $\poss(\stochAction)$, a set of propositions, are
satisfied in the current state -- I.e., $\poss(\stochAction) \subseteq
\state$. We write $\stochActions(\state)$ for set of actions
applicable at state $\state$.  If action $\stochAction \in
\stochActions(\state)$ is applied at \state, nature chooses one
element amongst a small set of deterministic outcomes
$\detActions(\stochAction) \equiv \{\detAction_1, \ldots, \detAction_k
\}$. We denote $\mu_{\stochAction}(\detAction_i)$ the probability that
nature takes outcome $\detAct_i$, and for all \stochAction\ we require
$\sum_{\detAction_i \in \detActions(\stochAction)}
\mu_{\stochAction}(\detAction_i) = 1$. The chosen outcome has an
effect on the state given in terms of two lists of propositions. The
add-list $\add(\detAction)$ and delete-list
$\delete(\detAction)$.\footnote{If a proposition is in the add-list of
$\detAction$, then it cannot be in the delete-list and vice versa.}
If outcome $\detAction$ with $\add(\detAction) := [\prop_1,
..,\prop_n]$ and $\delete(\detAction) := [\overline\prop_{n+1},
..,\overline\prop_m]$ is chosen by nature when \stochAction\ is
applied at state $\state$, then the resultant state is $ ( \state \cup
\add(\detAction) ) \backslash \delete(\detAction)$ -- i.e.,
propositions from $\add(\detAction)$ are added to $\state$, and those
from $\delete(\detAction)$ are removed from $\state$.

We are concerned with problems that feature partial
observability. These have a perceptual model given in terms of a
finite set of stochastic {\em senses} $\stochSenses$, deterministic
sensing outcomes $\detSenses$, and perceptual propositions
$\percepts$, called {\em percepts}. Here, an observation $\observ$ is
a set of percepts $\observ \subseteq \percepts$, and we denote
\observations\ the set of reachable observations. The underlying state
of the process cannot be observed directly, rather, senses
$\stochSense \in \stochSenses$ effect an observation $\observ \in
\observations$ that informs what should be believed about the state a
process is in. In detail, if $\action$ is applied effecting a
transition to a successor state $\state'$, then an observation occurs
according to the active senses $\stochSenses(\stochAction, \state')
\subseteq \stochSenses$. A sense $\stochSense$ is active, written
$\stochSense \in \stochSenses(\stochAction, \state')$, if the senses'
action-precondition, $\poss_\stochActions(\stochSense)$, is equal to
$\stochAction$, and the state-precondition $\poss_\states(\stochSense)
\subseteq \propositions$ is satisfied by the state $\state'$; In the
usual sense that $\poss_\states(\stochSense) \subseteq \state'$.
%%
When a sense is active, nature must choose exactly one outcome amongst
a small set of deterministic choices $\detSenses(\stochSense)
\equiv \{\detSense_1, \ldots, \detSense_k \}$, so that for each
$i$ we have $\detSense_i \subseteq \percepts$. The probability of
the $i^{th}$ element being chosen is given by
$\psi_{\stochSense}(\detSense_i)$, where $\sum_{\detSense_i \in
\detSenses(\stochSense)} \psi_{\stochSense}(\detSense_i) =
1$. The observation received by the agent corresponds to the union of
perceptual propositions from chosen elements of active senses.

A POMDP has a starting configuration that corresponds to a Bayesian
belief-state. Intuitively, this is the robot's subjective belief about
its environment. Formally, a belief-state $\bstate$ is a probability
distribution over process states. We write $\bstate(\state)$ to denote
the probability that the process is in $\state$ according to
$\bstate$, and $\bstate_0$ when discussing the starting
configuration. 

Finally, we make a number of fairly standard assumptions. First, that
action execution and sensing occurs instantaneously, and that only one
action can be applied at a plan-step. Second, it can arise that in
some propositional states an action is applicable, and in others it is
not. Moreover, a belief-state can assign non-zero probability to
states where that applicability holds, and states where it does
not. We differ
from~\citeauthor{younes:littman:04}~(\citeyear{younes:littman:04}),
because we forbid the execution of actions that are not applicable in
any state of the current belief.  Also,
unlike~\citeauthor{hoffmann:brafman:2006}~(\citeyear{hoffmann:brafman:2006}),
we do incorporate a PPDDL-like default semantics, supposing that when
an action \stochAction\ is executed at $\state_i$, that it has no
effect on states $\state_i$ if $\poss(\stochAction) \not\subseteq
\state_i$.


\subsection{Costs, Rewards, and Belief Revision}

Whereas until now we have considered a POMDP model that is factored in
terms of propositions and percepts, in order to discuss utilities and
policies it is convenient to consider the underlying decision process
in a flat format. This underlying decision process is given by the
tuple $\langle \states, \bstate_0, \actions, \transProb, \reward,
\observations, \obsDist \rangle$. Here $\bstate_0$ is the initial
belief-state, \states\ is the finite set of reachable propositional
states, \actions\ is the finite set of actions, and \observations\ is
the finite set of reachable observations (i.e., perceptual states).
Where $\state, \state' \in \states$, $a \in \actions$, from $\mu$ we
have a state transition function $\transProb(\state, \action,
\state')$ giving the probability of a transition from state $s$ to
$s'$ if $a$ is applied. For any $\state$ and $\action$ we have
$\sum_{\state' \in \states} \transProb(\state, \action, \state') = 1$.
%%
Function $\reward:\states \times \actions \to \Re$ is a bounded real
valued reward function. Therefore a finite positive constant $c$
exists so that for all $\state \in \states$ and $\action \in
\actions$, $|\reward(\state, \action)| < c$. We model costs are
negative rewards.
%%
From $\psi$ we have that for each $\state \in \states$ and action
$\action \in \actions$, an observation $\observ \in \observations$ is
generated independently according to a probability distribution
$\obsDist(\state, \action)$. We denote $\obsDist_\observ(\state,
\action)$ the probability of getting observation $\observ$ in state
$\state$. For $\state$ and $\action$ we have $\sum_{\observ \in
\observations} \obsDist_\observ(\state, \action) = 1$.

In the POMDP model successive state estimation is available by
application of Bayes' rule.  Taking the current belief $\bstate$ as
the {\em prior}, and supposing action $\action$ is executed with
perceptive outcome $\observ$, then the probability that we are in
$\state$ in the successive belief-state $\bstate'$ is given by:

\begin{equation}\label{eq:revision}
\bstate'(\state) = \frac{\obsDist_\observ(\state, \action)
  \sum_{\state'\in \states} \transProb(\state', \action, \state) \bstate(\state') }{\pp{Pr}(\observ | \action, \bstate)}
\end{equation}

\noindent where $\pp{Pr}(\observ | \action, \bstate)$ is the
normalising factor, that is, the probability of getting observation
$\observ$ given we execute $\action$ in $\bstate$.

\subsection{Plan Evaluation}

An optimal solution to a finite-horizon POMDP is a contingent plan,
and can be expressed as a mapping from observation histories to
actions. Although suboptimal in general, useful plans can also take a
classical sequential format. This is the case in {\em conformant}
planning, where the objective is to find a sequence of actions that
achieves a goal ---I.e., reaches a state that satisfies some given
Boolean condition--- with probability $1$.  More generally, a useful
formalism for representing solutions to POMDPs is the finite-state
controller (FSC). This is a three-tuple $\langle \nodes, \selection,
\transition \rangle$ where: $\node \in \nodes$ is a set of controller
states, $\selection_\node(\action) = P(\action | \node)$ gives the
probability that the controller prescribes \action\ when in state
\node, and $\transition_\node(\action, \observation, \node') =
P(\node'|\node, \action, \observation)$ gives the probability the
controller transitions to state $\node'$, supposing we execute
\action\ in state \node\ receiving observation \observation. When
acting for finite $N$ steps, the value of a controller corresponds to
the expected cumulative reward:

\begin{equation}\label{eq:expectedvalue}
V_{\pp{FSC}}(\bstate) = \Expect \bigg{[} 
\sum_{t=0}^{N}  \reward(\bstate_t) \mid \pp{FSC}, \bstate = \bstate_0 \bigg{]}
\end{equation}

\noindent Where $b_t$ is the belief state at time $t$. Finally, it is
useful to note that there is a corresponding deterministic FSCs for
both sequential (i.e., conformant) and contingent plans.


\section{Decision-Theoretic PDDL}

The modelling language of choice for planning in probabilistic
problems is the Probabilistic Planning Domain Definition
Language~\cite{younes:littman:04,younes:etal:2005}. PPDDL was used in
all 3 of the International Planning Competitions since 2004. A
variation on PDDL for describing domains with stochastic actions and
uncertain starting configurations, PPDDL is a declarative first-order
(a.k.a., relational) language that facilitates factored description of
domains and problem instances. There are straightforward compilations
from problems expressed compactly in PPDDL to propositional
representations amenable to state-of-the-art planning procedures.

Because PPDDL cannot model domains that feature partial observability,
we developed an extension of that language, called DTPDDL. This can
express probabilistic models of the sensing consequences of acting, to
quantitatively capture unreliability in perception. That expressive
power is achieved by incorporating perceptual analogues of fluent,
predicate, and action definitions. In detail, we have declarations of
state characterising predicate and fluent symbols according to the
PPDDL syntax. In addition, we allow two other declarative blocks, for
perceptual predicates and fluents respectively. For example, suppose
our robot is tasked with exploring {\em locations} in order to
identify the whereabouts of a {\em visual-object}. We must describe
state and perceptual facts that model the {\em true}, resp. perceived,
locations of objects. In DTPDDL, these declarations appear as:

\small
\begin{tabtt}
(\=:functions  ;; state fluents\\
  \> (is-in ?v - visual-object) - location )\\
(:observable-functions  ;; perceptual fluents\\
  \> (o-is-in ?v - visual-object) - location )
\end{tabtt}
\normalsize

\noindent To model the sensing capabilities of the agent, we have
operator-like descriptive elements, with preconditions in both action
and state symbols, and uniformly positive effects over perceptual
predicates -- I.e, the negation normal form of a sensing effect
formula cannot contain negation. Whereas the effects of an {\em
operator} schema are over state propositions, and describe how the
states change under application of actions, the effects of a {\em
sense} schema are perceptual, and describe the composition of an
observation following the execution of an action. Also, and without a
loss of generality, we find it convenient to separate the actional
precondition from the state precondition. 

Making the above ideas concrete with an example, suppose a robot is
able to look for a visual-object, such as a box, at a given place. We
can model that deterministic action using the following operator
schema:


\small
\begin{tabtt}
(\=:action look-for-object \+ \\
   :parameters (\=?r - robot ?v - visual-object\\
   \> ?l - location) \\
   :precondition (and (= (is-in ?r) ?l) ) \\
   :effect (and (assign (reward) -3) ) ) \\
\end{tabtt}
\normalsize


\noindent In a slight departure from PPDDL, we suppose that
$\pp{look-for-object}$ has an effect on the state. That is, its
execution incurs a penalty, $3$, corresponding to the {\em cost} of
performing the visual search.\footnote{In PPDDL {\em reward} is a {\em
reserved word}, and occurs in {\em increased} and {\em decreased}
clauses of operator schemata. In that setting, {\em reward} is
interpreted as accumulated (resp. instantaneous) rewards and
penalties.}

Completing the example, we model the sensing outcome that results from
instantiating the $\pp{look-for-object}$ operator:

\small
\begin{tabtt}
(\= :sense vision \+\\
 :parameters \= (\= ?r - robot ?v - visual-object\\
 \>\>  ?l - location) \\
 :execution \> ( \> look-for-object ?r ?v ?l) \\
 :precondition (and (= (is-in ?r) ?l) ) \\
 :effect \>  (  \> and (when (= (is-in ?v) ?l) \\
   \> \> (probabilistic 0.8 \\
   \>  \>(= (o-is-in ?v) ?l))) \\
  \> (when (not (= (is-in ?v) ?l)) \\
   \>  \> (probabilistic 0.1 \\
   \>  \> (= (o-is-in ?v) ?l))))) \\
\end{tabtt}
\normalsize


\noindent Here, the {\em execution} clause is a lifted description of
the sense action-precondition -- I.e,
$(\pp{look-for-object}~\pp{R2D2}~\pp{box}~\pp{office}) =
\poss_\stochActions(\pp{vision}~\pp{R2D2}~\pp{box}~\pp{office})$. Above,
we include a redundant state-precondition, $(=(\pp{is-in}~?r)?l)$ in
order to fully demonstrate the syntax. Interpreting the above schema,
if action $(\pp{look-for-object}~\pp{R2D2}~\pp{box}~\pp{office})$ is
executed, there is a $0.8$ chance of perceiving the box if it is in
the $\pp{office}$, and otherwise a $0.1$ of perceiving it.


The syntax and semantics for describing initial state distributions in
DTPDDL is taken verbatim from PPDDL. It is useful to present that
factored tree-like structure here, in order to aid our discussion of
sequential planning in POMDP determinisations. The distribution is
expressed in a tree-like structure of terms. Each term is either: (1)
atomic, e.g., $(=(\pp{location}~\pp{box})~\pp{office})$, (2)
probabilistic, e.g., $(\pp{probabilistic}~p_1 (T_1) .. p_n (T_n))$
where $T_i$ are conjunctive, or (3) a conjunct over probabilistic and
atomic terms. The root term is always conjunctive, and the leaves are
atomic. For example, the starting distribution for $\pp{R2D2}$ might
be described thusly:

\small
\begin{tabtt}
(\=:init (= (is-in R2D2) kitchen) \+ \\
       (probabilistic \=.8 (= (is-in box) office)  \\
		      \>.2 (= (is-in box) kitchen)) \\
       (probabilistic .3 (= (is-in cup) office)  \\
		      \>.7 (= (is-in cup) kitchen))) \\
\end{tabtt}
\normalsize


\noindent The interpretation of such an expression can be given
according to a recursive descent. Here, an atom is visited as soon as
its conjunctive parent is visited, and a conjunctive term is visited
if all its immediate subterms are visited. A probabilistic term is
visited if exactly one of its subterms, $T_i$, is visited. Each valid
visitation of the root term according to this recursive definition
encapsulates a starting state, along with the probability it
occurs. The former corresponds to the union of all visited atoms, and
the latter corresponds to the product of $p_i$ entries on the visited
subterms of probabilistic elements. Making these ideas concrete, our
example yields the following flat state distribution:


\small
\begin{tabular}{cccc}
\hline
Probability & (is-in R2D2)  & (is-in box)  & (is-in cup) \\
\hline
.24 & kitchen & office & office \\
.06 & kitchen & kitchen & office \\
.56 & kitchen & office & kitchen \\
.14 & kitchen & kitchen & kitchen \\
\hline
\end{tabular}
\normalsize



\section{Sequential Planning in POMDP Determinisations}



Goal expressions in the problem description ---for example
$(\pp{kval}~(\pp{location}~\pp{box}))$ expresses ``the robot knows the
location of the {\em box}--- are treated as control-knowledge by the
planner. Here, a plan is not considered valid unless in the finial
stet the goal condition is satisfied.


\section{Evaluation}


\section{Discussion and Related Work}

There have been a number of developments recently towards planning
under uncertainty using systems that were intended for sequential
planning in deterministic problems.  Notably,
\system{FFR$_a$}~\cite{yoon:etal:2007}, the winning entry from the
probabilistic track of the 2004 International Planning Competition.
In the continual paradigm, \system{FFR$_a$} uses the fast satisficing
procedure \system{FF}~\cite{hoffmann:nebel:2001} to compute sequential
plans and corresponding execution traces.
%%
More computationally expensive approaches in this vein combine
sampling strategies on valuations over {\em runtime variables} with
deterministic planning procedures. The outcome is typically a more
robust sequential plan~\cite{yoon:etal:2008}, or contingent
plan~\cite{majercik:2006}.
%%
Also leveraging deterministic planners in problems that feature
uncertainty, \system{Conformant-FF}~\cite{hoffmann:brafman:2006} and
$T_0$~\cite{palacios:geffner:2009} demonstrate how conformant planning
---i.e., sequential planning in unobservable worlds--- can be modelled
as a deterministic problem, and therefore solved using sequential
systems. In this conformant setting, advances have been towards
compact representations of beliefs amenable to existing best-first
search planning procedures, and lazy evaluations of beliefs. Most
recently this research thread has been extended to contingent planning
in fully observable non-deterministic
environments~\cite{albore:etal:2009}.

\section{Concluding Remarks}

From an automated planning perspective, the problem of practical
mobile robot control poses important and contrary challenges.
%%
On the one hand, planning and execution monitoring must be
lightweight, robust, timely, and should span the lifetime of the
robot. Those processes must seamlessly accommodate exogenous events,
changing objectives, and the underlying unpredictability of the
environment.
%%
On the other hand, robot planning should perform computationally
expensive reasoning about contingencies, and possible revisions of
subjective belief according to quantitatively modelled uncertainty in
acting and sensing. 

In this paper we address these challenges, developing a continual
planner that switches between sequential and decision-theoretic
planning. Given a POMDP model of the environment, sequential planning
is used to compute an initial deterministic sequential plan and
complementary runtime evolution of the decision process. That plan is
executed until a validation of an assumption about a runtime
proposition is requested.







The switching continual planning system we have described serves as
the underlying planner for CogX.




the effects of a {\em sense} schema are perceptual, whereas the
effects of an {\em operator} schema are over state propositions.




posted by a motivational component of the underlying robotic
architecture. 

contingent sensory plans that are tailored to current
objectives.

In this paper we present an approach to continual planing that uses
two planning systems. The first 

 to a distinct class of
challenges. We suppose 
%%
The underlying environment is modelled as a POMDP. We use the fast
classical satisficing system FastDownward to find a deterministic
sequential plan and complementary runtime evolution of that
process. This corresponds to a generalisation of replanning in
probabilistic planning to problems with partial observability.

Interaction between the sequential planner and execution proceeds
more-or-less analogously to popular replanning approaches

Addressing these challenges in a monolithic framework, we present a
{\em switching} continual planner, that uses the fast sequential
satisficing procedure FastDownward to perform net-benefit
%%
makes reasonable assumptions about the evolution of the runtime state
given a POMDP model of the environment. 

contingent sensory plans that are tailored to current
objectives.


The decision-theoretic planner is able to tailor sensory processing on
a robot platform to the current objective, while FastDownward  quickly 


In this paper we develop a continual planing system that uses two
planning systems. The first, is a state-of-the-art domain independent
planner for deterministic problems. The second is a information-state
contingency planning the information-state space of 


Continual planning is a powerful technique that goes some way to
addressing those challenges. That approach interleaves planning and
execution, deliberately postponing planning for contingencies unless
they eventuate during execution. 


computing a single sequential plan and
eventuality



To these challenges it seems a continual planning approach is best, 

where the runtime state evolves during plan execution 


interleaved planning and execution. 

the latter is able to tailor sensory processing on a robot platform,
in order that it.

{\em ad-hoc} 

, reason about degrees of belief and uncertainty about the world, 

probabilistic sequential decision making in practical sized problems
is intractable.

quantitative probabilistic models of the perception and action .


\bibliographystyle{aaai}
\bibliography{papers}

\end{document}


























\subsection{PPDDL}

We briefly discuss the description language PPDDL, a fairly
straight-forward extension of the Planning Domain Definition Language
for describing fully observable Markov decision processes.

\small{
\begin{tabtt}
(\= :action look-at-object \\
  \> :parameters \=(?r - robot ?v - visual-object\\
  \> \> ?l - label ?l - location) \\
  \> :precondition (and (= (is-in ?r) ?l) \\
  \> \> (= (label ?o) ?l) ) \\
  \> :effect (and (assign (reward) -3) ) ) \\
\end{tabtt}
}

\small{
\begin{tabtt}
(\= :observe visual-object \\
  \> :parameters \=(?r - robot ?v - visual-object\\
  \> \> ?l - label ?l - location) \\
  \> :execution (look-at-object ?r ?v ?l ?l) \\
  \> :effect (when (= (is-in ?o) ?l) (probabilistic 0.8)) \\
\end{tabtt}
}

\small{
\begin{tabtt}
(\= :sensor look-for-object \\
  \> :agent (?a - robot) \\
  \> :parameters (?l - label ?l - location) \\
  \> :variables (?o - visual-object) \\
  \> :precondition (\=and (= (is-in ?r) ?l) \\
  \> \> (= (label ?o) ?l) (assume (is-in ?o) ?l) ) \\
  \> :effect () \\
  \> :sense (= (is-in ?o) ?l) \\
\end{tabtt}
}

\subsection{MAPL}



%% \scriptsize
%% \begin{nopagebreak}\begin{tabtt}
%% <observation-def> \=::= \=(:sense <observation symbol> \\
%%                         \> \> :parameters (<typed list (variable)>)  \\
%%                         \> \> <o-def body>) \\
%%   <o-symbol> \> ::= <name> \\
%%   <o-def body> \> ::= [:precondition <GD>] \\
%%   \> \> [:execution <atomic action(term)> ] \\
%%     \> \> [:effect <o-effect>] \\
%%   <atomic action(t)> \> ::= (<action symbol> t\zom) \\
%%   <o-effect> \> ::= (and <c-o-effect>\zom) \\
%%   <o-effect> \> ::= <c-o-effect> \\
%%   <c-o-effect> \> ::= \req{:probabilistic-effects} (probabilistic <prob> <o-effect>) \\
%%   <c-o-effect> \> ::= <p-o-effect> \\
%%   <atomic o-formula(t)> \> ::= (<observation> t\zom) \\
%%   <p-o-effect> \> ::= <atomic o-formula(term)> \\
%%   <p-o-effect> \> ::= (not <atomic o-formula(term)>) \\
%%   <o-f-comp> \> ::= (<binary-comp> <o-f-exp> <o-f-exp>)\\
%%   <o-f-exp> \> ::= <number>\\
%%   <o-f-exp> \> ::= (- <o-f-exp>)\\
%%   <o-f-exp> \> ::= <o-f-head>\\
%%   <o-f-head> \> ::= (<o-function-symbol> <term>\zom )\\
%%   <o-f-head> \> ::= <o-function-symbol>\\
%% \end{tabtt}\end{nopagebreak}
%% \noteme{<structure-def> ::= <attach-def>}
%% \normalsize



%% \footnote{We have simplified the schemata for illustrative
%% purposes. In practice, the probabilities of a particular observation
%% here should be parametrised by the type of visual object the robot is
%% searching for, and category of the location.}


We suppose $\bstate_0$ is given in a factored tree format of the
form:


%% \small{
%% \begin{tabtt}
%% (:init  (and \=f_{11} f_{12} ..
%%  \> (probabilistic p_{}) 
%%  \> (probabilistic ..) ..) \\
%% \end{tabtt}
%% }

%% \[(:\pp{init} (and f_{11} f_{12} .. (\pp{probabilistic} p_) ))\]


planning in practical sized POMDPs is intractable. One approach is to
described the task hierarchically, thereby constrain and simplify the
policy search task. The approach ~\cite{}. 


%% More generally, a useful
%% formalism for representing solutions to POMDPs is the finite-state
%% controller (FSC). This is a three-tuple $\langle \nodes, \selection,
%% \transition \rangle$ where: $\node \in \nodes$ is a set of nodes,
%% $\selection_\node(\action) = P(\action | \node)$, and
%% $\transition_\node(\action, \observation, \node') = P(\node'|\node,
%% \action, \observation)$. Where the value of acting is the discounted
%% accumulated reward over an infinite horizon, then each controller can
%% be assigned a value, written $V_\node(\state)$, expressed in the usual
%% way according to the Bellman equation:

%% \begin{equation}\label{eq:evaluation}
%% \begin{array}{lcl}
%% V_\node(\state) & = & \sum_{\action \in \actions}
%% \selection_\node(\action) \reward(\state, \action) \;\; + \vspace{1ex} \\

%% && \hspace{-10ex} \beta \sum_{\action, \observation,
%% \state', \node'} \transition_\node(\action, \observation, \node')
%% \transProb(\state, \action, \state') \obsDist_\observ(\state',
%% \action) V_{\node'}(\state')
%% \end{array}
%% \end{equation}

%% \noindent Here, the value of {\em prior} $\bstate$ given an FSC is
%% then:

%% \begin{equation} \label{eq:valueBelief}
%% V_{\pp{FSC}}(\bstate) = \max_{\node \in \nodes} \sum_{\state \in \states} \bstate(\state) V_\node(\state)
%% \end{equation}

%% \noindent There is a corresponding deterministic FSCs for both
%% sequential (i.e., conformant) and contingent
%% plans. Eq~\ref{eq:evaluation} gives their value in the finite-horizon
%% case, if we take $\beta = 1$, and alter the process dynamics so there
%% is a compulsory transition to a zero utility sink state when that
%% horizon is reached.


 ---i.e., the values of {\em
runtime variables} in the continual paradigm--- are treated , and
therefore plan for a specific eventuality, replanning from scratch if
something