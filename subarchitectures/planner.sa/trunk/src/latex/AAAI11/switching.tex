
\newcommand{\entropy}{\ensuremath{\mathrm{H}}}


We now describe our {\em switching} planning system that operates
according to the continual planning paradigm. The system {\em
  switches} in the sense that planning and plan execution proceed in
interleaved sessions in which the {\em base planner} is either {\em
  sequential} or {\em decision-theoretic}.  The first session is
sequential, and begins when a DTPDDL description of the current
problem and domain are posted to the system.  During a sequential
session a serial plan is computed that corresponds to one
execution-{\em trace} in the underlying decision-process. That trace
is a reward-giving sequence of process actions and {\em assumptive}
actions. Each {\em assumptive} action corresponds to an assertion
about some facts that are unknown at plan time -- e.g. that a box of
cornflakes is located on the corner bench in the kitchen. The trace
specifies a plan and characterises a {\em deterministic approximation}
(see~\cite{yoon:etal:2008}) of the underlying process in which that
plan is valuable. Traces are computed by a cost-optimising classical
planner which trades off action costs, goal rewards, and
determinacy. Execution of a trace proceeds according to the process
actions in the order that they appear in the trace. If, according to
the underlying belief-state, the outcome of the next action scheduled
for execution is not predetermined above a threshold (here 95\%), then
the system switches to a DT session.


%% due to their {\em relevance} to the
%% action whose scheduled execution triggered the switch to the DT
%% session.  


Because online DT planning is impractical for the size of problem we
are interested in, DT sessions plan in a small abstract problem
defined in terms of the trace from the proceeding sequential session.
This abstract state-space is characterised by a limited number of
propositions, chosen because they relate evidence about assumptions
in the trace.  To allow the DT planner to judge
assumptions from the trace, we add {\em disconfirm} and {\em confirm}
actions to the problem for each of them. Those yield a relatively
small reward/penalty if the corresponding judgement is true/false. If
a judgement action is scheduled for execution, then the DT session is
terminated, and a new sequential session begins.

Whatever the session type, our continual planner maintains a factored
representation of successive belief-states.  As an internal
representation of the $(\pp{:init})$ declaration, we keep a
tree-shaped Bayesian network which gets updated whenever an action is
performed, or an observation received. That belief-state
representation is used: (1) as the source of candidate
determinisations for sequential planning, (2) in determining when to
switch to a DT session, and (3) as a mechanism to guide construction
of an abstract process for DT sessions.

\subsection{Sequential Sessions}


%% Writing \#\ if the value of a proposition is unspecified,
%% taking the $(\pp{:init})$ example from the previous section, we have
%% the following assumptions:

%% \small
%% \begin{tabular}{cccc}
%% \hline
%% Probability & (is-in Robot)  & (is-in box)  & (is-in cup) \\
%% \hline
%% .7 & kitchen & \# &  kitchen\\
%% .3 & kitchen & \# & office \\
%% .8 & kitchen & office & \# \\
%% .2 & kitchen & kitchen & \# \\
%% 1.0 & kitchen & \# & \# \\
%% \hline
%% \end{tabular}
%% \normalsize




%% \noindent Each assumption corresponds to one distinct {\em
%% relaxed} visitation of the root term. Here, a conjunctive term is
%% visited iff its atomic subterms are visited, and zero or one of its
%% immediate probabilistic subterms are visited. 


As we only consider deterministic-action POMDPs, all state uncertainty
is expressed in the $(\pp{:init})$ declaration. This declaration is
used by our approach to define the starting state for sequential
sessions, and the set of assumptive actions available to sequential
planning. Without a loss of generality we also suppose that actions do not have negative preconditions. For a sequential
session the starting state corresponds to the set of facts that are
true with probability $1$. Continuing our example, that starting state
is the singleton:

\small
\[
\begin{array}{l}
\state_0 \equiv \{(=(\pp{is-in}~\pp{Robot})\pp{kitchen})\}.
\end{array}
\]
\normalsize

To represent state assumptions we augment the problem posed during a
sequential session with an \emph{assumptive action} $\assumptiveS{i}$
for each element, $\prob_i (T_i)$, of each probabilistic term from
$(\pp{:init})$. Here, $\assumptiveS{i}$ can be executed if no
$\assumptiveS{j}$, $j \neq i$, has been executed from the same
probabilistic term, and, either
$(\pp{probabilistic}~..\prob_i~(T_i)..)$ is in the root conjunct, or
it occurs in $T_k$ for some executed $\assumptiveS{k}$.  We also add
constraints that forbid scheduling of assumptions about facts after
actions with preconditions or effects that mention those facts. For
example, the robot cannot assume it is plugged into a power source
immediately after it unplugs itself.  Executing $\assumptiveS{i}$ in a
state $\state$ effects a transition to a successor state
$\state^{T_i}$, the union of $\state$ with atomic terms from $T_i$.
For example, consider the following sequential plan:

\small
\[
\begin{array}{l}
\actions^{\circ}(.8;(=(\pp{is-in}~\pp{box})\pp{kitchen}));\\
\actions^{\circ}(.3;(=(\pp{is-in}~\pp{cup})\pp{office}));\\
(\pp{look}~\pp{box}~\pp{kitchen});
(\pp{look}~\pp{cup}~\pp{office});\\
(\pp{report}~\pp{box}~\pp{kitchen}); 
(\pp{report}~\pp{cup}~\pp{office})
\end{array}
\]
\normalsize

\noindent Applying the first action in $\state_0$ yields:


\small
\[
\hspace{-1ex}\begin{array}{l}
\{(=(\pp{is-in}~\pp{Robot})\pp{kitchen}),(=(\pp{is-in}~\pp{box})\pp{kitchen})\}
\end{array}
\]
\normalsize

\noindent with a probability of $0.8$. The assumed state before the
scheduled execution of action $(\pp{look}~\pp{box}~\pp{kitchen})$ is:

\small
\[
\hspace{-1ex}\begin{array}{l}
\{(=(\pp{is-in}~\pp{Robot})\pp{kitchen}),
(=(\pp{is-in}~\pp{box})\pp{kitchen}), \\\;\;(=(\pp{is-in}~\pp{cup})\pp{office})\}
\end{array}
 \]
\normalsize

and has a probability of $0.24$.

To describe the optimisation criteria used during sequential sessions
we model $\assumptiveS{i}$ probabilistically, supposing that its
application in state $\state$ effects a transition to $\state^{T_i}$
with probability $\prob_i$, and to $\state^\bot$ with probability $1 -
\prob_i$. State $\state^\bot$ is an added sink. Taking $\prob_i$ to be
the probability that the $i^{th}$ sequenced action, $\action_i$, from
a trace of state-action pairs $\langle \state_0, \action_0,\state_1,
\action_1,.., \state_N \rangle$ does not transition to $\state^\bot$,
then the optimal sequential plan has value:

\small
\[
V^* = \max_N \max_{\state_0, \action_0,.., \state_N} \prod_{i=1..N-1} \prob_i \sum_{i=1..N-1}
\reward(\state_i, \action_i),
\]
\normalsize

\subsection{DT Sessions}

When an action is scheduled whose outcome is uncertain according to
the underlying belief-state, the planner switches to a DT
session. That plans for {\em small} abstract processes defined
according to the action that triggered the DT session, the assumptive
actions in the proceeding trace, and the current
belief-state. Targeted sensing is encouraged by augmenting the reward
model to reflect a heuristic value of knowing the truth about
assumptions. In detail, all rewards from the underlying problem are
retained. Additionally, for each relevant assumptive action
$\assumptiveS{i}$ in the current trace, we have a {\em disconfirm
action} $\assumptiveDT{i}$ so that for all states $\state$:

\vspace{-1ex}
\small
\[
\reward(\state, \assumptiveDT{i}) = \bigg\{ \begin{array}{ll}
\$(T_i) & \pp{if}~\;\;T_i \not\subseteq \state \\
\hat\$(T_i) & \pp{otherwise} \\
\end{array}
\]
\normalsize

\vspace{-1ex}

\noindent where $\$(T_i)$ (resp. $\hat\$(T_i)$) is a small positive
(negative) numeric quantity which captures the utility the agent
receives for correctly (incorrectly) rejecting an assumption.  In
terms of action physics, a disconfirm action can only be executed
once, and otherwise is modelled as a self-transformation.  We only
consider {\em relevant} assumptions when constructing the abstract
model.  If \switchAction\ is the action that switched the system to a
DT session, then an assumption $\assumptiveS{i}$ is {\em relevant} if
it is necessary for the outcome of \switchAction\ to be
determined. For example, taking the switching action \switchAction\ to
be $(\pp{look}~\pp{box}~\pp{kitchen})$ from our earlier sequential
plan example, we have that
$\actions^{\circ}(.3;(=(\pp{is-in}~\pp{cup})\pp{office}))$ is not
relevant, and therefore we exclude the corresponding disconfirm action
from the abstract decision process. Given \switchAction, we also
include another once-only self-transition action
$\actions.\poss(\switchAction)$, a \emph{confirmation action} with the
reward property:

\[
\reward(\state, \actions.\poss(\switchAction)) = \bigg\{ \begin{array}{ll}
\$(\poss(\switchAction)) & \pp{if}~\;\; \poss(\switchAction) \subseteq \state \\
\hat\$(\poss(\switchAction)) & \pp{otherwise} \\
\end{array}
\]

\noindent Execution of either a disconfirmation or the confirmation action
returns control to a sequential session, which then continues from the
underlying belief-state.

Turning to the detail of (dis-)confirmation rewards, in our integrated
system these are sourced from a motivational subsystem. In this paper,
for $\assumptiveDT{i}$ actions we set $\$(x)$ to be a small positive
constant, and have $\hat\$(x)= - \$(x)(1 - \prob) /
\prob$ where $\prob$ is the probability that $x$ is true. For
$\actions.\poss(\switchAction)$ actions we have $\hat\$(x)= -
\$(x)\prob/(1-\prob)$.


In order to guarantee fast DT sessions, those plan in an abstract
process determined by the current trace and underlying belief-state.
The abstract process posed to the DT planner is constructed by first
constraining as statically false all propositions except those which
are true with probability 1, or which are the subject of {\em
  relevant} assumptions. For example, taking the above trace and
switching action {\texttt \mbox{(look~box~office)}}, the underlying
belief in Fig.~\ref{fig:beliefs}B would determine a fully constrained
belief given by Fig.~\ref{fig:beliefs}A.  Next, static constraints are
removed, one proposition at a time, until the number of states that
can be true with non-zero probability in the initial belief of the
abstract process reaches a given threshold.  In detail, for each
statically-false proposition we compute the {\em entropy} of the
relevant assumptions of the current trace {\em conditional} on that
proposition.  Let $X$ be a set of propositions and $2^X$ the powerset
of $X$, then taking


\small
\[
\chi = \{\bigwedge_{x \in X'
  \cap X}x \; \land \bigwedge_{x \in X \setminus X'}\neg x \;|\; X' \in 2^X\},
\]
\normalsize

\noindent we have that $\chi$ is a set of conjunctions each of which
corresponds to one truth assignment to elements in $X$. Where
$p(\phi)$ gives the probability that a conjunction $\phi$ holds in the
belief-state of the DTPDDL process, the entropy of $X$
\emph{conditional} on a proposition $y$, written $\entropy(X|y)$, is
given by Eq.~\ref{eq:condent}.

\vspace{-1ex}
\small
\begin{equation}\label{eq:condent}
  \entropy(X|y) = \sum_{x \in \chi, y' \in \{y, \neg y\}} p(x \land y') \log_2
  \frac{p(y')}{p(x \land y')}
\end{equation}
\normalsize

A low $\entropy(X|y)$ value suggests that knowing the truth value of
$y$ is useful for determining whether or not some assumptions $X$
hold. When removing a static constraint on propositions during the
abstract process construction, $y_i$ is considered before $y_j$ if
$\entropy(X|y_i)~<~\entropy(X|y_j)$. For example, if the serial plan
assumes the box is in the kitchen, then propositions about the
contents of kitchens containing a box,
e.g. $(=(\pp{is-in}~\pp{milk})\pp{kitchen})$, are added to
characterise the abstract process' states. Taking a relevant
assumption $X$ to be $(=(\pp{is-in}~\pp{box})\pp{kitchen})$, in
relaxing static constraints the following entropies are calculated:

\small
\begin{tabtt}
.47 \= = \entropy(\=X|(=(is-in~milk)office))\+ \\
 = \entropy(X|(=(is-in~milk)kitchen)) \-\\
.97 = \entropy(X|(=(is-in~cup)office))\+\\
 = \entropy(X|(=(is-in~cup)kitchen))
\end{tabtt}
\normalsize

\noindent Therefore, the first static constraint to be relaxed is for
\texttt{(=(is-in~milk)office)}, or
equivalently \texttt{(=(is-in~milk)kitchen)}, giving a refined
abstract belief state depicted in
Fig.~\ref{fig:beliefs}C. Summarising, if for Fig.\ref{fig:beliefs}B
the DT session is restricted to belief-states with fewer than $8$
elements, then the starting belief-state of the DT session does not
mention a ``cup''.


\begin{figure}
\tiny
\begin{tabularx}{\columnwidth}{XX}
\hline
\scriptsize\vspace{0.5ex}(A) Fully constrained belief & 
\vspace{0.5ex}\hspace{-4.0ex}(C) Partially constrained belief\\
\vspace{-3.0ex}\begin{tabtt}
(\=:init (=(is-in~Robot)kitchen) \+ \\
       (\=.6(=(is-in~box)kitchen))) \\
\end{tabtt} & 
\vspace{-3.0ex}\begin{tabtt}
\hspace{-5.0ex}(\=:init (=(is-in~Robot)kitchen) \+ \\
       (\=.6(a\=nd(=(is-in~box)kitchen) \+\+ \\
        (\=.9(=(is-in~milk)~kitchen))\+\\
         .1(=(is-in~milk)office))\-\-  \\
       .4(and(=(is-in~box)office) \+\\
        (.1(=(is-in~milk)kitchen))\+\\
        .9(=(is-in~milk)office)))
\end{tabtt}\\
\vspace{-16.5ex}(B) Underlying DTPDDL belief & \\ 
\vspace{-17.0ex}\begin{tabtt}
(\=:init (=(is-in~Robot)kitchen) \+ \\
       (\=.6(a\=nd(=(is-in~box)kitchen) \+\+ \\
        (\=.9(=(is-in~milk)kitchen))\+\\
         .1(=(is-in~milk)office))\-\-  \\
       .4(and(=(is-in~box)office) \+\\
        (.1(=(is-in~milk)kitchen))\+\\
        .9(=(is-in~milk)office))) \-\-\-\\
       (\=.6(=(is-in~cup)office) \+ \\
      .4(=(is-in~cup)kitchen))) \\
\end{tabtt} & \\
\hline
\end{tabularx}
\caption{Simplified examples of belief-states from
       DT sessions. \label{fig:beliefs}}
\normalsize
\end{figure}



%% \begin{figure}
%% \tiny
%% \begin{tabularx}{\columnwidth}{XX}
%% \hline
%% \vspace{1ex}
%% \scriptsize (A) Partially constrained belief & %abstract belief-state & 
%% \vspace{1ex}\scriptsize \hspace{-4ex}(B) Underlying DTPDDL belief \\
%% \vspace{-2ex} 
%% \begin{tabtt}\hspace{-4ex}
%% (\=:init (=(is-in~Robot)kitchen) \+ \\
%%        (\=.6(a\=nd(=(is-in~box)kitchen) \+\+ \\
%%         (\=.9(=(is-in~milk)~kitchen))\+\\
%%          .1(=(is-in~milk)office))\-\-  \\
%%        .4(and(=(is-in~box)office) \+\\
%%         (.1(=(is-in~milk)kitchen))\+\\
%%         .9(=(is-in~milk)office)))
%% \end{tabtt} &
%% \vspace{-2ex} 
%% \begin{tabtt}
%% \hspace{-4ex}(\=:init (=(is-in~Robot)office) \+ \\
%%        (\=.6(a\=nd(=(is-in~box)kitchen) \+\+ \\
%%         (\=.9(=(is-in~milk)kitchen))\+\\
%%          .1(=(is-in~milk)office))\-\-  \\
%%        .4(and(=(is-in~box)office) \+\\
%%         (.1(=(is-in~milk)kitchen))\+\\
%%         .9(=(is-in~milk)office))) \-\-\-\\
%%        (\=.6(=(is-in~cup)office) \+ \\
%%       .4(=(is-in~cup)kitchen))) \\
%% \end{tabtt} \\
%% \vspace{-8ex}\scriptsize(C) Fully constrained belief & \\
%% \vspace{-7.5ex} \begin{tabtt}
%% (\=:init (=(is-in~Robot)kitchen) \+ \\
%%        (\=.6(=(is-in~box)kitchen))) \\
%% \end{tabtt} & \\
%% \hline
%% \end{tabularx}
%% \caption{Simplified examples of abstract belief-states from
%%        DT sessions. \label{fig:beliefs}}
%% \normalsize
%% \end{figure}







%%% Local Variables: 
%%% mode: latex
%%% TeX-master: "aaai11"
%%% End: 
