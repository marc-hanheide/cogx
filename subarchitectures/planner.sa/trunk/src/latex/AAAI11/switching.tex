
We now describe our {\em switching} planning system that operates
according to the continual planning paradigm, given a description of
the current problem and domain in DTPDDL. The system {\em
switches}, in the sense that planning proceeds in interleaved
sessions, in which the base planner is either {\em sequential}/linear
or {\em decision-theoretic}.
%%
During a sequential session, a rewarding {\em trace} of a possible
execution is computed using a cost-optimising satisficing planner
which trades action costs, goal rewards, and determinacy.
%%
Formatted as a linear plan, the trace specifies a sequence of actions
that achieves some reachable goals following a deterministic
approximation of the problem at hand.
%%
Structurally, a trace is a sequence of elements that are either: (i) ground
actions from the DTPDDL description of the world, or (ii) atomic {\em
assumptions}, modelled as deterministic actions, made about the truth
value of facts that can only be determined at runtime (e.g., that a
box of cornflakes is located on the corner bench in the kitchen).
%%
The system always begins in a sequential session, so that plan
execution proceeds by applying DTPDDL actions from the trace in
sequence until the applicability of the next scheduled action is too
uncertain according to a threshold parameter~(here, 95\%). In that
eventuality a DT session commences, which tailors sensory processing
to determine whether the assumptions made in the trace hold, or which
otherwise acts to achieve the goals. 


Because online DT planning in large problems is impractical (we seek
response times in seconds), a DT session plans in an abstract process
determined by the current trace and underlying belief-state. This
abstraction is constructed by first excluding all propositions (and
related actions) that are not true, or {\em assumed } true, of states
in the trace, then adding them back, using as an heuristic the entropy
of the trace assumptions conditional on a candidate
proposition. Propositions are added, one at a time, until the number
of states in the initial belief-state reaches a given
threshold.\footnote{Our experiments are performed using a number of
different threshold parameters.}  To the resulting abstract model we
also add {\em disconfirm} and {\em confirm} actions that the
DT session can schedule in order to judge an atomic assumption
in the trace. In the abstract model these actions yield a relatively
small reward if the corresponding judgement is true (or small penalty
otherwise). If a judgement action is scheduled for execution the
DT session is terminated, and control is returned to a
sequential session.


Finally, whether proceeding in a sequential or DT session, our
continual planner maintains a factored representation of successive
belief-states by performing belief revision. Our internal
representation of the underlying belief-state corresponds more-or-less
directly to an $(\pp{:init})$ declaration. That belief-state
distribution is used: (1) as the source of candidate determinisations
for sequential planning, (2) in determining when to switch to a DT
session, and (3) as a mechanism to guide construction of an abstract
process for DT sessions.

\subsection{Sequential Sessions}

We only consider deterministic-action POMDPs, where all state
uncertainty is expressed in the $(\pp{:init})$ declaration. This
declaration is used by our approach to define the starting state for
sequential sessions, and the set of state-assumptions available to
sequential planning.  Writing \#\ if the value of a proposition is
unspecified, taking the $(\pp{:init})$ example from the previous
section, we have the following assumptions:


\small
\begin{tabular}{cccc}
\hline
Probability & (is-in R2D2)  & (is-in box)  & (is-in cup) \\
\hline
%% .24 & kitchen & office & office \\
%% .06 & kitchen & kitchen & office \\
%% .56 & kitchen & office & kitchen \\
%% .14 & kitchen & kitchen & kitchen \\
.7 & kitchen & \# &  kitchen\\
.3 & kitchen & \# & office \\
.8 & kitchen & office & \# \\
.2 & kitchen & kitchen & \# \\
1.0 & kitchen & \# & \# \\
\hline
\end{tabular}
\normalsize

\noindent Each assumption corresponds to one distinct {\em
relaxed} visitation of the root. Here, a conjunctive term is visited
iff its atomic subterms are visited, and zero or one of its immediate
probabilistic subterms are visited. For a sequential session, the
starting state corresponds to an assumption (usually unique) with
probability $1$. In our example, that starting state is:

\small
\[
\begin{array}{l}
\state_0 \equiv \{(=(\pp{is-in}~\pp{R2D2})~\pp{kitchen}),\\
\;\;(=(\pp{is-in}~\pp{box})~\#), (=(\pp{is-in}~\pp{cup})~\#)\}.
\end{array}
\]
\normalsize

Encapsulating state-assumptions, we annotate the problem posed during
a sequential session with an assumptive action $\assumptiveS{i}$ for
each element $\prob_i (T_i)$, of each probabilistic term from
$(\pp{:init})$. Here, $\assumptiveS{i}$ can be executed if no
$\assumptiveS{j}$, $j \neq i$, has been executed from the same
probabilistic term, and, either
$(\pp{probabilistic}~..\prob_i~(T_i)..)$ is in the root conjunct, or
it occurs in $T_k$ for some executed $\assumptiveS{k}$.
%%
We add additional constraints that forbid the scheduling of
assumptions about facts after actions with preconditions, or effects,
that mention those facts. For example, the robot cannot assume it is
plugged into a power source immediately after it unplugs itself.
%%
Executing $\assumptiveS{i}$ in a state $\state$ effects a transition
to a successor state $\state^{T_i}$ with probability $\prob_i$, and
$\state^\bot$ with probability $1 - \prob_i$. Here, $\state^{T_i}$ is
the union of $\state$ with atomic terms from $T_i$. State
$\state^\bot$ is an added sink.


We now describe the optimisation criteria used during sequential
sessions. Where $\prob_i$ is the probability that the $i^{th}$
sequenced action, $\action_i$, from a trace of state-action pairs
$\langle \state_0, \action_0,\state_1, \action_1,.., \state_N \rangle$
does not transition to $\state^\bot$, we have that the optimal trace
has value:

%% define the value of a trace
%% $\state_0, \action_0, \state_1, \action_1,.., \state_N$ from the
%% deterministic model to equal:

%% \small
%% \begin{equation}\label{eq:tracevalue}
%% V(\state_0, \action_0, \state_1, \action_1,.., \state_N) =  \prod_{i=1..N-1} \prob_i \sum_{i=1..N-1} \reward(\state_i, \action_i)
%% \end{equation}
%% \normalsize

%% \noindent The
%% optimal trace given a plan, i.e., the sequence of non-assumptive
%% actions from the trace, therefore has value:

\small
\[
V^* = \max_N \max_{\state_0, \action_0,.., \state_N} \prod_{i=1..N-1} \prob_i \sum_{i=1..N-1}
\reward(\state_i, \action_i),
\]
\normalsize

\noindent  Here,  $\reward(\state_i, \action_i)$ is the instantaneous
reward ---i.e., can be negative for action costs, or positive for goal
achievement--- received for executing action $\action_i$ in state
$\state_i$. Finally, it is worth clarifying that in practise we do not
artificially limit the length of traces that the sequential planner
can consider. Moreover, for problems we consider there is always a
finite optimal trace.


\subsection{DT Sessions}

A DT session plans in a {\em small} abstract processes defined
according to the assumptive actions in the proceeding trace, and the
$(\pp{:init})$ declaration that models the underlying
belief-state. Targeted sensing is encouraged by augmenting the reward
model to reflect a heuristic value of knowing the truth about
assumptions in the trace. In detail, all rewards from the original
POMDP are retained. Additionally, for each $\assumptiveS{i}$ action
scheduled by the current trace, we have a {\em dual}
$\assumptiveDT{i}$ so that for all states $\state$:

\small
\[
\reward(\state, \assumptiveDT{i}) = \bigg\{ \begin{array}{ll}
\$(T_i) & \pp{if}~\;\;T_i \not\subseteq \state \\
\hat\$(T_i) & \pp{otherwise} \\
\end{array}
\]
\normalsize

\noindent where $\$(T_i)$ (resp. $\hat\$(T_i)$) is a 
relatively\footnote{Compared to rewards from the original process
model.} small positive (negative) numeric quantity which captures the
utility the agent receives for correctly (incorrectly) rejecting an
assumption. A dual action can only be executed once, and except in the
applicability of that dual corresponds to a self-transformation
%%
Furthermore, if \switchAction\ is the action that switched the system
to a DT session, then an assumption $\assumptiveS{i}$ is only {\em
active} if
\switchAction\ is not applicable in the sequential model unless
$\assumptiveS{i}$ is scheduled in the trace prefix
to \switchAction. We only include the duals of active assumptions in
DT sessions. Continuing our simplified example, if the trace is:

\small
\[
\begin{array}{l}
\actions^{\circ}(.8;(=(\pp{is-in}~\pp{box})\pp{office}));\\
\actions^{\circ}(.3;(=(\pp{is-in}~\pp{cup})\pp{kitchen}));\\
(\pp{look}~\pp{box}~\pp{office});
(\pp{look}~\pp{cup}~\pp{kitchen});\\
(\pp{report}~\pp{box}~\pp{office}); 
(\pp{report}~\pp{cup}~\pp{kitchen})
\end{array}
\]
\normalsize

\noindent Taking the switching action \switchAction\ to be
$(\pp{look}~\pp{box}~\pp{office})$, we have that
$\actions^{\circ}(.3;(=(\pp{is-in}~\pp{cup})\pp{kitchen}))$ is not
active, and therefore exclude it from the
POMDP posed to the DT session. 

Given \switchAction, we also include another self-transition action
$\actions.\poss(\switchAction)$ with the reward property:

\[
\reward(\state, \actions.\poss(\switchAction)) = \bigg\{ \begin{array}{ll}
\$(\poss(\switchAction)) & \pp{if}~\;\; \poss(\switchAction) \subseteq \state \\
\hat\$(\poss(\switchAction)) & \pp{otherwise} \\
\end{array}
\]

Execution of either a dual action $\assumptiveDT{i}$, or the
$\actions.\poss(\switchAction)$, returns control to a sequential
session, which then continues from the underlying
belief-state. Turning to the detail of dual rewards, in our integrated
system these are sourced from a motivational subsystem. In this paper,
for $\assumptiveDT{i}$ actions we set $\$(x)$ to be a small positive
constant, and have $\hat\$(x)= - \$(x)(1 - \prob) / \prob$ where
$\prob$ is the probability that $x$ is true. For
$\actions.\poss(\switchAction)$ actions we have $\hat\$(x)= -
\$(x)\prob/(1-\prob)$



\Omit{
%%
Finally, 
If an
assumption was rejected, we prohibit that sequential session from
making it again.
}


The process posed to a DT session is abstract. The abstraction is
defined by first constructing an $(\pp{:init})$ declaration without
any-active assumptions from the trace -- e.g.,
(Fig.\ref{fig:abstraction-b}) gives an example, where diamonds are
probabilistic terms, and circles are atomic and/or conjunctive.
%%
For each excluded atom, we compute the {\em entropy} of the active
assumptions, {\em conditional} on that atom. Intuitively, lower
entropy indicates an atom gives better information about
assumptions. Facts are iteratively added to the belief-state in
increasing order according to that measure
(Fig.\ref{fig:abstraction-c}).


%% In the first step, we remove all facts that are not part of an
%% assumption (Fig. \ref{fig:abstraction-b}). At this point, the session
%% would proceed in an abstraction of the environment that does not
%% contain $\pp{place1}$, the $\pp{cup}$ or a $\pp{kitchen}$. 
%% %%
%% In a second step, we iteratively refine the relaxed declaration by
%% adding terms from the original statement of $(\pp{:init})$ while the
%% number of abstract states in $\bstate_0$ that occur with non-zero
%% probability according to that refined declaration remains of a
%% practicable size. In detail, 





\begin{figure}[h!]
  \centering
  \tikzstyle{tree} = [sibling distance=4.5mm]
  \tikzstyle{toplevel} = [grow'=right, sibling distance=22mm]
  \tikzstyle{seclevel} = [sibling distance=9mm]
  \tikzstyle{pnode} = [diamond, draw=black, minimum size=2.5mm]
  \tikzstyle{cnode} = [circle, draw=black, minimum size=3mm]
  \tikzstyle{assumption} = [solid, very thick, draw=black]
  \tikzstyle{selected} = [solid, draw=black]
  \tikzstyle{unused} = [densely dashed, draw=black!40]
  %% \subfloat[The initial belief-state with the assumptions made by the continual
  %%   planner in bold.]{
  %%     \label{fig:abstraction-a}
  %%     \begin{tikzpicture}[
  %%   level 3/.style={tree}]
  %%   \node[pnode, assumption] (cat) at (1,1) {} [toplevel]
  %%     child[seclevel] {node[cnode, assumption] (office) {}
  %%       child {node [pnode, assumption] (box) {}
  %%         child {node [cnode] (boxp1) {}}
  %%         child {node [cnode, assumption] (boxp2) {}}
  %%       }
  %%       child {node [pnode] (cup) {} 
  %%         child {node [cnode] (cupp1) {}}
  %%         child {node [cnode] (cupp2) {}}
  %%       }
  %%     }
  %%     child {node[cnode] (kitchen) {}
  %%       child {node [pnode] (box2) {} 
  %%         child {node [cnode] (box2p1) {}}
  %%         child {node [cnode] (box2p2) {}}
  %%       }
  %%     };
  %%  \tiny
  %%  \draw[assumption] (cat) -- (office) -- (box) -- (boxp2);
  %%  \node[above=0 of cat] {$\pp{(category room1)}$};
  %%  \node[below=0 of kitchen] {$\pp{kitchen}$};
  %%  \node[below=0 of office] {$\pp{office}$};
  %%  \node[below=0 of box] {$\pp{(is-in box)}$};
  %%  \node[below=0 of box2] {$\pp{(is-in box)}$};
  %%  \node[below=0 of cup] {$\pp{(is-in cup)}$};
  %%  \node[right=0 of boxp1] {$\pp{place1}$};
  %%  \node[right=0 of boxp2] {$\pp{place2}$};
  %%  \node[right=0 of box2p1] {$\pp{place1}$};
  %%  \node[right=0 of box2p2] {$\pp{place2}$};
  %%  \node[right=0 of cupp1] {$\pp{place1}$};
  %%  \node[right=0 of cupp2] {$\pp{place2}$};
  %%   % \node[node] (kitchen) right of (cat) {};
  %% \end{tikzpicture}}
\qquad
  \subfloat[Removing facts that are not part of an assumption.]{
    \label{fig:abstraction-b}
    \begin{tikzpicture}[
    level 3/.style={tree}]
    \node[pnode, assumption] (cat) at (1,1) {} [toplevel, unused]
      child[seclevel] {node[cnode, assumption] (office) {}
        child {node [pnode, assumption] (box) {}
          child {node [cnode, unused] (boxp1) {}}
          child {node [cnode, assumption] (boxp2) {}}
        }
        child {node [pnode, unused] (cup) {} 
          child {node [cnode, unused] (cupp1) {}}
          child {node [cnode, unused] (cupp2) {}}
        }
      }
      child {node[cnode, selected] (kitchen) {}
        child {node [pnode, selected] (box2) {} 
          child {node [cnode, unused] (box2p1) {}}
          child {node [cnode, selected] (box2p2) {}}
        }
      };
   \draw[assumption] (cat) -- (office) -- (box) -- (boxp2);
   \draw[selected] (cat) -- (kitchen) -- (box2) -- (box2p2);
   \tiny
   \node[above=0 of cat] {$\pp{(category room1)}$};
   \node[below=0 of office] {$\pp{office}$};
   \node[below=0 of box] {$\pp{(is-in box)}$};
   \node[below=0 of box2] {$\pp{(is-in box)}$};
   \node[right=0 of boxp2] {$\pp{place2}$};
   \node[right=0 of box2p2] {$\pp{place2}$};
    % \node[node] (kitchen) right of (cat) {};
  \end{tikzpicture}}
\vspace{2mm}
  \subfloat[Refinement, by adding relevant facts.]{
    \label{fig:abstraction-c}
    \begin{tikzpicture}[
    level 3/.style={tree}]
    \node[pnode, assumption] (cat) at (1,1) {} [toplevel, unused]
      child[seclevel] {node[cnode, assumption] (office) {}
        child {node [pnode, assumption] (box) {}
          child {node [cnode, selected] (boxp1) {}}
          child {node [cnode, assumption] (boxp2) {}}
        }
        child {node [pnode, unused] (cup) {} 
          child {node [cnode, unused] (cupp1) {}}
          child {node [cnode, unused] (cupp2) {}}
        }
      }
      child {node[cnode, selected] (kitchen) {}
        child {node [pnode, selected] (box2) {} 
          child {node [cnode, selected] (box2p1) {}}
          child {node [cnode, selected] (box2p2) {}}
        }
      };
   \draw[assumption] (cat) -- (office) -- (box) -- (boxp2);
   \draw[selected] (box) -- (boxp1);
   \draw[selected] (cat) -- (kitchen) -- (box2) -- (box2p2);
   \draw[selected] (box2) -- (box2p1);
   \tiny
   \node[above=0 of cat] {$\pp{(category room1)}$};
   \node[below=0 of office] {$\pp{office}$};
   \node[below=0 of box] {$\pp{(is-in box)}$};
   \node[below=0 of box2] {$\pp{(is-in box)}$};
   \node[right=0 of boxp1] {$\pp{place1}$};
   \node[right=0 of boxp2] {$\pp{place2}$};
   \node[right=0 of box2p1] {$\pp{place1}$};
   \node[right=0 of box2p2] {$\pp{place2}$};
    % \node[node] (kitchen) right of (cat) {};
  \end{tikzpicture}}
  
  \caption{Abstract belief-state, and refinement.}
\label{fig:abstraction}
\end{figure}








%%% Local Variables: 
%%% mode: latex
%%% TeX-master: "moritz_2011"
%%% End: 
