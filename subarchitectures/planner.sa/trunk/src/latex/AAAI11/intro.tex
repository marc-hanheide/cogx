




Recently there have been a number of integrated robotic systems which
incorporate a high-level {\em continual planning} and execution
monitoring
subsystem~\cite{wyattetal2010tamd,talamadupula:2010,Kraft2008}.
%%
For the purpose of planning, sensing is modelled {\em
deterministically}, and beliefs about the underlying state are
modelled {\em qualitatively}.
%%
Both~\citeauthor{talamadupula:2010} and
~\citeauthor{wyattetal2010tamd} identify
\emph{continual planning} with {\em probabilistic}
models as an important challenge for future research.
%%
Motivating that sentiment, planning according to an accurate
stochastic model of noisy sensing and state should yield more
efficient and robust deliberations.
%%
In the first place, that means moving from deterministic to
probabilistic models of noisy sensing, and from qualitative to
quantitative models of state uncertainty. The key challenge then, is
to develop a {\em base planner} that exhibits similar speed and
scalability to base planners employed in existing robotic systems
---e.g.,~\citeauthor{wyattetal2010tamd} use a {\em classical}
procedure--- which is also able to synthesise relatively efficient
deliberations according to detailed probabilistic models of the
environment.


This paper describes a planning approach we have developed to address
the challenge just outlined. The approach is implemented on a mobile
robotic platform that continuously deliberates in a stochastic dynamic
environment in order to achieve goals set by the user, and acquire
knowledge about its surroundings.


This domain features partial observability, particularly
because the state is not perfectly known, and state information is
gained by the robot through the use of noisy sensing
actions. 
%Using a propositionally factored state representation, for
For
interesting sized tasks the corresponding POMDP model is too large for a
POMDP solver to be applied directly. Inspired by the replanning
approaches used successfully for MDP
planning~\cite{yoon:etal:2007,yoon:etal:2008}, we propose a continual
planning approach that uses a classical planning system to compute a
reasonably valuable trace in the model, and then uses a
decision-theoretic planner on small subproblems where reasoning about
observations might be useful. We refer to the complete system as a
{\em switching continual planner}. The approach is not optimal,
particularly as it relies on the results of satisficing sequential planning
directly. It does nevertheless perform better than a purely sequential
replanner, and is fast enough to be used for real-time decision-making
on a mobile robot.





Our system is domain independent, taking domain and problem
descriptions in a first-order declarative language we have developed,
called the decision-theoretic planning domain definition language
(DTPDDL). In this paper, we restrict our attention to deterministic
action goal-oriented POMDPs\foonote{We note that POMDPs with
stochastic actions can be compiled into equivalent
deterministic-action POMDPs, where all the original action uncertainty
is expressed in the starting-state
distribution~\cite{ng:Jordan:2000}.} -- I.e., where a finite horizon
optimal contingent plan exists.  
%%
From these descriptions of POMDPs, we
automatically construct a deterministic model for the sequential
planner. That model includes actions which correspond to making
assumptions about the values of imperfectly known state
variables. Assumptions scheduled by the sequential system are used to
propose a pragmatic abstract belief-state space to the
decision-theoretic system, and to modify the reward function, so that
system might pursue sensing related to those assumptions.


The switching planner provides important advantages in our
mobile robot domain. First, the significant source of uncertainty in the
domain is the unreliability of observations and this approach is
particularly suited to problems that combine a deterministic task
planning problem with observations because the decision-theoretic planner
is only invoked to deal with state uncertainty.
%determine a characteristic of the underlying state. 
Secondly, replanning makes the system robust to
changing objectives and discoveries about the world,
both of which feature in our domain.



The remainder of the paper proceeds as follows. We begin by describing
related work. Next we describe our DTPDDL language with example
declarations from a mobile robot exploration task. Then we describe
the switching planner, how sequential planning proceeds in our problem
models, and how this then provides the input for the
decision-theoretic planner. Finally we provide an evaluation of the
system on the example robot domain, and then provide some concluding
remarks and future directions.





%% BEGIN SCRAPPY


%% The initial abstract process, let's suppose, has two states. The state
%% where everything assumed is true, and the null state. This has a KL
%% divergence from the :init term, because we can suppose it uniformly
%% chooses one concrete state, given the abstract :init states.

%% As you add propositions to the abstract :init term, then the KL
%% divergence decreases. 


%% END SCRAPPY

%%
%% The underlying planning procedures are {\em classical}, i.e.,
%% sequential, planning system.



%% Planning for agents acting in partially observable environments with
%% stochastic actions is extremely computationally
%% expensive~\cite{mdp-complexity}. Even relatively small partially
%% observable Markov decision problems (POMDPs), represented by tens of
%% propositional variables, have state spaces far larger than existing
%% optimal or close to optimal planners can handle. For the easier
%% problem of planning in completely observable stochastic domains, an
%% alternative has been to use classical sequential planners and
%% replan when actions have unplanned-for outcomes~\cite{yoon:etal:2007}.
%% Execution monitoring tracks properties of the state that determine
%% whether further planning is required. For partially observable
%% domains, only probabilistic information about such properties will, in
%% general, be available.



%% However, these approaches rely
%% crucially on being able to determine properties of the current state
%% in order to decide to replan. For partially observable domains, only
%% probabilistic information about these properties will in general be
%% available.



%% Switching gives us some important advantages in our robotic
%% domain. First, the significant source of uncertainty in the domain is
%% the unreliability of observations, and this approach is particularly
%% suited to problems that combine a deterministic task planning problem
%% with observations as the decision-theoretic planner is often invoked
%% to determine a characteristic of the underlying state. Secondly,
%% replanning makes our system robust to changing objectives and the
%% discovery of new facts about the world.


%% As an illustration, consider a robot searching for a box that it knows
%% is often found in offices. The sequential planner might produce a plan
%% that assumes that {\sc Room a} is an office, that the box is in {\sc
%% Room a}, and that the box is in location 1 in {\sc Room a}. Given
%% these assumptions, the plan might be to go to {\sc Room a}, go to
%% location 1 and search for the box. When the plan gets executed and the
%% search begins the decision-theoretic planner is called since searching
%% involves making observations. The decision-theoretic planner might
%% retract the last assumption but leave the others, and build a plan to
%% search the whole of {\sc Room a} for the box. If this plan fails to
%% find the box, it will disprove the assumption that the box is in {\sc
%% Room a}, which will then lead to replanning by the sequential planner
%% based on the newly discovered knowledge.


%To accompany
%that language we have implemented an information-state
%\laostar\ procedure for solving problems expressed in DTPDDL. 

%%

%Have a good understanding of the contents and function of rooms, as
%well as the linguistic referrents to rooms, widgets, and their visual
%qualities.

%Having confidence in its beliefs about the linguistic terms for
%spaces, and the function of those spaces.



%sometimes difficult to predict, with exogenous events, such as the
%changing of the objective, 





%the speed and scalability of software for sequential planning in
%deterministic




%% We developed a first-order declarative language, called DTPDDL, for
%% describing domains of planning problems that correspond to POMDPs.  To
%% accompany that language we have implemented an information-state
%% \laostar\ procedure for solving problems expressed in DTPDDL. 

%% We have extended MAPSIM to parse and simulate DTPDDL problems, and
%% modified \fastdownward\ so that it can find useful sequential executions
%% given DTPDDL models of the problem at hand.




%%  called DTPDDL, along
%% the lines of PDDL for the partially observable case, an \laostar\
%% solution procedure, a determinisation of the DTPDDL problem in MAPL,
%% and modify the \fastdownward\ system to find high quality sequential plans
%% .

%% We model the environment as a partially observable Markov decision
%% process. Although planning in that model is undecidable in
%% general~\cite{}, an optimal finite-horizon plan corresponds to a
%% contingent plan, that is a function mapping action-observation
%% histories to actions.

%% our continual planner is reactive, replanning whenever the underlying
%% domain and problem models change. For example, replanning occurs if
%% the motivational component alters the objectives, and if an assumed
%% outcome of a sensing action is not realised.

%% brittle sensing model, 

%% the evaluation of a fluent at a state is either known. Moreover, there
%% is a sensing process that run-time variables  after-which 

%% For $\prop \in \state$ we say proposition $\prop$ characterises state
%% $\state$. There is always a unique starting state $\state_0$. The goal
%% $\goal$ is a set of propositions, and we say that state $\state$ is a
%% goal state iff $\goal \subseteq \state$.

%%  To keep this exposition
%% simple, for any two distinct actions $\stochAction_i \neq
%% \stochAction_j$, if outcome $\detAct$ is a possibility for
%% $\stochAction_i$ then it cannot also be a possibility for
%% $\stochAction_j$ -- i.e., if $\detAct \in
%% \detActions(\stochAction_i)$ then $\detAct \not\in
%% \detActions(\stochAction_j)$.


%% The solution to a probabilistic planning problem is a contingency
%% plan. This consists of an assignment of actions to states at each
%% discrete timestep up to the planning horizon $n$. The optimal
%% contingency plan is one which prescribes actions to states that
%% maximise the probability that the goal is achieved within $n$ steps
%% from the starting state $\state_0$. For the purposes of this paper
%% we say a plan fails, i.e. achieves the goal with probability $0$, in
%% situations where it does not prescribe an action. Computing the
%% optimal plan for a problem is computationally intractable, and an
%% important direction for research in the field is to develop
%% heuristic mechanisms for generating small linear plans
%% quickly~\cite{littman:etal:98}

%%  which can operate
%% with the original representation of the domain.


%% The sequential planner we use is based
%% on \fastdownward~\cite{fast-downward}. We add the capability to replan
%% ~\cite{brenner:nebel:jaamas09}, and also allow agent
%% knowledge to be represented explicitly, so we can write actions that
%% gain the agent knowledge. LEASE CITE MICHAEL's WORK AS THE BASIS FOR
%% THIS. For the decision-theoretic planner we have implemented our own
%% forward search in the belief-state space.
